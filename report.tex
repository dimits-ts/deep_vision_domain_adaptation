\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[backend=biber]{biblatex}
\usepackage[greek, british]{babel}
\usepackage{alphabeta}
\usepackage{libertine}
\usepackage{csquotes}
\usepackage{float}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage[a4paper,margin=1in,footskip=0.25in]{geometry}
% \pagenumbering{arabic}

\graphicspath{ {./images/} }
\addbibresource{refs.bib}

\newcommand{\loss}{\mathcal{L}}
\newcommand{\threshold}{\mathcal{T}}

\def\code#1{\texttt{#1}}

\title{\Huge Do It Yourself: Generic Pseudo-labeling for Domain Adaptation in Deep Computer Vision}

\author{\LARGE Tsirmpas Dimitris}

\begin{document}
	\begin{titlepage}
		\maketitle
		\thispagestyle{empty} % No page number on the first page
			
	\end{titlepage}
	
	\maketitle
	
	\pagenumbering{arabic} % Switch to Arabic numbering
	
	\section{Introduction}
	
	
	\section{Related Works}
	
	\section{Pseudo-labeling for Classification Enhancement}
	
	\subsection{Model-agnostic Pseudo-labeling algorithm}
	
	We use a modified version of the iCAN algorithm \cite{ican}. The original iCAN algorithm was designed around a model created with the explicit purpose of learning domain-invariant features between the source and target datasets (CAN model). When the unmodified algorithm uses a generic deep model however a number of issues arise. Our approach seeks to remedy the most major such issues.
	
	First of all, in the original paper, the authors propose adding the loss of the fully supervised dataset $\loss_{source}$, the loss of the target dataset $\loss_{tar}$ and the loss of their model's domain-agnostic penalty $\loss_{CAN}$ for each mini-batch as $\loss = \loss_{source} + \loss_{tar} + \loss_{CAN}$. This design decision most likely exists because of the need to have both a source and a target mini batch loaded on the network in order to calculate $\loss_{CAN}$. Since this penalty does not exist in our algorithm, we instead split the training epoch into distinct backward passes for the source and target mini-batches, in order to reduce GPU VRAM requirements.
	
	Secondly, the original iCAN algorithm selects pseudo-labeled batches for each backward pass because of the aforementioned mini-batch requirements. Since our algorithm proves much more unstable, as the underlying generic model may not learn domain-agnostic features, we instead perform pseudo-labeling once every $N_{period}$ epochs. This mechanism ensures that our model will have acquired knowledge of the target distribution from the previously selected pseudo-labeled samples, before being asked to perform pseudo-labeling on the less-easily classifiable, unlabeled samples.
	
	Thirdly, we do not use the "domain bias reweighing function" used by the original authors when calculating $\loss{tar}$. Aside from necessitating a second "domain" classifier, the sampling strategy we employ is inverse to the one proposed by the authors. iCAN attempts to select samples that do not fit the source distribution, in order to prevent its model from selecting target samples that are very similar to the source dataset (since they would score higher confidence scores). Our model-agnostic algorithm attempts to select samples that are closer to the source distribution and, as the model becomes more accustomed to the target distribution, slowly include samples closer to the latter. This is also the motivation behind not re-labeling pseudo-labeled samples.
	
	Aside from these modifications, we use the same \texttt{adaptive threshold} function used in the original paper, which adjusts the confidence threshold used to decide whether to pseudo-label a sample. The function is defined as $adaptive\_threshold(acc, \rho) = \frac{1}{1+e^{-\rho*acc}}$, where $acc$ is the accuracy of the classifier, and $\rho$ a tunable hyperparameter, with $\rho=3$ in the original paper. Higher $\rho$ values lead to a steeper decision curve, as see in Figure. %TODO: include figure
	
		
	The modified procedure can be found in Algorithm \ref{al::modified-ican}, where $D_{source}$ is the source dataset containing the training instances, $D_{target}$ is the equivalent target dataset and $\%$ is the modulo operator.
	
	\begin{algorithm}
		\caption{Modified general incremental learning algorithm} 
		\label{al::modified-ican}
		\begin{algorithmic}[1]	
			\State Train model on dataset $D_{source}$
			
			\State $D_{pseudo}$ = \{\}
			
			\For{epoch}
				\If {$epoch \% N_{period} = 0$}
					\State Calculate $accuracy$ on the validation source dataset
					\State $\threshold = adaptive\_threshold(accuracy, \rho)$
					\For{each $d \in D_{target}$}
						\State $label, confidence = model(d)$
						\If{$confidence > \threshold$}
							\State $D_{pseudo} = D_{pseudo} \cup \{d: label\}$
							\State $D_{target} = D_{target} - \{d\}$
						\EndIf
					\EndFor
				\EndIf
				\State $D_{rand\_source}$ = \{\}
				\State Select random samples from $D_{source}$ and add to $D_{rand\_source}$ such as $\lvert D_{rand\_source} \rvert = \lvert D_{pseudo} \rvert$
				\State Train epoch on $D_{rand\_source}$
				\State Train epoch on $D_{pseudo}$
			\EndFor
			
		\end{algorithmic} 
	\end{algorithm}

	\printbibliography
\end{document}
