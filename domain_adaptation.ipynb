{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "VAL_SPLIT = .2\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def resnet_preprocessor(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses an image for ResNet model.\n",
    "\n",
    "    :param numpy.ndarray image: The input image.\n",
    "    :return: Preprocessed image.\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    # assume RGB dimension, either as first or last dimension\n",
    "    #assert image.shape[0] == 3 or image.shape[-1] == 3\n",
    "\n",
    "    #if image.shape[-1] == 3:\n",
    "        #image = np.moveaxis(image, -1, 0)\n",
    "        #print(image.shape)\n",
    "\n",
    "    preprocess = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = preprocess(image)\n",
    "    #image = image.unsqueeze(0) #mini-batch creation\n",
    "    return image\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Lazily loads images from a root directory.\n",
    "    Directory is assumed to be of shape \"<root>/<class_name>/<instance_file>\".\n",
    "    Allows custom functions for reading, preprocessing each image and setting the label encodings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        parser_func: Callable = imageio.imread,\n",
    "        preprocessing_func: Callable[[np.ndarray], np.ndarray] = resnet_preprocessor,\n",
    "        label_encoder=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ImageDataset.\n",
    "\n",
    "        :param str data_dir: Root directory containing the dataset.\n",
    "        :param parser_func: Function to parse images.\n",
    "        :type parser_func: Callable, optional\n",
    "        :param preprocessing_func: Function to preprocess images.\n",
    "        :type preprocessing_func: Callable[[numpy.ndarray], numpy.ndarray], optional\n",
    "        :param label_encoder: Encoder for label encoding.\n",
    "        :type label_encoder: sklearn.preprocessing.LabelEncoder or None, optional\n",
    "        \"\"\"\n",
    "        self.parser_func = parser_func\n",
    "        self.preprocessing_func = preprocessing_func\n",
    "        self.label_encoder = label_encoder\n",
    "        self.samples = self._load_dataset_paths(data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = self.parser_func(image_path)\n",
    "        image = self.preprocessing_func(image)\n",
    "\n",
    "        if not torch.is_tensor(image):\n",
    "            image = torch.tensor(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def _load_dataset_paths(self, data_dir):\n",
    "        \"\"\"\n",
    "        Loads paths of images in the dataset.\n",
    "\n",
    "        :param str data_dir: Root directory containing the dataset.\n",
    "        :return: List of tuples containing image paths and their corresponding labels.\n",
    "        :rtype: List[Tuple[str, int]]\n",
    "        \"\"\"\n",
    "        class_names = os.listdir(data_dir)\n",
    "\n",
    "        if self.label_encoder is None:\n",
    "            self.label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "            self.label_encoder.fit(class_names)\n",
    "\n",
    "        samples = []\n",
    "        for class_name in tqdm(class_names):\n",
    "            class_data_dir = os.path.join(data_dir, class_name)\n",
    "\n",
    "            for file_name in os.listdir(class_data_dir):\n",
    "                samples.append(\n",
    "                    (\n",
    "                        os.path.join(class_data_dir, file_name),\n",
    "                        self.label_encoder.transform([class_name])[0],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Sort the batch by image height in descending order\n",
    "    batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
    "\n",
    "    # Get the maximum height and width among all images in the batch\n",
    "    max_height = max(img.shape[1] for img, _ in batch)\n",
    "    max_width = max(img.shape[2] for img, _ in batch)\n",
    "\n",
    "    # Pad each image to match the maximum height and width\n",
    "    padded_batch = []\n",
    "    for img, label in batch:\n",
    "        # Calculate padding sizes\n",
    "        pad_height = max_height - img.shape[1]\n",
    "        pad_width = max_width - img.shape[2]\n",
    "\n",
    "        # Pad the image\n",
    "        padded_img = torch.nn.functional.pad(img, (0, pad_width, 0, pad_height))\n",
    "\n",
    "        padded_batch.append((padded_img, label))\n",
    "\n",
    "    # Stack images and labels into tensors\n",
    "    images = torch.stack([img for img, _ in padded_batch])\n",
    "    labels = torch.tensor([label for _, label in padded_batch])\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db7a21d137a4ac08375b9c084ac4b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "dataset = ImageDataset(os.path.join(DATA_DIR, \"amazon\"))\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(VAL_SPLIT * dataset_size))\n",
    "\n",
    "np.random.default_rng(RANDOM_SEED).shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=custom_collate\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=valid_sampler, collate_fn=custom_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "sample_image, label = dataset[0]\n",
    "print(sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2817\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device: str,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    train_size: int,\n",
    "    val_size: int,\n",
    "    num_epochs: int = 25,\n",
    "):\n",
    "\n",
    "    dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "    dataset_sizes = {\"train\": train_size, \"val\": val_size}\n",
    "\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
    "            print(\"-\" * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                if phase == \"train\":\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == \"train\":\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == \"val\" and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\n",
    "            f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\"\n",
    "        )\n",
    "        print(f\"Best val Acc: {best_acc:4f}\")\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c248dae02940de95fa7671dcc520e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "    \"pytorch/vision:v0.10.0\", \"resnet34\", weights=torchvision.models.ResNet34_Weights.DEFAULT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model.parameters())\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer_ft,\n",
    "    exp_lr_scheduler,\n",
    "    device,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    len(train_indices),\n",
    "    len(val_indices),\n",
    "    num_epochs=25,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
