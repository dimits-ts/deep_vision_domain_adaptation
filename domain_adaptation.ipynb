{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import tasks.data\n",
    "import tasks.torch_train_eval\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "SOURCE_DATASET = \"amazon\"\n",
    "VAL_SPLIT = .15\n",
    "TEST_SPLIT = .1\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def resnet_preprocessor(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses an image for ResNet model.\n",
    "\n",
    "    :param numpy.ndarray image: The input image.\n",
    "    :return: Preprocessed image.\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    preprocess = torchvision.transforms.Compose(\n",
    "        [    \n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.uint8, scale=True),  # optional, most input are already uint8 at this point\n",
    "            #v2.RandomResizedCrop(size=(224, 224), antialias=True),  # Or Resize(antialias=True)\n",
    "            v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "            v2.ToTensor(),\n",
    "            v2.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = preprocess(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def image_read_func(image_path):\n",
    "    return imageio.imread(image_path, pilmode='RGB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc7b8f4a89e47988670077eb1b0f783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_dataset = tasks.data.ImageDataset(\n",
    "    os.path.join(DATA_DIR, SOURCE_DATASET),\n",
    "    parser_func=image_read_func,\n",
    "    preprocessing_func=resnet_preprocessor,\n",
    ")\n",
    "\n",
    "source_train_loader, source_val_loader, source_test_loader = (\n",
    "    tasks.data.train_val_test_loaders(\n",
    "        source_dataset, BATCH_SIZE, VAL_SPLIT, TEST_SPLIT, tasks.data.collate_pad\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def try_load_weights(model, weights_path: str):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(weights_path))\n",
    "    except Exception as e:\n",
    "        print(\"Cannot load proper weights: \", e)\n",
    "    return model\n",
    "\n",
    "\n",
    "def try_load_history(history_path):\n",
    "    try:\n",
    "        with open(history_path, 'rb') as handle:\n",
    "            history = pickle.load(handle)\n",
    "    except:\n",
    "        print(\"No history found in path \", history_path)\n",
    "        history = None\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [3, 1000]                 --\n",
       "├─Conv2d: 1-1                            [3, 64, 150, 150]         9,408\n",
       "├─BatchNorm2d: 1-2                       [3, 64, 150, 150]         128\n",
       "├─ReLU: 1-3                              [3, 64, 150, 150]         --\n",
       "├─MaxPool2d: 1-4                         [3, 64, 75, 75]           --\n",
       "├─Sequential: 1-5                        [3, 64, 75, 75]           --\n",
       "│    └─BasicBlock: 2-1                   [3, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-1                  [3, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [3, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-3                    [3, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-4                  [3, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [3, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-6                    [3, 64, 75, 75]           --\n",
       "│    └─BasicBlock: 2-2                   [3, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-7                  [3, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [3, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-9                    [3, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-10                 [3, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [3, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-12                   [3, 64, 75, 75]           --\n",
       "├─Sequential: 1-6                        [3, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-3                   [3, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-13                 [3, 128, 38, 38]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [3, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-15                   [3, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-16                 [3, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [3, 128, 38, 38]          256\n",
       "│    │    └─Sequential: 3-18             [3, 128, 38, 38]          8,448\n",
       "│    │    └─ReLU: 3-19                   [3, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-4                   [3, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-20                 [3, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [3, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-22                   [3, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-23                 [3, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [3, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-25                   [3, 128, 38, 38]          --\n",
       "├─Sequential: 1-7                        [3, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-5                   [3, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-26                 [3, 256, 19, 19]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [3, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-28                   [3, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-29                 [3, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [3, 256, 19, 19]          512\n",
       "│    │    └─Sequential: 3-31             [3, 256, 19, 19]          33,280\n",
       "│    │    └─ReLU: 3-32                   [3, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-6                   [3, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-33                 [3, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [3, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-35                   [3, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-36                 [3, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [3, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-38                   [3, 256, 19, 19]          --\n",
       "├─Sequential: 1-8                        [3, 512, 10, 10]          --\n",
       "│    └─BasicBlock: 2-7                   [3, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-39                 [3, 512, 10, 10]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [3, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-41                   [3, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-42                 [3, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [3, 512, 10, 10]          1,024\n",
       "│    │    └─Sequential: 3-44             [3, 512, 10, 10]          132,096\n",
       "│    │    └─ReLU: 3-45                   [3, 512, 10, 10]          --\n",
       "│    └─BasicBlock: 2-8                   [3, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-46                 [3, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [3, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-48                   [3, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-49                 [3, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [3, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-51                   [3, 512, 10, 10]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [3, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [3, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.18\n",
       "==========================================================================================\n",
       "Input size (MB): 3.24\n",
       "Forward/backward pass size (MB): 217.09\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 267.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, SOURCE_DATASET)\n",
    "\n",
    "model = torch.hub.load(\n",
    "    \"pytorch/vision:v0.10.0\", \"resnet18\", weights=\"DEFAULT\"\n",
    ").to(device)\n",
    "model = try_load_weights(model, os.path.join(output_path, \"model.pt\"))\n",
    "\n",
    "torchinfo.summary(model, input_size=(BATCH_SIZE, 3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ac15467f7040f08e88d4e2e8026de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimits/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.0005)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "previous_history = try_load_history(os.path.join(output_path, \"history.pickle\"))\n",
    "\n",
    "model, history = tasks.torch_train_eval.train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer_ft,\n",
    "    exp_lr_scheduler,\n",
    "    device,\n",
    "    source_train_loader,\n",
    "    source_val_loader,\n",
    "    output_dir=output_path,\n",
    "    num_epochs=25,\n",
    "    patience=6,\n",
    "    previous_history=previous_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = tasks.torch_train_eval.test(model, source_test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "class_names = source_dataset.label_encoder.classes_\n",
    "print(\n",
    "    sklearn.metrics.classification_report(\n",
    "        actual, predicted, zero_division=0, target_names=class_names\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://towardsdatascience.com/neural-network-calibration-using-pytorch-c44b7221a61\n",
    "def T_scaling(logits, temperature):\n",
    "    return torch.div(logits, temperature)\n",
    "\n",
    "\n",
    "temperature = nn.Parameter(torch.ones(1).cuda())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.LBFGS([temperature], lr=0.001, max_iter=10000, line_search_fn='strong_wolfe')\n",
    "\n",
    "logits_list = []\n",
    "labels_list = []\n",
    "\n",
    "for i, data in enumerate(tqdm(val_loader, 0)):\n",
    "    images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      logits_list.append(net(images))\n",
    "      labels_list.append(labels)\n",
    "\n",
    "# Create tensors\n",
    "logits_list = torch.cat(logits_list).to(device)\n",
    "labels_list = torch.cat(labels_list).to(device)\n",
    "\n",
    "def _eval():\n",
    "  loss = criterion(T_scaling(logits_list, temperature), labels_list)\n",
    "  loss.backward()\n",
    "  return loss\n",
    "\n",
    "optimizer.step(_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
