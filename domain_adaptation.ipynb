{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Pseudo-labeling for Domain Adaptation in Deep Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Office Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lib.data\n",
    "import lib.torch_train_eval\n",
    "import lib.adaptive_train_eval\n",
    "\n",
    "import tasks.preprocessing\n",
    "import tasks.utils\n",
    "import tasks.results\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "OUTPUT_DIR = \"output/office\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 1\n",
    "PRINT_STATS_PERIOD = 500\n",
    "\n",
    "SOURCE_DATASET = \"amazon\"\n",
    "SOURCE_VAL_SPLIT = .15\n",
    "SOURCE_TEST_SPLIT = .1\n",
    "\n",
    "\n",
    "TARGET_VAL_SPLIT = .15\n",
    "TARGET_TEST_SPLIT = .15\n",
    "TARGET_DATASET = \"webcam\"\n",
    "\n",
    "RHO = 4\n",
    "SAMPLING_PERIOD = 20\n",
    "\n",
    "\n",
    "FINETUNED_SOURCE__MODEL_DIR = os.path.join(OUTPUT_DIR, \"classifier\")\n",
    "FINETUNED_TARGET_MODEL_DIR = os.path.join(OUTPUT_DIR, \"target_classifier\")\n",
    "UNSUPERVISED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"unsupervised\")\n",
    "SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20 = os.path.join(OUTPUT_DIR, \"semi-supervised-finetuned-20\")\n",
    "SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20 = os.path.join(OUTPUT_DIR, \"semi-supervised-adaptive-20\")\n",
    "SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10 = os.path.join(OUTPUT_DIR, \"semi-supervised-finetuned-10\")\n",
    "SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10 = os.path.join(OUTPUT_DIR, \"semi-supervised-adaptive-10\")\n",
    "\n",
    "FINETUNE_SOURCE_MODEL = False\n",
    "FINETUNE_TARGET_MODEL = True\n",
    "TRAIN_UNSUPERVISED_MODEL = False\n",
    "FINETUNE_SEMI_SUPERVISED_MODEL_20 = False\n",
    "TRAIN_SEMI_SUPERVISED_MODEL_20 = False\n",
    "FINETUNE_SEMI_SUPERVISED_MODEL_10 = False\n",
    "TRAIN_SEMI_SUPERVISED_MODEL_10 = False\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1f153ab98645a28ed4748d9fbb60b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "source_dataset.load_from_directory(os.path.join(DATA_DIR, SOURCE_DATASET))\n",
    "\n",
    "source_train_dataset, source_val_dataset, source_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        source_dataset, SOURCE_VAL_SPLIT, SOURCE_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "source_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_train_dataset, shuffle=True\n",
    ")\n",
    "source_val_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_val_dataset, shuffle=False\n",
    ")\n",
    "source_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976adafddbf74c58b3efe1aa5665c7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    "    label_encoder=source_dataset.label_encoder,  # use same classes\n",
    ")\n",
    "target_dataset.load_from_directory(os.path.join(DATA_DIR, TARGET_DATASET))\n",
    "\n",
    "target_train_dataset, target_val_dataset, target_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        target_dataset, TARGET_VAL_SPLIT, TARGET_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "target_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_train_dataset, shuffle=True\n",
    ")\n",
    "target_val_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_val_dataset, shuffle=False\n",
    ")\n",
    "target_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "unlabeled_dataset_20.load_from_image_dataset(target_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 2112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_unlabeled_dataset_20, labeled_dataset_20 = lib.data.stratified_split(\n",
    "    target_train_dataset, test_size=0.2\n",
    ")\n",
    "\n",
    "unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=labeled_dataset_20.parser_func,\n",
    "    preprocessing_func=labeled_dataset_20.preprocessing_func,\n",
    ")\n",
    "unlabeled_dataset_20.load_from_image_dataset(to_be_unlabeled_dataset_20)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in source_train_dataset.samples:\n",
    "    labeled_dataset_20.add(sample_img, sample_label)\n",
    "\n",
    "len(labeled_dataset_20), len(source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168, 2112)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_unlabeled_dataset_10, labeled_dataset_10 = lib.data.stratified_split(\n",
    "    target_train_dataset, test_size=0.1\n",
    ")\n",
    "\n",
    "unlabeled_dataset_10 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=labeled_dataset_10.parser_func,\n",
    "    preprocessing_func=labeled_dataset_20.preprocessing_func,\n",
    ")\n",
    "unlabeled_dataset_10.load_from_image_dataset(to_be_unlabeled_dataset_10)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in source_train_dataset.samples:\n",
    "    labeled_dataset_10.add(sample_img, sample_label)\n",
    "\n",
    "len(labeled_dataset_10), len(source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = source_train_dataset.label_encoder.classes_\n",
    "\n",
    "encodings = {\n",
    "    label: class_name\n",
    "    for label, class_name in enumerate(source_train_dataset.label_encoder.classes_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1000]                 --\n",
       "├─Conv2d: 1-1                            [1, 64, 750, 750]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 750, 750]         128\n",
       "├─ReLU: 1-3                              [1, 64, 750, 750]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 375, 375]         --\n",
       "├─Sequential: 1-5                        [1, 64, 375, 375]         --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 375, 375]         --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 375, 375]         --\n",
       "├─Sequential: 1-6                        [1, 128, 188, 188]        --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 188, 188]        73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 188, 188]        256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 188, 188]        147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 188, 188]        256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 188, 188]        8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 188, 188]        --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 188, 188]        147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 188, 188]        256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 188, 188]        147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 188, 188]        256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 188, 188]        --\n",
       "├─Sequential: 1-7                        [1, 256, 94, 94]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 94, 94]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 94, 94]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 94, 94]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 94, 94]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 94, 94]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 94, 94]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 94, 94]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 94, 94]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 94, 94]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 94, 94]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 94, 94]          --\n",
       "├─Sequential: 1-8                        [1, 512, 47, 47]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 47, 47]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 47, 47]          1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 47, 47]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 47, 47]          1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 47, 47]          132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 47, 47]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 47, 47]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 47, 47]          1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 47, 47]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 47, 47]          1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 47, 47]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 81.62\n",
       "==========================================================================================\n",
       "Input size (MB): 27.00\n",
       "Forward/backward pass size (MB): 1785.37\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 1859.13\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "def get_model(replace_fc_layer: bool=False, num_classes: int=0) -> nn.Module:\n",
    "    model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet18\", weights=\"DEFAULT\").to(\n",
    "        device\n",
    "    )\n",
    "    if replace_fc_layer:\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes).to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "#https://arxiv.org/pdf/2405.13698\n",
    "optimizer_ft = optim.Adam(model.parameters())\n",
    "# disable lr for adam\n",
    "exp_lr_scheduler = None\n",
    "\n",
    "\n",
    "torchinfo.summary(model, input_size=(BATCH_SIZE, 3, 1500, 1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if FINETUNE_SOURCE_MODEL:\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        source_train_loader,\n",
    "        source_val_loader,\n",
    "        output_dir=FINETUNED_SOURCE__MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FINETUNE_TARGET_MODEL:\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mget\u001b[49m\n\u001b[1;32m      3\u001b[0m     history \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtry_load_history(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FINETUNED_TARGET_MODEL_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtry_load_weights(model, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FINETUNED_TARGET_MODEL_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get' is not defined"
     ]
    }
   ],
   "source": [
    "if FINETUNE_TARGET_MODEL:\n",
    "    model = get_model(replace_fc_layer=True, num_classes=len(encodings))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=10e-6)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        target_train_loader,\n",
    "        target_val_loader,\n",
    "        output_dir=FINETUNED_TARGET_MODEL_DIR,\n",
    "        num_epochs=100,\n",
    "        patience=10,\n",
    "        warmup_period=25,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=20000,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Domain Adaptation\n",
    "\n",
    "https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af\n",
    "\n",
    "https://stats.stackexchange.com/questions/364584/why-does-using-pseudo-labeling-non-trivially-affect-the-results\n",
    "\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S1077314222001102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if TRAIN_UNSUPERVISED_MODEL:\n",
    "    model = get_model()\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(UNSUPERVISED_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "    \n",
    "    source_history = tasks.utils.try_load_history(os.path.join(UNSUPERVISED_MODEL_DIR, \"source_history.pickle\"))\n",
    "    target_history = tasks.utils.try_load_history(os.path.join(UNSUPERVISED_MODEL_DIR, \"target_history.pickle\"))\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset_20,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=1, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_20,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=UNSUPERVISED_MODEL_DIR,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, UNSUPERVISED_MODEL_DIR)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised domain adaptation: 20% target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FINETUNE_SEMI_SUPERVISED_MODEL_20:\n",
    "    model = get_model()\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            labeled_dataset_20, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=source_val_loader,\n",
    "        output_dir=SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        previous_history=history\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.2)\n",
    "\n",
    "if TRAIN_SEMI_SUPERVISED_MODEL_20:\n",
    "    model = get_model()\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    source_history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20, \"source_history.pickle\"))\n",
    "    target_history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20, \"target_history.pickle\"))\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset_20,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, sampler=sampler, shuffle=False\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_20,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised domain adaptation: 10% target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FINETUNE_SEMI_SUPERVISED_MODEL_10:\n",
    "    model = get_model()\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            labeled_dataset_10, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=source_val_loader,\n",
    "        output_dir=SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        previous_history=None,\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_SEMI_SUPERVISED_MODEL_10:\n",
    "    model = get_model()\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    source_history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10, \"source_history.pickle\"))\n",
    "    target_history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10, \"target_history.pickle\"))\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset_10,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=1, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_10,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=None,\n",
    "            previous_target_history=None,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptiope dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "PRINT_STATS_PERIOD = 500\n",
    "\n",
    "AD_DATA_DIR = \"data/adaptiope\"\n",
    "AD_OUTPUT_DIR = \"output/adaptiope\"\n",
    "\n",
    "AD_SOURCE_DATASET = \"product_images\"\n",
    "AD_TARGET_DATASET = \"real_life\"\n",
    "\n",
    "AD_FINETUNED_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"classifier\")\n",
    "AD_UNSUPERVISED_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"unsupervised\")\n",
    "AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-finetuned-20\")\n",
    "AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-adaptive-20\")\n",
    "AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-finetuned-10\")\n",
    "AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-adaptive-10\")\n",
    "\n",
    "AD_FINETUNE_MODEL = False\n",
    "AD_TRAIN_UNSUPERVISED_MODEL = False\n",
    "AD_FINETUNE_SEMI_SUPERVISED_MODEL_20 = False\n",
    "AD_TRAIN_SEMI_SUPERVISED_MODEL_20 = False\n",
    "AD_FINETUNE_SEMI_SUPERVISED_MODEL_10 = False\n",
    "AD_TRAIN_SEMI_SUPERVISED_MODEL_10 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_source_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "ad_source_dataset.load_from_directory(os.path.join(AD_DATA_DIR, AD_SOURCE_DATASET))\n",
    "\n",
    "ad_source_train_dataset, ad_source_val_dataset, ad_source_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        ad_source_dataset, SOURCE_VAL_SPLIT, SOURCE_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "ad_source_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    ad_source_train_dataset, shuffle=True\n",
    ")\n",
    "ad_source_val_loader = tasks.preprocessing.single_batch_loader(\n",
    "    ad_source_val_dataset, shuffle=False\n",
    ")\n",
    "ad_source_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    ad_source_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_target_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    "    label_encoder=ad_source_dataset.label_encoder,  # use same classes\n",
    ")\n",
    "ad_target_dataset.load_from_directory(os.path.join(AD_DATA_DIR, AD_TARGET_DATASET))\n",
    "\n",
    "ad_target_train_dataset, ad_target_val_dataset, ad_target_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        ad_target_dataset, TARGET_VAL_SPLIT, TARGET_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "ad_target_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    ad_target_train_dataset, shuffle=True\n",
    ")\n",
    "ad_target_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    ad_target_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ad_source_train_dataset.label_encoder.classes_\n",
    "\n",
    "encodings = {\n",
    "    label: class_name\n",
    "    for label, class_name in enumerate(ad_source_train_dataset.label_encoder.classes_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet18\", weights=\"DEFAULT\").to(\n",
    "    device\n",
    ")\n",
    "model.fc = nn.Linear(model.fc.in_features, len(set(encodings))).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "#https://arxiv.org/pdf/2405.13698\n",
    "optimizer_ft = optim.AdamW(model.parameters(), weight_decay=0.0005)\n",
    "# disable lr for adam\n",
    "exp_lr_scheduler = None\n",
    "\n",
    "\n",
    "torchinfo.summary(model, input_size=(BATCH_SIZE, 3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AD_FINETUNE_MODEL:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\"))\n",
    "        \n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        ad_source_train_loader,\n",
    "        ad_source_val_loader,\n",
    "        output_dir=AD_FINETUNED_MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        gradient_accumulation=3,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(range(len(history[\"train_loss\"]))), history[\"train_loss\"])\n",
    "plt.plot(np.array(range(len(history[\"val_loss\"]))), history[\"val_loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[\"train_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation accuracy has been calculated wrong here, ignore it for now\n",
    "plt.plot(np.array(range(len(history[\"train_acc\"]))), history[\"train_acc\"])\n",
    "plt.plot(np.array(range(len(history[\"val_acc\"]))), history[\"val_acc\"])\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
