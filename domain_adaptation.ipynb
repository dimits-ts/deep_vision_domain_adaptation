{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tasks.data\n",
    "import tasks.torch_train_eval\n",
    "import tasks.adaptive_train_eval\n",
    "import tasks.calibration\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "SOURCE_DATASET = \"amazon\"\n",
    "SOURCE_VAL_SPLIT = .15\n",
    "SOURCE_TEST_SPLIT = .1\n",
    "\n",
    "TARGET_VAL_SPLIT = .15\n",
    "TARGET_TEST_SPLIT = .15\n",
    "TARGET_DATASET = \"webcam\"\n",
    "\n",
    "FINETUNED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"classifier\")\n",
    "UNSUPERVISED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"unsupervised\")\n",
    "SEMI_SUPERVISED_FINETUNED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"semi-supervised-finetuned\")\n",
    "SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR = os.path.join(OUTPUT_DIR, \"semi-supervised-adaptive\")\n",
    "\n",
    "FINETUNE_MODEL = False\n",
    "TRAIN_UNSUPERVISED_MODEL = False\n",
    "FINETUNE_SEMI_SUPERVISED_MODEL = False\n",
    "TRAIN_SEMI_SUPERVISED_MODEL = True\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Office Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def resnet_preprocessor(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses an image for ResNet model.\n",
    "\n",
    "    :param numpy.ndarray image: The input image.\n",
    "    :return: Preprocessed image.\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    preprocess = torchvision.transforms.Compose(\n",
    "        [    \n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "            v2.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = preprocess(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def image_read_func(image_path):\n",
    "    return imageio.imread(image_path, pilmode='RGB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a5b28bafdb47e89cc890fd1fca85e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_dataset = tasks.data.ImageDataset(\n",
    "    parser_func=image_read_func,\n",
    "    preprocessing_func=resnet_preprocessor,\n",
    ")\n",
    "source_dataset.load_from_directory(os.path.join(DATA_DIR, SOURCE_DATASET))\n",
    "\n",
    "source_train_dataset, source_val_dataset, source_test_dataset = tasks.data.train_val_test_split(\n",
    "    source_dataset, SOURCE_VAL_SPLIT, SOURCE_TEST_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_dataloader(\n",
    "    dataset: tasks.data.ImageDataset,\n",
    "    shuffle: bool = True,\n",
    "    sampler = None\n",
    "):\n",
    "    # sampler and shuffle are mutually exclusive\n",
    "    if sampler is None:\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=tasks.data.collate_pad,\n",
    "        )\n",
    "    else:\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sampler=sampler,\n",
    "            collate_fn=tasks.data.collate_pad,\n",
    "        )\n",
    "\n",
    "\n",
    "source_train_loader = create_padded_dataloader(source_train_dataset, shuffle=True)\n",
    "source_val_loader = create_padded_dataloader(source_val_dataset, shuffle=False)\n",
    "source_test_loader = create_padded_dataloader(source_test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def try_load_weights(model, weights_path: str):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(weights_path))\n",
    "    except:\n",
    "        print(\"No weights found in path \", weights_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def try_load_history(history_path):\n",
    "    try:\n",
    "        with open(history_path, 'rb') as handle:\n",
    "            history = pickle.load(handle)\n",
    "    except:\n",
    "        print(\"No history found in path \", history_path)\n",
    "        history = None\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b97c17f952499bbcab960f2478949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_dataset = tasks.data.ImageDataset(\n",
    "    parser_func=image_read_func,\n",
    "    preprocessing_func=resnet_preprocessor,\n",
    "    label_encoder=source_dataset.label_encoder # use same classes\n",
    ")\n",
    "target_dataset.load_from_directory(os.path.join(DATA_DIR, TARGET_DATASET))\n",
    "\n",
    "target_train_dataset, target_val_dataset, target_test_dataset = train_val_test_split(\n",
    "    target_dataset, TARGET_VAL_SPLIT, TARGET_TEST_SPLIT\n",
    ")\n",
    "\n",
    "target_train_loader = create_padded_dataloader(target_train_dataset, shuffle=True)\n",
    "target_test_loader = create_padded_dataloader(target_test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = tasks.data.UnlabeledImageDataset(\n",
    "    parser_func=image_read_func,\n",
    "    preprocessing_func=resnet_preprocessor,\n",
    ")\n",
    "unlabeled_dataset.load_from_image_dataset(target_train_dataset)\n",
    "\n",
    "source_history = try_load_history(os.path.join(UNSUPERVISED_MODEL_DIR, \"source_history.pickle\"))\n",
    "target_history = try_load_history(os.path.join(UNSUPERVISED_MODEL_DIR, \"target_history.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556, 112, 556)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_unlabeled_dataset, labeled_dataset = tasks.data.stratified_split(\n",
    "    target_train_dataset, test_size=0.2\n",
    ")\n",
    "\n",
    "len(unlabeled_dataset), len(labeled_dataset), len(target_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 2112)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_dataset = tasks.data.UnlabeledImageDataset(\n",
    "    parser_func=labeled_dataset.parser_func,\n",
    "    preprocessing_func=labeled_dataset.preprocessing_func,\n",
    ")\n",
    "unlabeled_dataset.load_from_image_dataset(to_be_unlabeled_dataset)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in source_train_dataset.samples:\n",
    "    labeled_dataset.add(sample_img, sample_label)\n",
    "\n",
    "len(labeled_dataset), len(source_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.0005)\n",
    "# disable lr for adam\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=100000, gamma=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [2, 1000]                 --\n",
       "├─Conv2d: 1-1                            [2, 64, 150, 150]         9,408\n",
       "├─BatchNorm2d: 1-2                       [2, 64, 150, 150]         128\n",
       "├─ReLU: 1-3                              [2, 64, 150, 150]         --\n",
       "├─MaxPool2d: 1-4                         [2, 64, 75, 75]           --\n",
       "├─Sequential: 1-5                        [2, 64, 75, 75]           --\n",
       "│    └─BasicBlock: 2-1                   [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-1                  [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-3                    [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-4                  [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-6                    [2, 64, 75, 75]           --\n",
       "│    └─BasicBlock: 2-2                   [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-7                  [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-9                    [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-10                 [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-12                   [2, 64, 75, 75]           --\n",
       "├─Sequential: 1-6                        [2, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-3                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-13                 [2, 128, 38, 38]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-15                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-16                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [2, 128, 38, 38]          256\n",
       "│    │    └─Sequential: 3-18             [2, 128, 38, 38]          8,448\n",
       "│    │    └─ReLU: 3-19                   [2, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-4                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-20                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-22                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-23                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-25                   [2, 128, 38, 38]          --\n",
       "├─Sequential: 1-7                        [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-5                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-26                 [2, 256, 19, 19]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-28                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-29                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [2, 256, 19, 19]          512\n",
       "│    │    └─Sequential: 3-31             [2, 256, 19, 19]          33,280\n",
       "│    │    └─ReLU: 3-32                   [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-6                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-33                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-35                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-36                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-38                   [2, 256, 19, 19]          --\n",
       "├─Sequential: 1-8                        [2, 512, 10, 10]          --\n",
       "│    └─BasicBlock: 2-7                   [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-39                 [2, 512, 10, 10]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-41                   [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-42                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [2, 512, 10, 10]          1,024\n",
       "│    │    └─Sequential: 3-44             [2, 512, 10, 10]          132,096\n",
       "│    │    └─ReLU: 3-45                   [2, 512, 10, 10]          --\n",
       "│    └─BasicBlock: 2-8                   [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-46                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-48                   [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-49                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-51                   [2, 512, 10, 10]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [2, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [2, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.79\n",
       "==========================================================================================\n",
       "Input size (MB): 2.16\n",
       "Forward/backward pass size (MB): 144.73\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 193.65\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "def get_default_model():\n",
    "    return torch.hub.load(\n",
    "        \"pytorch/vision:v0.10.0\", \"resnet18\", weights=\"DEFAULT\"\n",
    "    ).to(device)\n",
    "\n",
    "model = get_default_model()\n",
    "\n",
    "torchinfo.summary(model, input_size=(BATCH_SIZE, 3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "history = try_load_history(os.path.join(FINETUNED_MODEL_DIR, \"history.pickle\"))\n",
    "\n",
    "\n",
    "if FINETUNE_MODEL:\n",
    "    model, history = tasks.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        source_train_loader,\n",
    "        source_val_loader,\n",
    "        output_dir=FINETUNED_MODEL_DIR,\n",
    "        num_epochs=1,\n",
    "        patience=5,\n",
    "        warmup_period=1,\n",
    "        previous_history=history\n",
    "    )\n",
    "else:\n",
    "    model = try_load_weights(model, os.path.join(FINETUNED_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(range(len(history[\"train_loss\"]))), history[\"train_loss\"])\n",
    "plt.plot(np.array(range(len(history[\"val_loss\"]))), history[\"val_loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# validation accuracy has been calculated wrong here, ignore it for now\n",
    "plt.plot(np.array(range(len(history[\"train_acc\"]))), history[\"train_acc\"])\n",
    "plt.plot(np.array(range(len(history[\"val_acc\"]))), history[\"val_acc\"])\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# save logits for later calibration\n",
    "source_actual, source_predicted = tasks.torch_train_eval.test(\n",
    "    model, source_test_loader, device\n",
    ")\n",
    "class_names = source_dataset.label_encoder.classes_\n",
    "\n",
    "print(\n",
    "    sklearn.metrics.classification_report(\n",
    "        source_actual,\n",
    "        source_predicted,\n",
    "        zero_division=0,\n",
    "        target_names=class_names,\n",
    "        labels=np.arange(0, len(class_names), 1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "source_cf_matrix = sklearn.metrics.confusion_matrix(source_actual, source_predicted)\n",
    "display = sklearn.metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=source_cf_matrix, display_labels=class_names\n",
    ")\n",
    "display.plot()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "target_actual, target_predicted = tasks.torch_train_eval.test(model, target_test_loader, device)\n",
    "class_names = target_dataset.label_encoder.classes_\n",
    "\n",
    "print(\n",
    "    sklearn.metrics.classification_report(\n",
    "        target_actual,\n",
    "        target_predicted,\n",
    "        zero_division=0,\n",
    "        target_names=class_names,\n",
    "        labels=np.arange(0, len(class_names), 1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "target_cf_matrix = sklearn.metrics.confusion_matrix(target_actual, target_predicted, labels=np.arange(0, len(class_names), 1))\n",
    "display = sklearn.metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=target_cf_matrix, display_labels=class_names\n",
    ")\n",
    "display.plot()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Domain Adaptation\n",
    "\n",
    "https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af\n",
    "\n",
    "https://stats.stackexchange.com/questions/364584/why-does-using-pseudo-labeling-non-trivially-affect-the-results\n",
    "\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S1077314222001102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if TRAIN_UNSUPERVISED_MODEL:\n",
    "    model, source_history, target_history = (\n",
    "        tasks.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=source_train_dataset,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: create_padded_dataloader(\n",
    "                dataset,\n",
    "                sampler=sampler,\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=1, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=UNSUPERVISED_MODEL_DIR,\n",
    "            num_epochs=20,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(range(len(target_history[\"train_acc\"]))), target_history[\"train_acc\"])\n",
    "plt.plot(np.array(range(len(target_history[\"val_acc\"]))), target_history[\"val_acc\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_actual, target_predicted = tasks.torch_train_eval.test(model, target_test_loader, device)\n",
    "class_names = target_dataset.label_encoder.classes_\n",
    "\n",
    "print(\n",
    "    sklearn.metrics.classification_report(\n",
    "        target_actual,\n",
    "        target_predicted,\n",
    "        zero_division=0,\n",
    "        target_names=class_names,\n",
    "        labels=np.arange(0, len(class_names), 1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cf_matrix = sklearn.metrics.confusion_matrix(target_actual, target_predicted, labels=np.arange(0, len(class_names), 1))\n",
    "display = sklearn.metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=target_cf_matrix, display_labels=class_names\n",
    ")\n",
    "display.plot()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FINETUNE_SEMI_SUPERVISED_MODEL:\n",
    "    print(\"Starting fine-tuning on mixed dataset...\")\n",
    "    model, history = tasks.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        #train_dataloader=source_train_loader,\n",
    "        train_dataloader=create_padded_dataloader(labeled_dataset, shuffle=True),\n",
    "        val_dataloader=source_val_loader,\n",
    "        output_dir=SEMI_SUPERVISED_FINETUNED_MODEL_DIR,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        previous_history=history\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pseudo-labeling task...\n",
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb81c1d6e356405099f81997d9bbbd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c58b0b6b064f9698e8e5abfae6e3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 169/444 images to be included in next epoch\n",
      "[('data/office/webcam/punchers/frame_0025.jpg', 23), ('data/office/webcam/ring_binder/frame_0037.jpg', 30), ('data/office/webcam/paper_notebook/frame_0010.jpg', 18), ('data/office/webcam/monitor/frame_0038.jpg', 30), ('data/office/webcam/headphones/frame_0018.jpg', 10), ('data/office/webcam/mobile_phone/frame_0009.jpg', 16), ('data/office/webcam/file_cabinet/frame_0008.jpg', 9), ('data/office/webcam/monitor/frame_0010.jpg', 15), ('data/office/webcam/ring_binder/frame_0014.jpg', 30), ('data/office/webcam/pen/frame_0004.jpg', 19), ('data/office/webcam/phone/frame_0012.jpg', 20), ('data/office/webcam/printer/frame_0002.jpg', 23), ('data/office/webcam/punchers/frame_0008.jpg', 28), ('data/office/webcam/monitor/frame_0015.jpg', 15), ('data/office/webcam/mouse/frame_0004.jpg', 16), ('data/office/webcam/desk_lamp/frame_0010.jpg', 8), ('data/office/webcam/stapler/frame_0017.jpg', 23), ('data/office/webcam/punchers/frame_0018.jpg', 28), ('data/office/webcam/punchers/frame_0021.jpg', 28), ('data/office/webcam/calculator/frame_0018.jpg', 12), ('data/office/webcam/bookcase/frame_0001.jpg', 3), ('data/office/webcam/mouse/frame_0011.jpg', 22), ('data/office/webcam/laptop_computer/frame_0016.jpg', 12), ('data/office/webcam/calculator/frame_0010.jpg', 12), ('data/office/webcam/mouse/frame_0026.jpg', 16), ('data/office/webcam/monitor/frame_0021.jpg', 15), ('data/office/webcam/headphones/frame_0007.jpg', 10), ('data/office/webcam/laptop_computer/frame_0022.jpg', 12), ('data/office/webcam/back_pack/frame_0012.jpg', 0), ('data/office/webcam/printer/frame_0005.jpg', 21), ('data/office/webcam/ring_binder/frame_0018.jpg', 30), ('data/office/webcam/bookcase/frame_0002.jpg', 3), ('data/office/webcam/phone/frame_0008.jpg', 20), ('data/office/webcam/trash_can/frame_0008.jpg', 30), ('data/office/webcam/keyboard/frame_0002.jpg', 12), ('data/office/webcam/headphones/frame_0002.jpg', 10), ('data/office/webcam/mobile_phone/frame_0016.jpg', 29), ('data/office/webcam/speaker/frame_0015.jpg', 30), ('data/office/webcam/punchers/frame_0003.jpg', 23), ('data/office/webcam/ring_binder/frame_0024.jpg', 24), ('data/office/webcam/ring_binder/frame_0022.jpg', 30), ('data/office/webcam/file_cabinet/frame_0004.jpg', 9), ('data/office/webcam/tape_dispenser/frame_0001.jpg', 29), ('data/office/webcam/calculator/frame_0025.jpg', 5), ('data/office/webcam/file_cabinet/frame_0006.jpg', 9), ('data/office/webcam/monitor/frame_0040.jpg', 30), ('data/office/webcam/mouse/frame_0020.jpg', 9), ('data/office/webcam/calculator/frame_0027.jpg', 5), ('data/office/webcam/monitor/frame_0007.jpg', 30), ('data/office/webcam/back_pack/frame_0009.jpg', 0), ('data/office/webcam/bike_helmet/frame_0002.jpg', 21), ('data/office/webcam/stapler/frame_0022.jpg', 28), ('data/office/webcam/punchers/frame_0020.jpg', 28), ('data/office/webcam/mug/frame_0023.jpg', 17), ('data/office/webcam/projector/frame_0027.jpg', 23), ('data/office/webcam/stapler/frame_0018.jpg', 28), ('data/office/webcam/projector/frame_0030.jpg', 23), ('data/office/webcam/bike_helmet/frame_0018.jpg', 2), ('data/office/webcam/paper_notebook/frame_0002.jpg', 18), ('data/office/webcam/bike_helmet/frame_0021.jpg', 2), ('data/office/webcam/printer/frame_0008.jpg', 21), ('data/office/webcam/trash_can/frame_0004.jpg', 30), ('data/office/webcam/laptop_computer/frame_0002.jpg', 12), ('data/office/webcam/phone/frame_0010.jpg', 20), ('data/office/webcam/calculator/frame_0007.jpg', 5), ('data/office/webcam/headphones/frame_0001.jpg', 10), ('data/office/webcam/desk_lamp/frame_0005.jpg', 7), ('data/office/webcam/headphones/frame_0023.jpg', 10), ('data/office/webcam/calculator/frame_0022.jpg', 12), ('data/office/webcam/bottle/frame_0016.jpg', 30), ('data/office/webcam/paper_notebook/frame_0027.jpg', 18), ('data/office/webcam/bike_helmet/frame_0006.jpg', 2), ('data/office/webcam/headphones/frame_0004.jpg', 10), ('data/office/webcam/headphones/frame_0011.jpg', 10), ('data/office/webcam/bike_helmet/frame_0022.jpg', 2), ('data/office/webcam/trash_can/frame_0019.jpg', 8), ('data/office/webcam/desktop_computer/frame_0010.jpg', 8), ('data/office/webcam/mouse/frame_0006.jpg', 16), ('data/office/webcam/phone/frame_0016.jpg', 20), ('data/office/webcam/mouse/frame_0003.jpg', 16), ('data/office/webcam/mouse/frame_0027.jpg', 16), ('data/office/webcam/ruler/frame_0004.jpg', 30), ('data/office/webcam/headphones/frame_0009.jpg', 10), ('data/office/webcam/calculator/frame_0026.jpg', 5), ('data/office/webcam/monitor/frame_0032.jpg', 30), ('data/office/webcam/monitor/frame_0020.jpg', 30), ('data/office/webcam/monitor/frame_0036.jpg', 15), ('data/office/webcam/laptop_computer/frame_0003.jpg', 12), ('data/office/webcam/stapler/frame_0014.jpg', 28), ('data/office/webcam/printer/frame_0003.jpg', 23), ('data/office/webcam/pen/frame_0018.jpg', 19), ('data/office/webcam/bottle/frame_0013.jpg', 30), ('data/office/webcam/ruler/frame_0007.jpg', 8), ('data/office/webcam/phone/frame_0009.jpg', 20), ('data/office/webcam/laptop_computer/frame_0014.jpg', 12), ('data/office/webcam/scissors/frame_0005.jpg', 26), ('data/office/webcam/punchers/frame_0026.jpg', 23), ('data/office/webcam/projector/frame_0029.jpg', 27), ('data/office/webcam/file_cabinet/frame_0005.jpg', 9), ('data/office/webcam/keyboard/frame_0012.jpg', 5), ('data/office/webcam/monitor/frame_0025.jpg', 30), ('data/office/webcam/ring_binder/frame_0040.jpg', 30), ('data/office/webcam/letter_tray/frame_0010.jpg', 23), ('data/office/webcam/ring_binder/frame_0015.jpg', 30), ('data/office/webcam/ring_binder/frame_0038.jpg', 30), ('data/office/webcam/laptop_computer/frame_0017.jpg', 12), ('data/office/webcam/bottle/frame_0014.jpg', 30), ('data/office/webcam/monitor/frame_0028.jpg', 30), ('data/office/webcam/calculator/frame_0012.jpg', 5), ('data/office/webcam/monitor/frame_0012.jpg', 15), ('data/office/webcam/bike_helmet/frame_0014.jpg', 28), ('data/office/webcam/scissors/frame_0004.jpg', 26), ('data/office/webcam/monitor/frame_0043.jpg', 30), ('data/office/webcam/calculator/frame_0020.jpg', 12), ('data/office/webcam/mobile_phone/frame_0006.jpg', 14), ('data/office/webcam/desk_lamp/frame_0012.jpg', 28), ('data/office/webcam/desktop_computer/frame_0021.jpg', 30), ('data/office/webcam/mobile_phone/frame_0007.jpg', 22), ('data/office/webcam/bike_helmet/frame_0017.jpg', 2), ('data/office/webcam/ruler/frame_0008.jpg', 8), ('data/office/webcam/keyboard/frame_0014.jpg', 12), ('data/office/webcam/headphones/frame_0005.jpg', 10), ('data/office/webcam/desk_lamp/frame_0015.jpg', 8), ('data/office/webcam/letter_tray/frame_0015.jpg', 23), ('data/office/webcam/headphones/frame_0025.jpg', 26), ('data/office/webcam/bike_helmet/frame_0012.jpg', 28), ('data/office/webcam/speaker/frame_0017.jpg', 30), ('data/office/webcam/calculator/frame_0017.jpg', 5), ('data/office/webcam/monitor/frame_0034.jpg', 30), ('data/office/webcam/stapler/frame_0006.jpg', 28), ('data/office/webcam/pen/frame_0005.jpg', 19), ('data/office/webcam/desk_chair/frame_0039.jpg', 30), ('data/office/webcam/calculator/frame_0011.jpg', 5), ('data/office/webcam/scissors/frame_0023.jpg', 26), ('data/office/webcam/monitor/frame_0039.jpg', 30), ('data/office/webcam/bike_helmet/frame_0003.jpg', 2), ('data/office/webcam/bike_helmet/frame_0011.jpg', 2), ('data/office/webcam/punchers/frame_0022.jpg', 23), ('data/office/webcam/monitor/frame_0022.jpg', 30), ('data/office/webcam/mouse/frame_0016.jpg', 16), ('data/office/webcam/mug/frame_0015.jpg', 17), ('data/office/webcam/letter_tray/frame_0002.jpg', 3), ('data/office/webcam/monitor/frame_0006.jpg', 30), ('data/office/webcam/phone/frame_0002.jpg', 20), ('data/office/webcam/pen/frame_0009.jpg', 19), ('data/office/webcam/phone/frame_0015.jpg', 20), ('data/office/webcam/laptop_computer/frame_0027.jpg', 12), ('data/office/webcam/file_cabinet/frame_0009.jpg', 9), ('data/office/webcam/laptop_computer/frame_0005.jpg', 8), ('data/office/webcam/mug/frame_0012.jpg', 17), ('data/office/webcam/bottle/frame_0015.jpg', 30), ('data/office/webcam/paper_notebook/frame_0014.jpg', 18), ('data/office/webcam/headphones/frame_0019.jpg', 10), ('data/office/webcam/projector/frame_0013.jpg', 21), ('data/office/webcam/mouse/frame_0010.jpg', 29), ('data/office/webcam/desk_chair/frame_0038.jpg', 30), ('data/office/webcam/desk_chair/frame_0032.jpg', 27), ('data/office/webcam/trash_can/frame_0017.jpg', 30), ('data/office/webcam/bottle/frame_0011.jpg', 30), ('data/office/webcam/bike_helmet/frame_0023.jpg', 2), ('data/office/webcam/tape_dispenser/frame_0021.jpg', 28), ('data/office/webcam/file_cabinet/frame_0013.jpg', 9), ('data/office/webcam/mouse/frame_0030.jpg', 16), ('data/office/webcam/punchers/frame_0009.jpg', 28), ('data/office/webcam/headphones/frame_0020.jpg', 10), ('data/office/webcam/phone/frame_0001.jpg', 20), ('data/office/webcam/mobile_phone/frame_0022.jpg', 16), ('data/office/webcam/tape_dispenser/frame_0011.jpg', 30), ('data/office/webcam/back_pack/frame_0004.jpg', 30)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ba77561644d848a2c2e4b37bcb43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d24f274564402abc4f1e4a2dc2ffde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset Val Loss: 4.8788 Val Acc: 0.3445\n",
      "Epoch 1/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca13a61c8874787b878e990ec00c8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99be2d827627478b9a798b14e214d735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source dataset Train Loss: 1.3494 Train Acc: 0.9290\n",
      "Source dataset Val Loss: 2.7688 Val Acc: 0.8104\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e583be1524494e35b75765ba95cf7bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 22/275 images to be included in next epoch\n",
      "[('data/office/webcam/bottle/frame_0009.jpg', 4), ('data/office/webcam/scissors/frame_0022.jpg', 26), ('data/office/webcam/back_pack/frame_0002.jpg', 0), ('data/office/webcam/monitor/frame_0011.jpg', 15), ('data/office/webcam/bottle/frame_0007.jpg', 4), ('data/office/webcam/keyboard/frame_0015.jpg', 11), ('data/office/webcam/calculator/frame_0028.jpg', 5), ('data/office/webcam/phone/frame_0007.jpg', 20), ('data/office/webcam/phone/frame_0003.jpg', 20), ('data/office/webcam/pen/frame_0026.jpg', 19), ('data/office/webcam/keyboard/frame_0007.jpg', 5), ('data/office/webcam/bike/frame_0007.jpg', 1), ('data/office/webcam/keyboard/frame_0017.jpg', 11), ('data/office/webcam/keyboard/frame_0009.jpg', 11), ('data/office/webcam/keyboard/frame_0016.jpg', 11), ('data/office/webcam/keyboard/frame_0013.jpg', 11), ('data/office/webcam/punchers/frame_0014.jpg', 23), ('data/office/webcam/back_pack/frame_0011.jpg', 0), ('data/office/webcam/ruler/frame_0009.jpg', 25), ('data/office/webcam/ring_binder/frame_0023.jpg', 24), ('data/office/webcam/bike/frame_0019.jpg', 1), ('data/office/webcam/bike_helmet/frame_0009.jpg', 21)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c7a8ca64b74980956f4695f59a7f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAIN_SEMI_SUPERVISED_MODEL:\n",
    "    print(\"Starting pseudo-labeling task...\")\n",
    "    model, source_history, target_history, label_history = (\n",
    "        tasks.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: create_padded_dataloader(\n",
    "                dataset,\n",
    "                sampler=sampler,\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=1, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR,\n",
    "            num_epochs=20,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dustbin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "output_ls = []\n",
    "# Iterate over batches\n",
    "for inputs, labels in tqdm(source_test_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output_ls.append(model(inputs))\n",
    "\n",
    "logits_list = torch.cat(output_ls)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import tasks\n",
    "import tasks.calibration\n",
    "\n",
    "tasks.calibration.reliability_diagram(\n",
    "    source_actual, source_predicted, np.array(tasks.calibration.T_scaling(logits_list.cpu(), 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(\"Calibrating ideal temperature...\")\n",
    "ideal_temperature = tasks.calibration.calibrate_temperature(\n",
    "    model, source_val_loader, criterion, device, iters=6\n",
    ")\n",
    "print(\"Ideal temperature \", ideal_temperature.item())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "tasks.calibration.reliability_diagram(\n",
    "    source_actual,\n",
    "    source_predicted,\n",
    "    np.array(\n",
    "        tasks.calibration.T_scaling(\n",
    "            logits_list,\n",
    "            ideal_temperature,\n",
    "        ).detach().to(\"cpu\")\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
