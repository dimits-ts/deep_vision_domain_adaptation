{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Pseudo-labeling for Domain Adaptation in Deep Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "> Dimitris Tsirmpas <br>\n",
    "> MSc in Data Science f3352315 <br>\n",
    "> Athens University of Economics and Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Structure\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "Main files:\n",
    "- domain_adaptation.ipynb: is the main Jupyter Notebook containing the project code\n",
    "- report.pdf: Supplemental material containing Figures, Tables and analysis on the results of the project\n",
    "  \n",
    "Directories:\n",
    "- lib: a library of general functions for Data Science tasks\n",
    "- tasks: task-specific modules\n",
    "- data: the input data\n",
    "- output: the model training output\n",
    "- results: Graphs, Tables and Figures produced in the project\n",
    "- scripts: pre-processing scripts applied to the data\n",
    "\n",
    "Notes:\n",
    "\n",
    "* This notebook mainly discusses the implementation and design decisions of the project. For the theory, experimental procedures\n",
    "and results, consult [the main report](report.pdf).\n",
    "\n",
    "* This notebook does not contain the fundamental code for preprocessing, loading data, training models, implementing the pseudo-labeling procedure\n",
    "or our incremental learning algorithm. The code, as well as extensive comments and documentation, exist in the respective source files in the\n",
    "`tasks` and `lib` directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Office Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lib.data\n",
    "import lib.torch_train_eval\n",
    "import lib.adaptive_train_eval\n",
    "\n",
    "import tasks.preprocessing\n",
    "import tasks.utils\n",
    "import tasks.results\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "OUTPUT_DIR = \"output/office\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 1\n",
    "PRINT_STATS_PERIOD = 500\n",
    "\n",
    "SOURCE_DATASET = \"amazon\"\n",
    "SOURCE_VAL_SPLIT = .15\n",
    "SOURCE_TEST_SPLIT = .1\n",
    "\n",
    "TARGET_VAL_SPLIT = .15\n",
    "TARGET_TEST_SPLIT = .15\n",
    "TARGET_DATASET = \"webcam\"\n",
    "\n",
    "RHO = 4\n",
    "SAMPLING_PERIOD = 20\n",
    "\n",
    "\n",
    "FINETUNED_SOURCE__MODEL_DIR = os.path.join(OUTPUT_DIR, \"classifier\")\n",
    "FINETUNED_TARGET_MODEL_DIR = os.path.join(OUTPUT_DIR, \"target_classifier\")\n",
    "UNSUPERVISED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"unsupervised\")\n",
    "SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20 = os.path.join(OUTPUT_DIR, \"semi-supervised-finetuned-20\")\n",
    "SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20 = os.path.join(OUTPUT_DIR, \"semi-supervised-adaptive-20\")\n",
    "SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10 = os.path.join(OUTPUT_DIR, \"semi-supervised-finetuned-10\")\n",
    "SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10 = os.path.join(OUTPUT_DIR, \"semi-supervised-adaptive-10\")\n",
    "\n",
    "FINETUNE_SOURCE_MODEL = False\n",
    "FINETUNE_TARGET_MODEL = False\n",
    "TRAIN_UNSUPERVISED_MODEL = False\n",
    "FINETUNE_SEMI_SUPERVISED_MODEL_20 = False\n",
    "TRAIN_SEMI_SUPERVISED_MODEL_20 = False\n",
    "FINETUNE_SEMI_SUPERVISED_MODEL_10 = False\n",
    "TRAIN_SEMI_SUPERVISED_MODEL_10 = True\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing\n",
    "\n",
    "Most of the work here is done through the ImageDataset class, which lazy loads all images from the given root directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data from the source dataset (Modern Office-31 Amazon domain)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5c4f3ca8394424ba7607cbea55b051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "source_dataset.load_from_directory(os.path.join(DATA_DIR, SOURCE_DATASET))\n",
    "\n",
    "source_train_dataset, source_val_dataset, source_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        source_dataset, SOURCE_VAL_SPLIT, SOURCE_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "source_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_train_dataset, shuffle=True\n",
    ")\n",
    "source_val_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_val_dataset, shuffle=False\n",
    ")\n",
    "source_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the respective target dataset (Modern Office-31 Webcam domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff621df76da24c9dafa0cb7cb6e2464b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    "    label_encoder=source_dataset.label_encoder,  # use same classes\n",
    ")\n",
    "target_dataset.load_from_directory(os.path.join(DATA_DIR, TARGET_DATASET))\n",
    "\n",
    "target_train_dataset, target_val_dataset, target_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        target_dataset, TARGET_VAL_SPLIT, TARGET_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "target_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_train_dataset, shuffle=True\n",
    ")\n",
    "target_val_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_val_dataset, shuffle=False\n",
    ")\n",
    "target_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import convert the target domain data into unlabelled and labelled datasets. \n",
    "\n",
    "We use a stratified split for all classes, converting 10%/20% of the target domain samples into labelled data and then adding them to the source domain dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 2112)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "unlabeled_dataset_20.load_from_image_dataset(target_train_dataset)\n",
    "\n",
    "to_be_unlabeled_dataset_20, labeled_dataset_20 = lib.data.stratified_split(\n",
    "    target_train_dataset, test_size=0.2\n",
    ")\n",
    "\n",
    "unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=labeled_dataset_20.parser_func,\n",
    "    preprocessing_func=labeled_dataset_20.preprocessing_func,\n",
    ")\n",
    "unlabeled_dataset_20.load_from_image_dataset(to_be_unlabeled_dataset_20)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in source_train_dataset.samples:\n",
    "    labeled_dataset_20.add(sample_img, sample_label)\n",
    "\n",
    "len(labeled_dataset_20), len(source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168, 2112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_unlabeled_dataset_10, labeled_dataset_10 = lib.data.stratified_split(\n",
    "    target_train_dataset, test_size=0.1\n",
    ")\n",
    "\n",
    "unlabeled_dataset_10 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=labeled_dataset_10.parser_func,\n",
    "    preprocessing_func=labeled_dataset_20.preprocessing_func,\n",
    ")\n",
    "unlabeled_dataset_10.load_from_image_dataset(to_be_unlabeled_dataset_10)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in source_train_dataset.samples:\n",
    "    labeled_dataset_10.add(sample_img, sample_label)\n",
    "\n",
    "len(labeled_dataset_10), len(source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = source_train_dataset.label_encoder.classes_\n",
    "\n",
    "encodings = {\n",
    "    label: class_name\n",
    "    for label, class_name in enumerate(source_train_dataset.label_encoder.classes_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source-only model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the ResNet-18 model, pre-trained on the ImageNet dataset. \n",
    "\n",
    "From our experience, keeping the pre-trained classification head of 1000 classes actually significantly speeds up training and avoids overfitting. Thus, we do not replace it with our own 31-class dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1000]                 --\n",
       "├─Conv2d: 1-1                            [1, 64, 750, 750]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 750, 750]         128\n",
       "├─ReLU: 1-3                              [1, 64, 750, 750]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 375, 375]         --\n",
       "├─Sequential: 1-5                        [1, 64, 375, 375]         --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 375, 375]         --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 375, 375]         --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 375, 375]         36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 375, 375]         128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 375, 375]         --\n",
       "├─Sequential: 1-6                        [1, 128, 188, 188]        --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 188, 188]        73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 188, 188]        256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 188, 188]        147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 188, 188]        256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 188, 188]        8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 188, 188]        --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 188, 188]        147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 188, 188]        256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 188, 188]        --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 188, 188]        147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 188, 188]        256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 188, 188]        --\n",
       "├─Sequential: 1-7                        [1, 256, 94, 94]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 94, 94]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 94, 94]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 94, 94]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 94, 94]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 94, 94]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 94, 94]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 94, 94]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 94, 94]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 94, 94]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 94, 94]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 94, 94]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 94, 94]          --\n",
       "├─Sequential: 1-8                        [1, 512, 47, 47]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 47, 47]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 47, 47]          1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 47, 47]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 47, 47]          1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 47, 47]          132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 47, 47]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 47, 47]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 47, 47]          1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 47, 47]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 47, 47]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 47, 47]          1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 47, 47]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 81.62\n",
       "==========================================================================================\n",
       "Input size (MB): 27.00\n",
       "Forward/backward pass size (MB): 1785.37\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 1859.13\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "#https://arxiv.org/pdf/2405.13698\n",
    "# disable lr for adam\n",
    "exp_lr_scheduler = None\n",
    "\n",
    "\n",
    "torchinfo.summary(tasks.utils.get_model(device=device), input_size=(BATCH_SIZE, 3, 1500, 1500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finetune the pretrained model on the source domain dataset (Amazon domain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if FINETUNE_SOURCE_MODEL:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        source_train_loader,\n",
    "        source_val_loader,\n",
    "        output_dir=FINETUNED_SOURCE__MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the learning curves as well as the classification results on both source and target domain test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that our baseline, the source-only model, can not effectively distinguish between any of the classes (except for the first few)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target only model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure described above is repeated for the model trained on the target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device, replace_fc_layer=True, num_classes=len(encodings))\n",
    "\n",
    "if FINETUNE_TARGET_MODEL:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=10e-3)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        target_train_loader,\n",
    "        target_val_loader,\n",
    "        output_dir=FINETUNED_TARGET_MODEL_DIR,\n",
    "        num_epochs=100,\n",
    "        patience=10,\n",
    "        warmup_period=25,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=20000,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, there is no way we can effectively train the target-only model which was supposed to be our baseline. \n",
    "Due to the low number of datapoints and the difficulty of the domain, it immediately overfits. No amount of increasing regularization\n",
    "(such as AdamW with weight_decay=$10^{-2}$) seems to influence the results. Additionally, we can not use data augmentation, since that\n",
    "would make the comparison between models unfair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained and evaluated our baselines, we can begin experimenting with the incremental learning procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if TRAIN_UNSUPERVISED_MODEL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=source_train_dataset,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_20,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=UNSUPERVISED_MODEL_DIR,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, UNSUPERVISED_MODEL_DIR)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised domain adaptation: 10% target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if FINETUNE_SEMI_SUPERVISED_MODEL_10:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            labeled_dataset_10, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=source_val_loader,\n",
    "        output_dir=SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        previous_history=None,\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/159\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimits/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 17/500 images on threshold 0.9609767078649294\n",
      "[('data/office/webcam/phone/frame_0001.jpg', 20), ('data/office/webcam/keyboard/frame_0013.jpg', 5), ('data/office/webcam/calculator/frame_0005.jpg', 5), ('data/office/webcam/bike_helmet/frame_0020.jpg', 2), ('data/office/webcam/bike_helmet/frame_0011.jpg', 2), ('data/office/webcam/calculator/frame_0026.jpg', 5), ('data/office/webcam/calculator/frame_0022.jpg', 11), ('data/office/webcam/calculator/frame_0011.jpg', 5), ('data/office/webcam/phone/frame_0012.jpg', 20), ('data/office/webcam/calculator/frame_0012.jpg', 5), ('data/office/webcam/calculator/frame_0024.jpg', 5), ('data/office/webcam/calculator/frame_0028.jpg', 5), ('data/office/webcam/phone/frame_0011.jpg', 20), ('data/office/webcam/calculator/frame_0023.jpg', 5), ('data/office/webcam/keyboard/frame_0009.jpg', 5), ('data/office/webcam/stapler/frame_0022.jpg', 28), ('data/office/webcam/calculator/frame_0017.jpg', 5)]\n",
      "Target dataset Val Loss: 6.3066 Val Acc: 0.1345\n",
      "Epoch 1/159\n",
      "----------\n",
      "Source dataset Train Loss: 3.2733 Train Acc: 0.3529\n",
      "Source dataset Val Loss: 3.9047 Val Acc: 0.6398\n",
      "\n",
      "Target dataset Val Loss: 5.2508 Val Acc: 0.1513\n",
      "Epoch 2/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.6726 Train Acc: 0.7647\n",
      "Source dataset Val Loss: 4.5108 Val Acc: 0.6611\n",
      "\n",
      "Target dataset Val Loss: 4.9857 Val Acc: 0.1261\n",
      "Epoch 3/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.9039 Train Acc: 0.6471\n",
      "Source dataset Val Loss: 4.7376 Val Acc: 0.6256\n",
      "\n",
      "Target dataset Val Loss: 4.6460 Val Acc: 0.1765\n",
      "Epoch 4/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.4797 Train Acc: 0.7059\n",
      "Source dataset Val Loss: 4.4722 Val Acc: 0.6303\n",
      "\n",
      "Target dataset Val Loss: 4.5782 Val Acc: 0.1345\n",
      "Epoch 5/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.8467 Train Acc: 0.5882\n",
      "Source dataset Val Loss: 5.1583 Val Acc: 0.5498\n",
      "\n",
      "Target dataset Val Loss: 4.8981 Val Acc: 0.1261\n",
      "Epoch 6/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.4638 Train Acc: 0.8235\n",
      "Source dataset Val Loss: 5.4306 Val Acc: 0.6374\n",
      "\n",
      "Target dataset Val Loss: 4.7895 Val Acc: 0.1849\n",
      "Epoch 7/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3370 Train Acc: 0.7059\n",
      "Source dataset Val Loss: 3.9329 Val Acc: 0.6777\n",
      "\n",
      "Target dataset Val Loss: 4.3251 Val Acc: 0.2017\n",
      "Epoch 8/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.2729 Train Acc: 0.7647\n",
      "Source dataset Val Loss: 6.5360 Val Acc: 0.6682\n",
      "\n",
      "Target dataset Val Loss: 4.7774 Val Acc: 0.1429\n",
      "Epoch 9/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.1053 Train Acc: 0.8824\n",
      "Source dataset Val Loss: 5.0874 Val Acc: 0.6469\n",
      "\n",
      "Target dataset Val Loss: 4.4293 Val Acc: 0.1261\n",
      "Epoch 10/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.8094 Train Acc: 0.5882\n",
      "Source dataset Val Loss: 6.4165 Val Acc: 0.5142\n",
      "\n",
      "Target dataset Val Loss: 5.3194 Val Acc: 0.1429\n",
      "Epoch 11/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.7303 Train Acc: 0.5882\n",
      "Source dataset Val Loss: 5.3540 Val Acc: 0.6730\n",
      "\n",
      "Target dataset Val Loss: 6.0340 Val Acc: 0.1765\n",
      "Epoch 12/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.4399 Train Acc: 0.5882\n",
      "Source dataset Val Loss: 4.8950 Val Acc: 0.6256\n",
      "\n",
      "Target dataset Val Loss: 4.7651 Val Acc: 0.0840\n",
      "Epoch 13/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.5560 Train Acc: 0.5294\n",
      "Source dataset Val Loss: 4.7258 Val Acc: 0.6114\n",
      "\n",
      "Target dataset Val Loss: 4.7586 Val Acc: 0.1092\n",
      "Epoch 14/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.6029 Train Acc: 0.6471\n",
      "Source dataset Val Loss: 4.6825 Val Acc: 0.5948\n",
      "\n",
      "Target dataset Val Loss: 5.1393 Val Acc: 0.1092\n",
      "Epoch 15/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3822 Train Acc: 0.7059\n",
      "Source dataset Val Loss: 6.1726 Val Acc: 0.5332\n",
      "\n",
      "Target dataset Val Loss: 6.1264 Val Acc: 0.0840\n",
      "Epoch 16/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.7836 Train Acc: 0.4706\n",
      "Source dataset Val Loss: 10.4357 Val Acc: 0.4502\n",
      "\n",
      "Target dataset Val Loss: 5.4505 Val Acc: 0.1345\n",
      "Epoch 17/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.4232 Train Acc: 0.5882\n",
      "Source dataset Val Loss: 4.4490 Val Acc: 0.6303\n",
      "\n",
      "Target dataset Val Loss: 5.5419 Val Acc: 0.1261\n",
      "Epoch 18/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.9665 Train Acc: 0.4706\n",
      "Source dataset Val Loss: 8.2683 Val Acc: 0.4645\n",
      "\n",
      "Target dataset Val Loss: 8.5344 Val Acc: 0.1429\n",
      "Epoch 19/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.5051 Train Acc: 0.7059\n",
      "Source dataset Val Loss: 6.1842 Val Acc: 0.5711\n",
      "\n",
      "Target dataset Val Loss: 5.9437 Val Acc: 0.0840\n",
      "Epoch 20/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.4548 Train Acc: 0.7647\n",
      "Source dataset Val Loss: 5.4596 Val Acc: 0.5711\n",
      "\n",
      "Selected 96/483 images on threshold 0.5832512310272909\n",
      "[('data/office/webcam/desktop_computer/frame_0002.jpg', 8), ('data/office/webcam/phone/frame_0010.jpg', 20), ('data/office/webcam/laptop_computer/frame_0023.jpg', 12), ('data/office/webcam/stapler/frame_0010.jpg', 21), ('data/office/webcam/mouse/frame_0023.jpg', 21), ('data/office/webcam/calculator/frame_0008.jpg', 11), ('data/office/webcam/back_pack/frame_0029.jpg', 10), ('data/office/webcam/desktop_computer/frame_0001.jpg', 8), ('data/office/webcam/calculator/frame_0025.jpg', 11), ('data/office/webcam/mouse/frame_0008.jpg', 17), ('data/office/webcam/keyboard/frame_0010.jpg', 5), ('data/office/webcam/paper_notebook/frame_0016.jpg', 17), ('data/office/webcam/laptop_computer/frame_0022.jpg', 12), ('data/office/webcam/keyboard/frame_0004.jpg', 5), ('data/office/webcam/headphones/frame_0024.jpg', 21), ('data/office/webcam/laptop_computer/frame_0024.jpg', 5), ('data/office/webcam/scissors/frame_0001.jpg', 26), ('data/office/webcam/headphones/frame_0018.jpg', 10), ('data/office/webcam/mug/frame_0009.jpg', 11), ('data/office/webcam/mug/frame_0023.jpg', 17), ('data/office/webcam/laptop_computer/frame_0017.jpg', 12), ('data/office/webcam/mobile_phone/frame_0004.jpg', 26), ('data/office/webcam/punchers/frame_0016.jpg', 12), ('data/office/webcam/ring_binder/frame_0032.jpg', 11), ('data/office/webcam/file_cabinet/frame_0013.jpg', 20), ('data/office/webcam/calculator/frame_0010.jpg', 11), ('data/office/webcam/keyboard/frame_0005.jpg', 5), ('data/office/webcam/calculator/frame_0020.jpg', 5), ('data/office/webcam/letter_tray/frame_0015.jpg', 21), ('data/office/webcam/speaker/frame_0012.jpg', 12), ('data/office/webcam/bike_helmet/frame_0009.jpg', 21), ('data/office/webcam/desk_lamp/frame_0015.jpg', 23), ('data/office/webcam/bike_helmet/frame_0025.jpg', 17), ('data/office/webcam/laptop_computer/frame_0016.jpg', 12), ('data/office/webcam/laptop_computer/frame_0002.jpg', 12), ('data/office/webcam/bike_helmet/frame_0021.jpg', 21), ('data/office/webcam/laptop_computer/frame_0021.jpg', 17), ('data/office/webcam/projector/frame_0016.jpg', 12), ('data/office/webcam/calculator/frame_0029.jpg', 11), ('data/office/webcam/keyboard/frame_0007.jpg', 5), ('data/office/webcam/punchers/frame_0005.jpg', 12), ('data/office/webcam/back_pack/frame_0008.jpg', 12), ('data/office/webcam/headphones/frame_0019.jpg', 10), ('data/office/webcam/bike_helmet/frame_0028.jpg', 17), ('data/office/webcam/keyboard/frame_0003.jpg', 5), ('data/office/webcam/mouse/frame_0002.jpg', 17), ('data/office/webcam/laptop_computer/frame_0027.jpg', 12), ('data/office/webcam/tape_dispenser/frame_0012.jpg', 12), ('data/office/webcam/keyboard/frame_0006.jpg', 5), ('data/office/webcam/phone/frame_0009.jpg', 20), ('data/office/webcam/keyboard/frame_0002.jpg', 11), ('data/office/webcam/mug/frame_0010.jpg', 11), ('data/office/webcam/printer/frame_0020.jpg', 21), ('data/office/webcam/calculator/frame_0006.jpg', 11), ('data/office/webcam/letter_tray/frame_0010.jpg', 21), ('data/office/webcam/calculator/frame_0001.jpg', 5), ('data/office/webcam/back_pack/frame_0004.jpg', 17), ('data/office/webcam/laptop_computer/frame_0001.jpg', 12), ('data/office/webcam/back_pack/frame_0016.jpg', 12), ('data/office/webcam/bike/frame_0007.jpg', 1), ('data/office/webcam/punchers/frame_0004.jpg', 12), ('data/office/webcam/calculator/frame_0019.jpg', 5), ('data/office/webcam/tape_dispenser/frame_0009.jpg', 17), ('data/office/webcam/headphones/frame_0023.jpg', 2), ('data/office/webcam/phone/frame_0016.jpg', 20), ('data/office/webcam/mouse/frame_0006.jpg', 17), ('data/office/webcam/desk_lamp/frame_0004.jpg', 17), ('data/office/webcam/projector/frame_0019.jpg', 21), ('data/office/webcam/phone/frame_0002.jpg', 20), ('data/office/webcam/laptop_computer/frame_0026.jpg', 17), ('data/office/webcam/phone/frame_0014.jpg', 20), ('data/office/webcam/letter_tray/frame_0013.jpg', 21), ('data/office/webcam/paper_notebook/frame_0023.jpg', 17), ('data/office/webcam/back_pack/frame_0010.jpg', 17), ('data/office/webcam/ring_binder/frame_0031.jpg', 11), ('data/office/webcam/printer/frame_0008.jpg', 21), ('data/office/webcam/calculator/frame_0018.jpg', 5), ('data/office/webcam/calculator/frame_0007.jpg', 11), ('data/office/webcam/printer/frame_0012.jpg', 17), ('data/office/webcam/mouse/frame_0001.jpg', 17), ('data/office/webcam/printer/frame_0010.jpg', 23), ('data/office/webcam/mouse/frame_0007.jpg', 17), ('data/office/webcam/bike/frame_0012.jpg', 17), ('data/office/webcam/punchers/frame_0020.jpg', 17), ('data/office/webcam/projector/frame_0013.jpg', 21), ('data/office/webcam/paper_notebook/frame_0026.jpg', 17), ('data/office/webcam/mouse/frame_0015.jpg', 21), ('data/office/webcam/mouse/frame_0010.jpg', 17), ('data/office/webcam/projector/frame_0011.jpg', 21), ('data/office/webcam/printer/frame_0002.jpg', 21), ('data/office/webcam/keyboard/frame_0011.jpg', 5), ('data/office/webcam/mobile_phone/frame_0023.jpg', 17), ('data/office/webcam/speaker/frame_0015.jpg', 17), ('data/office/webcam/stapler/frame_0024.jpg', 25), ('data/office/webcam/desk_lamp/frame_0005.jpg', 10), ('data/office/webcam/projector/frame_0020.jpg', 21)]\n",
      "Target dataset Val Loss: 7.6891 Val Acc: 0.1176\n",
      "Epoch 21/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.7921 Train Acc: 0.5929\n",
      "Source dataset Val Loss: 7.9818 Val Acc: 0.5166\n",
      "\n",
      "Target dataset Val Loss: 6.1870 Val Acc: 0.1513\n",
      "Epoch 22/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.4223 Train Acc: 0.6903\n",
      "Source dataset Val Loss: 6.6394 Val Acc: 0.5995\n",
      "\n",
      "Target dataset Val Loss: 7.3017 Val Acc: 0.1092\n",
      "Epoch 23/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.6477 Train Acc: 0.5752\n",
      "Source dataset Val Loss: 6.0882 Val Acc: 0.6374\n",
      "\n",
      "Target dataset Val Loss: 6.6430 Val Acc: 0.1597\n",
      "Epoch 24/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.5830 Train Acc: 0.6372\n",
      "Source dataset Val Loss: 6.2393 Val Acc: 0.5687\n",
      "\n",
      "Target dataset Val Loss: 8.4603 Val Acc: 0.1261\n",
      "Epoch 25/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3515 Train Acc: 0.6726\n",
      "Source dataset Val Loss: 5.4843 Val Acc: 0.6137\n",
      "\n",
      "Target dataset Val Loss: 11.4387 Val Acc: 0.1092\n",
      "Epoch 26/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3422 Train Acc: 0.6726\n",
      "Source dataset Val Loss: 5.6222 Val Acc: 0.5758\n",
      "\n",
      "Target dataset Val Loss: 6.6024 Val Acc: 0.1765\n",
      "Epoch 27/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.2679 Train Acc: 0.7168\n",
      "Source dataset Val Loss: 8.3661 Val Acc: 0.5616\n",
      "\n",
      "Target dataset Val Loss: 6.4429 Val Acc: 0.1345\n",
      "Epoch 28/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3546 Train Acc: 0.7257\n",
      "Source dataset Val Loss: 7.8477 Val Acc: 0.5545\n",
      "\n",
      "Target dataset Val Loss: 8.2885 Val Acc: 0.1345\n",
      "Epoch 29/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3761 Train Acc: 0.7345\n",
      "Source dataset Val Loss: 6.6962 Val Acc: 0.5592\n",
      "\n",
      "Target dataset Val Loss: 5.8616 Val Acc: 0.0924\n",
      "Epoch 30/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.7349 Train Acc: 0.6106\n",
      "Source dataset Val Loss: 4.4038 Val Acc: 0.5924\n",
      "\n",
      "Target dataset Val Loss: 6.4706 Val Acc: 0.1513\n",
      "Epoch 31/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.5944 Train Acc: 0.6549\n",
      "Source dataset Val Loss: 5.3220 Val Acc: 0.5972\n",
      "\n",
      "Target dataset Val Loss: 6.9954 Val Acc: 0.0924\n",
      "Epoch 32/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.5442 Train Acc: 0.6903\n",
      "Source dataset Val Loss: 6.9890 Val Acc: 0.5948\n",
      "\n",
      "Target dataset Val Loss: 5.6817 Val Acc: 0.1933\n",
      "Epoch 33/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3060 Train Acc: 0.7434\n",
      "Source dataset Val Loss: 6.8960 Val Acc: 0.5213\n",
      "\n",
      "Target dataset Val Loss: 5.5404 Val Acc: 0.1513\n",
      "Epoch 34/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.2564 Train Acc: 0.7876\n",
      "Source dataset Val Loss: 9.9907 Val Acc: 0.4834\n",
      "\n",
      "Target dataset Val Loss: 6.0014 Val Acc: 0.1513\n",
      "Epoch 35/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.1973 Train Acc: 0.8230\n",
      "Source dataset Val Loss: 6.2790 Val Acc: 0.5687\n",
      "\n",
      "Target dataset Val Loss: 7.4100 Val Acc: 0.1176\n",
      "Epoch 36/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.5380 Train Acc: 0.6460\n",
      "Source dataset Val Loss: 18.5025 Val Acc: 0.2393\n",
      "\n",
      "Target dataset Val Loss: 7.9307 Val Acc: 0.0504\n",
      "Epoch 37/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.4292 Train Acc: 0.6903\n",
      "Source dataset Val Loss: 6.7100 Val Acc: 0.5545\n",
      "\n",
      "Target dataset Val Loss: 6.3265 Val Acc: 0.1008\n",
      "Epoch 38/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.1707 Train Acc: 0.7788\n",
      "Source dataset Val Loss: 8.1927 Val Acc: 0.5284\n",
      "\n",
      "Target dataset Val Loss: 5.9363 Val Acc: 0.1261\n",
      "Epoch 39/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.2928 Train Acc: 0.7168\n",
      "Source dataset Val Loss: 8.8155 Val Acc: 0.5995\n",
      "\n",
      "Target dataset Val Loss: 6.2721 Val Acc: 0.1092\n",
      "Epoch 40/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3193 Train Acc: 0.7522\n",
      "Source dataset Val Loss: 5.8530 Val Acc: 0.5237\n",
      "\n",
      "Selected 26/387 images on threshold 0.6075379481321845\n",
      "[('data/office/webcam/stapler/frame_0014.jpg', 23), ('data/office/webcam/monitor/frame_0001.jpg', 15), ('data/office/webcam/bookcase/frame_0008.jpg', 22), ('data/office/webcam/monitor/frame_0018.jpg', 15), ('data/office/webcam/monitor/frame_0032.jpg', 15), ('data/office/webcam/mobile_phone/frame_0017.jpg', 5), ('data/office/webcam/phone/frame_0015.jpg', 5), ('data/office/webcam/monitor/frame_0009.jpg', 15), ('data/office/webcam/ring_binder/frame_0040.jpg', 2), ('data/office/webcam/headphones/frame_0020.jpg', 10), ('data/office/webcam/ring_binder/frame_0014.jpg', 16), ('data/office/webcam/laptop_computer/frame_0003.jpg', 12), ('data/office/webcam/keyboard/frame_0026.jpg', 5), ('data/office/webcam/ring_binder/frame_0037.jpg', 15), ('data/office/webcam/monitor/frame_0038.jpg', 15), ('data/office/webcam/bike_helmet/frame_0008.jpg', 21), ('data/office/webcam/ring_binder/frame_0009.jpg', 16), ('data/office/webcam/ring_binder/frame_0007.jpg', 12), ('data/office/webcam/monitor/frame_0013.jpg', 17), ('data/office/webcam/monitor/frame_0037.jpg', 15), ('data/office/webcam/ring_binder/frame_0016.jpg', 8), ('data/office/webcam/monitor/frame_0011.jpg', 15), ('data/office/webcam/letter_tray/frame_0002.jpg', 21), ('data/office/webcam/stapler/frame_0009.jpg', 22), ('data/office/webcam/bookcase/frame_0007.jpg', 22), ('data/office/webcam/keyboard/frame_0008.jpg', 5)]\n",
      "Target dataset Val Loss: 10.1217 Val Acc: 0.1092\n",
      "Epoch 41/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.6031 Train Acc: 0.6475\n",
      "Source dataset Val Loss: 9.0268 Val Acc: 0.5095\n",
      "\n",
      "Target dataset Val Loss: 7.0204 Val Acc: 0.1261\n",
      "Epoch 42/159\n",
      "----------\n",
      "Source dataset Train Loss: 2.3336 Train Acc: 0.7410\n",
      "Source dataset Val Loss: 9.6785 Val Acc: 0.4573\n",
      "\n",
      "Target dataset Val Loss: 7.4762 Val Acc: 0.1176\n",
      "Epoch 43/159\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if TRAIN_SEMI_SUPERVISED_MODEL_10:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset_10,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=1, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_10,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=None,\n",
    "            previous_target_history=None,\n",
    "            verbose=False\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised domain adaptation: 20% target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if FINETUNE_SEMI_SUPERVISED_MODEL_20:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            labeled_dataset_20, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=source_val_loader,\n",
    "        output_dir=SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        previous_history=history\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if TRAIN_SEMI_SUPERVISED_MODEL_20:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    source_history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20, \"source_history.pickle\"))\n",
    "    target_history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20, \"target_history.pickle\"))\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset_20,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, sampler=sampler, shuffle=False\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_20,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST/MNIST-M dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is near-identical to the one running the models on the Modern Office-31 dataset. \n",
    "\n",
    "* We slightly change hyperparameters because of the different nature of the dataset (batch size, $\\rho$, weight decay...)\n",
    "* We change the optimizer from Adam to AdamW (since the second implements weight decay in a mathematically correct manner)\n",
    "* We swap the classification head from the 1000 classes to 10 since:\n",
    "    * There is no correlation between the MNIST and ImageNet classes\n",
    "    * It makes the model smaller and thus harder to overfit, which is likely given the comparatively easier task of MNIST classification\n",
    "* We do not use pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 10e-6\n",
    "RHO = 3\n",
    "\n",
    "AD_DATA_DIR = \"data/digits\"\n",
    "AD_OUTPUT_DIR = \"output/digits\"\n",
    "\n",
    "AD_SOURCE_DATASET = \"mnist\"\n",
    "AD_TARGET_DATASET = \"mnist-m\"\n",
    "\n",
    "AD_FINETUNED_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"classifier\")\n",
    "AD_FINETUNED_TARGET_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"target_classifier\")\n",
    "AD_UNSUPERVISED_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"unsupervised\")\n",
    "AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-finetuned-20\")\n",
    "AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-adaptive-20\")\n",
    "AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-finetuned-10\")\n",
    "AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10 = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-adaptive-10\")\n",
    "\n",
    "AD_FINETUNE_MODEL = False\n",
    "AD_FINETUNE_TARGET_MODEL = False\n",
    "AD_TRAIN_UNSUPERVISED_MODEL = False\n",
    "AD_FINETUNE_SEMI_SUPERVISED_MODEL_20 = False\n",
    "AD_TRAIN_SEMI_SUPERVISED_MODEL_20 = False\n",
    "AD_FINETUNE_SEMI_SUPERVISED_MODEL_10 = False\n",
    "AD_TRAIN_SEMI_SUPERVISED_MODEL_10 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_source_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "ad_source_dataset.load_from_directory(os.path.join(AD_DATA_DIR, AD_SOURCE_DATASET))\n",
    "\n",
    "ad_source_train_dataset, ad_source_val_dataset, ad_source_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        ad_source_dataset, SOURCE_VAL_SPLIT, SOURCE_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "ad_source_train_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_source_train_dataset, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_source_val_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_source_val_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_source_test_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_source_test_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_target_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    "    label_encoder=ad_source_dataset.label_encoder,  # use same classes\n",
    ")\n",
    "ad_target_dataset.load_from_directory(os.path.join(AD_DATA_DIR, AD_TARGET_DATASET))\n",
    "\n",
    "ad_target_train_dataset, ad_target_val_dataset, ad_target_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        ad_target_dataset, TARGET_VAL_SPLIT, TARGET_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "ad_target_train_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_target_train_dataset, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_target_val_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_target_val_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_target_test_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_target_test_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_unlabeled_dataset = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "ad_unlabeled_dataset.load_from_image_dataset(ad_target_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_to_be_unlabeled_dataset_10, ad_labeled_dataset_10 = lib.data.stratified_split(\n",
    "    ad_target_train_dataset, test_size=0.1\n",
    ")\n",
    "\n",
    "ad_unlabeled_dataset_10 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=ad_labeled_dataset_10.parser_func,\n",
    "    preprocessing_func=ad_source_dataset.preprocessing_func,\n",
    ")\n",
    "ad_unlabeled_dataset_10.load_from_image_dataset(ad_to_be_unlabeled_dataset_10)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in ad_source_train_dataset.samples:\n",
    "    ad_labeled_dataset_10.add(sample_img, sample_label)\n",
    "\n",
    "len(ad_labeled_dataset_10), len(ad_source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "ad_unlabeled_dataset_20.load_from_image_dataset(ad_target_train_dataset)\n",
    "\n",
    "ad_to_be_unlabeled_dataset_20, ad_labeled_dataset_20 = lib.data.stratified_split(\n",
    "    ad_target_train_dataset, test_size=0.2\n",
    ")\n",
    "\n",
    "ad_unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=ad_labeled_dataset_20.parser_func,\n",
    "    preprocessing_func=ad_labeled_dataset_20.preprocessing_func,\n",
    ")\n",
    "ad_unlabeled_dataset_20.load_from_image_dataset(ad_to_be_unlabeled_dataset_20)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in ad_source_train_dataset.samples:\n",
    "    ad_labeled_dataset_20.add(sample_img, sample_label)\n",
    "\n",
    "len(ad_labeled_dataset_20), len(ad_source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ad_source_train_dataset.label_encoder.classes_\n",
    "\n",
    "encodings = {\n",
    "    label: class_name\n",
    "    for label, class_name in enumerate(ad_source_train_dataset.label_encoder.classes_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "#https://arxiv.org/pdf/2405.13698\n",
    "# disable lr for adam\n",
    "exp_lr_scheduler = None\n",
    "\n",
    "\n",
    "torchinfo.summary(tasks.utils.get_model(device=device,\n",
    "                                        replace_fc_layer=True,\n",
    "                                        num_classes=len(encodings)),\n",
    "                                        input_size=(BATCH_SIZE, 3, 30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device,\n",
    "                            replace_fc_layer=True,\n",
    "                            num_classes=len(encodings),\n",
    "                            use_default_weights=False)\n",
    "\n",
    "if AD_FINETUNE_MODEL:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        ad_source_train_loader,\n",
    "        ad_source_val_loader,\n",
    "        output_dir=AD_FINETUNED_MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=1,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation accuracy has been calculated wrong here, ignore it for now\n",
    "plt.plot(np.array(range(len(history[\"train_acc\"]))), history[\"train_acc\"])\n",
    "plt.plot(np.array(range(len(history[\"val_acc\"]))), history[\"val_acc\"])\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device, replace_fc_layer=True, num_classes=len(encodings))\n",
    "\n",
    "if  AD_FINETUNE_TARGET_MODEL:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        ad_target_train_loader,\n",
    "        ad_target_val_loader,\n",
    "        output_dir=AD_FINETUNED_TARGET_MODEL_DIR,\n",
    "        num_epochs=15,\n",
    "        patience=3,\n",
    "        warmup_period=1,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(encodings)\n",
    ")\n",
    "\n",
    "if AD_TRAIN_UNSUPERVISED_MODEL:\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=ad_source_train_dataset,\n",
    "            source_val_dataset=ad_source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            # we can not use padding with unlabeled data\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True, n_workers=8\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=ad_unlabeled_dataset,\n",
    "            target_val_dataset=ad_target_val_dataset,\n",
    "            output_dir=AD_UNSUPERVISED_MODEL_DIR,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, AD_UNSUPERVISED_MODEL_DIR)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Supervised 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(encodings)\n",
    ")\n",
    "\n",
    "if AD_FINETUNE_SEMI_SUPERVISED_MODEL_10:\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            ad_labeled_dataset_10, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=ad_source_val_loader,\n",
    "        output_dir=AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=1,\n",
    "        previous_history=history,\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(encodings)\n",
    ")\n",
    "\n",
    "if AD_TRAIN_SEMI_SUPERVISED_MODEL_10:\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=ad_labeled_dataset_10,\n",
    "            source_val_dataset=ad_source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            # we can not use padding with unlabeled data\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True, n_workers=8\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=ad_unlabeled_dataset_10,\n",
    "            target_val_dataset=ad_target_val_dataset,\n",
    "            output_dir=AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(\n",
    "        model, AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10\n",
    "    )\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(encodings)\n",
    ")\n",
    "\n",
    "if AD_FINETUNE_SEMI_SUPERVISED_MODEL_20:\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            ad_labeled_dataset_20, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=ad_source_val_loader,\n",
    "        output_dir=AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=1,\n",
    "        previous_history=history,\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(encodings)\n",
    ")\n",
    "\n",
    "if AD_TRAIN_SEMI_SUPERVISED_MODEL_20:\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=ad_labeled_dataset_20,\n",
    "            source_val_dataset=ad_source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            # we can not use padding with unlabeled data\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True, n_workers=8\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=ad_unlabeled_dataset_20,\n",
    "            target_val_dataset=ad_target_val_dataset,\n",
    "            output_dir=AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(\n",
    "        model, AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20\n",
    "    )\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
