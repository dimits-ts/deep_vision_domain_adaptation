{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "VAL_SPLIT = .15\n",
    "TEST_SPLIT = .15\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def resnet_preprocessor(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses an image for ResNet model.\n",
    "\n",
    "    :param numpy.ndarray image: The input image.\n",
    "    :return: Preprocessed image.\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    preprocess = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = preprocess(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Lazily loads images from a root directory.\n",
    "    Directory is assumed to be of shape \"<root>/<class_name>/<instance_file>\".\n",
    "    Allows custom functions for reading, preprocessing each image and setting the label encodings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        parser_func: Callable = imageio.imread,\n",
    "        preprocessing_func: Callable[[np.ndarray], np.ndarray] = resnet_preprocessor,\n",
    "        label_encoder=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ImageDataset.\n",
    "\n",
    "        :param str data_dir: Root directory containing the dataset.\n",
    "        :param parser_func: Function to parse images.\n",
    "        :type parser_func: Callable, optional\n",
    "        :param preprocessing_func: Function to preprocess images.\n",
    "        :type preprocessing_func: Callable[[numpy.ndarray], numpy.ndarray], optional\n",
    "        :param label_encoder: Encoder for label encoding.\n",
    "        :type label_encoder: sklearn.preprocessing.LabelEncoder or None, optional\n",
    "        \"\"\"\n",
    "        self.parser_func = parser_func\n",
    "        self.preprocessing_func = preprocessing_func\n",
    "        self.label_encoder = label_encoder\n",
    "        self.samples = self._load_dataset_paths(data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = self.parser_func(image_path)\n",
    "        image = self.preprocessing_func(image)\n",
    "\n",
    "        if not torch.is_tensor(image):\n",
    "            image = torch.tensor(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def _load_dataset_paths(self, data_dir):\n",
    "        \"\"\"\n",
    "        Loads paths of images in the dataset.\n",
    "\n",
    "        :param str data_dir: Root directory containing the dataset.\n",
    "        :return: List of tuples containing image paths and their corresponding labels.\n",
    "        :rtype: List[Tuple[str, int]]\n",
    "        \"\"\"\n",
    "        class_names = os.listdir(data_dir)\n",
    "\n",
    "        if self.label_encoder is None:\n",
    "            self.label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "            self.label_encoder.fit(class_names)\n",
    "\n",
    "        samples = []\n",
    "        for class_name in tqdm(class_names):\n",
    "            class_data_dir = os.path.join(data_dir, class_name)\n",
    "\n",
    "            for file_name in os.listdir(class_data_dir):\n",
    "                samples.append(\n",
    "                    (\n",
    "                        os.path.join(class_data_dir, file_name),\n",
    "                        self.label_encoder.transform([class_name])[0],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_pad(batch):\n",
    "    # Sort the batch by image height in descending order\n",
    "    batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
    "\n",
    "    # Get the maximum height and width among all images in the batch\n",
    "    max_height = max(img.shape[1] for img, _ in batch)\n",
    "    max_width = max(img.shape[2] for img, _ in batch)\n",
    "\n",
    "    # Pad each image to match the maximum height and width\n",
    "    padded_batch = []\n",
    "    for img, label in batch:\n",
    "        # Calculate padding sizes\n",
    "        pad_height = max_height - img.shape[1]\n",
    "        pad_width = max_width - img.shape[2]\n",
    "\n",
    "        # Pad the image\n",
    "        padded_img = torch.nn.functional.pad(img, (0, pad_width, 0, pad_height))\n",
    "\n",
    "        padded_batch.append((padded_img, label))\n",
    "\n",
    "    # Stack images and labels into tensors\n",
    "    images = torch.stack([img for img, _ in padded_batch])\n",
    "    labels = torch.tensor([label for _, label in padded_batch])\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf91a4f1558463ab0a7cfec8f10e43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "dataset = ImageDataset(os.path.join(DATA_DIR, \"amazon\"))\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# Create indices for the dataset\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate split indices\n",
    "val_split = int(np.floor(VAL_SPLIT * dataset_size))\n",
    "test_split = int(np.floor(TEST_SPLIT * dataset_size))\n",
    "\n",
    "# Split indices for train, validation, and test\n",
    "train_indices = indices[val_split + test_split:]\n",
    "val_indices = indices[:val_split]\n",
    "test_indices = indices[val_split:(val_split + test_split)]\n",
    "\n",
    "# Create PT data samplers and loaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_pad\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_pad\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_pad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "sample_image, label = dataset[0]\n",
    "print(sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [2, 1000]                 --\n",
       "├─Conv2d: 1-1                            [2, 64, 150, 150]         9,408\n",
       "├─BatchNorm2d: 1-2                       [2, 64, 150, 150]         128\n",
       "├─ReLU: 1-3                              [2, 64, 150, 150]         --\n",
       "├─MaxPool2d: 1-4                         [2, 64, 75, 75]           --\n",
       "├─Sequential: 1-5                        [2, 64, 75, 75]           --\n",
       "│    └─BasicBlock: 2-1                   [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-1                  [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-3                    [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-4                  [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-6                    [2, 64, 75, 75]           --\n",
       "│    └─BasicBlock: 2-2                   [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-7                  [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-9                    [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-10                 [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-12                   [2, 64, 75, 75]           --\n",
       "│    └─BasicBlock: 2-3                   [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-13                 [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-14            [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-15                   [2, 64, 75, 75]           --\n",
       "│    │    └─Conv2d: 3-16                 [2, 64, 75, 75]           36,864\n",
       "│    │    └─BatchNorm2d: 3-17            [2, 64, 75, 75]           128\n",
       "│    │    └─ReLU: 3-18                   [2, 64, 75, 75]           --\n",
       "├─Sequential: 1-6                        [2, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-4                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-19                 [2, 128, 38, 38]          73,728\n",
       "│    │    └─BatchNorm2d: 3-20            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-21                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-22                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-23            [2, 128, 38, 38]          256\n",
       "│    │    └─Sequential: 3-24             [2, 128, 38, 38]          8,448\n",
       "│    │    └─ReLU: 3-25                   [2, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-5                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-26                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-27            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-28                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-29                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-30            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-31                   [2, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-6                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-32                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-33            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-34                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-35                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-36            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-37                   [2, 128, 38, 38]          --\n",
       "│    └─BasicBlock: 2-7                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-38                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-39            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-40                   [2, 128, 38, 38]          --\n",
       "│    │    └─Conv2d: 3-41                 [2, 128, 38, 38]          147,456\n",
       "│    │    └─BatchNorm2d: 3-42            [2, 128, 38, 38]          256\n",
       "│    │    └─ReLU: 3-43                   [2, 128, 38, 38]          --\n",
       "├─Sequential: 1-7                        [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-8                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-44                 [2, 256, 19, 19]          294,912\n",
       "│    │    └─BatchNorm2d: 3-45            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-46                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-47                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-48            [2, 256, 19, 19]          512\n",
       "│    │    └─Sequential: 3-49             [2, 256, 19, 19]          33,280\n",
       "│    │    └─ReLU: 3-50                   [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-9                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-51                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-52            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-53                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-54                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-55            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-56                   [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-10                  [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-57                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-58            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-59                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-60                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-61            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-62                   [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-11                  [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-63                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-64            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-65                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-66                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-67            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-68                   [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-12                  [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-69                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-70            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-71                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-72                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-73            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-74                   [2, 256, 19, 19]          --\n",
       "│    └─BasicBlock: 2-13                  [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-75                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-76            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-77                   [2, 256, 19, 19]          --\n",
       "│    │    └─Conv2d: 3-78                 [2, 256, 19, 19]          589,824\n",
       "│    │    └─BatchNorm2d: 3-79            [2, 256, 19, 19]          512\n",
       "│    │    └─ReLU: 3-80                   [2, 256, 19, 19]          --\n",
       "├─Sequential: 1-8                        [2, 512, 10, 10]          --\n",
       "│    └─BasicBlock: 2-14                  [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-81                 [2, 512, 10, 10]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-82            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-83                   [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-84                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-85            [2, 512, 10, 10]          1,024\n",
       "│    │    └─Sequential: 3-86             [2, 512, 10, 10]          132,096\n",
       "│    │    └─ReLU: 3-87                   [2, 512, 10, 10]          --\n",
       "│    └─BasicBlock: 2-15                  [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-88                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-89            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-90                   [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-91                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-92            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-93                   [2, 512, 10, 10]          --\n",
       "│    └─BasicBlock: 2-16                  [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-94                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-95            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-96                   [2, 512, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-97                 [2, 512, 10, 10]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-98            [2, 512, 10, 10]          1,024\n",
       "│    │    └─ReLU: 3-99                   [2, 512, 10, 10]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [2, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [2, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 21,797,672\n",
       "Trainable params: 21,797,672\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 13.67\n",
       "==========================================================================================\n",
       "Input size (MB): 2.16\n",
       "Forward/backward pass size (MB): 218.36\n",
       "Params size (MB): 87.19\n",
       "Estimated Total Size (MB): 307.71\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "model = torch.hub.load(\n",
    "    \"pytorch/vision:v0.10.0\", \"resnet34\", weights=torchvision.models.ResNet34_Weights.DEFAULT\n",
    ").to(device)\n",
    "\n",
    "torchinfo.summary(model, input_size=(BATCH_SIZE, 3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device: str,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    val_dataloader: torch.utils.data.DataLoader,\n",
    "    train_size: int,\n",
    "    val_size: int,\n",
    "    output_path: str,\n",
    "    num_epochs: int = 25,\n",
    "    patience: int = 1\n",
    ") -> tuple[nn.Module, dict[str, np.ndarray]] :\n",
    "    dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "    dataset_sizes = {\"train\": train_size, \"val\": val_size}\n",
    "\n",
    "    history = {\"train_loss\": [],\"train_acc\": [], \"eval_loss\": [],\"eval_acc\": []}\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    torch.save(model.state_dict(), output_path)\n",
    "    best_acc = 0.0\n",
    "    # early stopping counter\n",
    "    epochs_no_progress = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = (running_loss / dataset_sizes[phase]).cpu()\n",
    "            epoch_acc = (running_corrects.double() / dataset_sizes[phase]).cpu()\n",
    "\n",
    "            if phase == \"train\":\n",
    "                history[\"train_loss\"].append(epoch_loss)\n",
    "                history[\"train_acc\"].append(epoch_acc)\n",
    "            else:\n",
    "                history[\"eval_loss\"].append(epoch_loss)\n",
    "                history[\"eval_acc\"].append(epoch_acc)\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), output_path)\n",
    "                epochs_no_progress = 0\n",
    "            else:\n",
    "                epochs_no_progress += 1\n",
    "            \n",
    "            # early stopping mechanism\n",
    "            if epochs_no_progress >= patience:\n",
    "                break\n",
    "\n",
    "        print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\n",
    "            f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\"\n",
    "        )\n",
    "        print(f\"Best val Acc: {best_acc:4f}\")\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(output_path))\n",
    "        history = {key: np.array(ls) for key, ls in history.items()}\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e35c754df9c437e8a522a8647cf46e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.95 GiB of which 2.81 MiB is free. Including non-PyTorch memory, this process has 3.87 GiB memory in use. Of the allocated memory 3.73 GiB is allocated by PyTorch, and 74.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer_ft \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m      3\u001b[0m exp_lr_scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer_ft, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffice_finetuned.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, device, train_dataloader, val_dataloader, train_size, val_size, output_path, num_epochs, patience)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.95 GiB of which 2.81 MiB is free. Including non-PyTorch memory, this process has 3.87 GiB memory in use. Of the allocated memory 3.73 GiB is allocated by PyTorch, and 74.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.05)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model, history = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer_ft,\n",
    "    exp_lr_scheduler,\n",
    "    device,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    len(train_indices),\n",
    "    len(val_indices),\n",
    "    output_path=os.path.join(OUTPUT_DIR, \"office_finetuned.pt\"),\n",
    "    num_epochs=1,\n",
    "    patience=3,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
