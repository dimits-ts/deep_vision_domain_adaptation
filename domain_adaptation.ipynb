{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "VAL_SPLIT = .2\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Lazily loads images from a root directory.\n",
    "    Directory is assumed to be of shape \"<root>/<class_name>/<instance_file>\".\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str, parser_func: Callable = imageio.imread):\n",
    "        self.parser_func = parser_func\n",
    "        self.label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "        self.samples = self._load_dataset_paths(data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = self.parser_func(image_path)\n",
    "        image_tensor = torch.tensor(image)\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "    def _load_dataset_paths(self, data_dir):\n",
    "        class_names = os.listdir(data_dir)\n",
    "        self.label_encoder.fit(class_names)\n",
    "\n",
    "        samples = []\n",
    "        for class_name in tqdm(class_names):\n",
    "            class_data_dir = os.path.join(data_dir, class_name)\n",
    "\n",
    "            for file_name in os.listdir(class_data_dir):\n",
    "                samples.append(\n",
    "                    (\n",
    "                        os.path.join(class_data_dir, file_name),\n",
    "                        self.label_encoder.transform([class_name])[0],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64fed554cf046f0ae0f04a9288216dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "dataset = ImageDataset(os.path.join(DATA_DIR, \"amazon\"))\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(VAL_SPLIT * dataset_size))\n",
    "\n",
    "np.random.default_rng(RANDOM_SEED).shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=train_sampler\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=valid_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 300, 3])\n"
     ]
    }
   ],
   "source": [
    "sample_image, label = dataset[0]\n",
    "print(sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2817\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
