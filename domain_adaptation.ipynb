{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Pseudo-labeling Performance Sans Supporting Algorithms and Models\n",
    "\n",
    "---\n",
    "\n",
    "> Dimitris Tsirmpas <br>\n",
    "> MSc in Data Science f3352315 <br>\n",
    "> Athens University of Economics and Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Structure\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "Main files:\n",
    "- domain_adaptation.ipynb: is the main Jupyter Notebook containing the project code\n",
    "- report.pdf: Supplemental material containing Figures, Tables and analysis on the results of the project\n",
    "  \n",
    "Directories:\n",
    "- lib: a library of general functions for Data Science tasks\n",
    "- tasks: task-specific modules\n",
    "- data: the input data\n",
    "- output: the model training output\n",
    "- results: Graphs, Tables and Figures produced in the project\n",
    "- scripts: pre-processing scripts applied to the data\n",
    "\n",
    "Notes:\n",
    "\n",
    "* This notebook mainly discusses the implementation and design decisions of the project. For the theory, experimental procedures\n",
    "and results, consult [the main report](report.pdf).\n",
    "\n",
    "* This notebook does not contain the fundamental code for preprocessing, loading data, training models, implementing the pseudo-labeling procedure\n",
    "or our incremental learning algorithm. The code, as well as extensive comments and documentation, exist in the respective source files in the\n",
    "`tasks` and `lib` directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Office Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lib.data\n",
    "import lib.torch_train_eval\n",
    "import lib.adaptive_train_eval\n",
    "import lib.coral_train\n",
    "\n",
    "import tasks.preprocessing\n",
    "import tasks.utils\n",
    "import tasks.results\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/office\"\n",
    "OUTPUT_DIR = \"output/office\"\n",
    "RESULTS_DIR = \"results\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 1\n",
    "PRINT_STATS_PERIOD = 500\n",
    "\n",
    "SOURCE_DATASET = \"amazon\"\n",
    "SOURCE_VAL_SPLIT = .15\n",
    "SOURCE_TEST_SPLIT = .1\n",
    "\n",
    "TARGET_VAL_SPLIT = .15\n",
    "TARGET_TEST_SPLIT = .15\n",
    "TARGET_DATASET = \"webcam\"\n",
    "\n",
    "RHO = 4\n",
    "SAMPLING_PERIOD = 20\n",
    "\n",
    "FINETUNED_SOURCE__MODEL_DIR = os.path.join(OUTPUT_DIR, \"classifier\")\n",
    "CORAL_SOURCE_MODEL_DIR = os.path.join(OUTPUT_DIR, \"coral\")\n",
    "FINETUNED_TARGET_MODEL_DIR = os.path.join(OUTPUT_DIR, \"target_classifier\")\n",
    "UNSUPERVISED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"unsupervised\")\n",
    "SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20 = os.path.join(OUTPUT_DIR, \"semi-supervised-finetuned-20\")\n",
    "SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20 = os.path.join(OUTPUT_DIR, \"semi-supervised-adaptive-20\")\n",
    "SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10 = os.path.join(OUTPUT_DIR, \"semi-supervised-finetuned-10\")\n",
    "SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10 = os.path.join(OUTPUT_DIR, \"semi-supervised-adaptive-10\")\n",
    "\n",
    "FINETUNE_SOURCE_MODEL = False\n",
    "TRAIN_CORAL_MODEL = False\n",
    "FINETUNE_TARGET_MODEL = False\n",
    "TRAIN_UNSUPERVISED_MODEL = False\n",
    "FINETUNE_SEMI_SUPERVISED_MODEL_20 = False\n",
    "TRAIN_SEMI_SUPERVISED_MODEL_20 = False\n",
    "FINETUNE_SEMI_SUPERVISED_MODEL_10 = False\n",
    "TRAIN_SEMI_SUPERVISED_MODEL_10 = False\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing\n",
    "\n",
    "Most of the work here is done through the ImageDataset class, which lazy loads all images from the given root directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data from the source dataset (Modern Office-31 Amazon domain)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "source_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "source_dataset.load_from_directory(os.path.join(DATA_DIR, SOURCE_DATASET))\n",
    "\n",
    "source_train_dataset, source_val_dataset, source_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        source_dataset, SOURCE_VAL_SPLIT, SOURCE_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "source_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_train_dataset, shuffle=True\n",
    ")\n",
    "source_val_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_val_dataset, shuffle=False\n",
    ")\n",
    "source_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    source_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the respective target dataset (Modern Office-31 Webcam domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    "    label_encoder=source_dataset.label_encoder,  # use same classes\n",
    ")\n",
    "target_dataset.load_from_directory(os.path.join(DATA_DIR, TARGET_DATASET))\n",
    "\n",
    "target_train_dataset, target_val_dataset, target_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        target_dataset, TARGET_VAL_SPLIT, TARGET_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "target_train_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_train_dataset, shuffle=True\n",
    ")\n",
    "target_val_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_val_dataset, shuffle=False\n",
    ")\n",
    "target_test_loader = tasks.preprocessing.single_batch_loader(\n",
    "    target_test_dataset, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import convert the target domain data into unlabelled and labelled datasets. \n",
    "\n",
    "We use a stratified split for all classes, converting 10%/20% of the target domain samples into labelled data and then adding them to the source domain dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "unlabeled_dataset_20.load_from_image_dataset(target_train_dataset)\n",
    "\n",
    "to_be_unlabeled_dataset_20, labeled_dataset_20 = lib.data.stratified_split(\n",
    "    target_train_dataset, test_size=0.2\n",
    ")\n",
    "\n",
    "unlabeled_dataset_20 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=labeled_dataset_20.parser_func,\n",
    "    preprocessing_func=labeled_dataset_20.preprocessing_func,\n",
    ")\n",
    "unlabeled_dataset_20.load_from_image_dataset(to_be_unlabeled_dataset_20)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in source_train_dataset.samples:\n",
    "    labeled_dataset_20.add(sample_img, sample_label)\n",
    "\n",
    "len(labeled_dataset_20), len(source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_unlabeled_dataset_10, labeled_dataset_10 = lib.data.stratified_split(\n",
    "    target_train_dataset, test_size=0.1\n",
    ")\n",
    "\n",
    "unlabeled_dataset_10 = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=labeled_dataset_10.parser_func,\n",
    "    preprocessing_func=labeled_dataset_20.preprocessing_func,\n",
    ")\n",
    "unlabeled_dataset_10.load_from_image_dataset(to_be_unlabeled_dataset_10)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in source_train_dataset.samples:\n",
    "    labeled_dataset_10.add(sample_img, sample_label)\n",
    "\n",
    "len(labeled_dataset_10), len(source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = source_train_dataset.label_encoder.classes_\n",
    "\n",
    "office_encodings = {\n",
    "    label: class_name\n",
    "    for label, class_name in enumerate(source_train_dataset.label_encoder.classes_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source-only model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the ResNet-18 model, pre-trained on the ImageNet dataset. \n",
    "\n",
    "From our experience, keeping the pre-trained classification head of 1000 classes actually significantly speeds up training and avoids overfitting. Thus, we do not replace it with our own 31-class dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "#https://arxiv.org/pdf/2405.13698\n",
    "# disable lr for adam\n",
    "exp_lr_scheduler = None\n",
    "\n",
    "torchinfo.summary(tasks.utils.get_model(device=device), input_size=(BATCH_SIZE, 3, 1500, 1500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finetune the pretrained model on the source domain dataset (Amazon domain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if FINETUNE_SOURCE_MODEL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        source_train_loader,\n",
    "        source_val_loader,\n",
    "        output_dir=FINETUNED_SOURCE__MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the learning curves as well as the classification results on both source and target domain test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that our baseline, the source-only model, can not effectively distinguish between any of the classes (except for the first few)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target only model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure described above is repeated for the model trained on the target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device, replace_fc_layer=True, num_classes=len(office_encodings))\n",
    "\n",
    "if FINETUNE_TARGET_MODEL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=10e-2)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        target_train_loader,\n",
    "        target_val_loader,\n",
    "        output_dir=FINETUNED_TARGET_MODEL_DIR,\n",
    "        num_epochs=100,\n",
    "        patience=10,\n",
    "        warmup_period=25,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=20000,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(FINETUNED_TARGET_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(FINETUNED_TARGET_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, there is no way we can effectively train the target-only model which was supposed to be our baseline. \n",
    "Due to the low number of datapoints and the difficulty of the domain, it immediately overfits. No amount of increasing regularization\n",
    "(such as AdamW with weight_decay=$10^{-2}$) seems to influence the results. Additionally, we can not use data augmentation, since that\n",
    "would make the comparison between models unfair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAL Source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device, replace_fc_layer=True, num_classes=len(office_encodings))\n",
    "\n",
    "if TRAIN_CORAL_MODEL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(CORAL_SOURCE_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=10e-2)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(CORAL_SOURCE_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.coral_train.coral_train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        source_train_dataloader=source_train_loader,\n",
    "        source_val_dataloader=source_val_loader,\n",
    "        target_train_dataloader=target_train_loader,\n",
    "        output_dir=CORAL_SOURCE_MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(CORAL_SOURCE_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(CORAL_SOURCE_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained and evaluated our baselines, we can begin experimenting with the incremental learning procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if TRAIN_UNSUPERVISED_MODEL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(FINETUNED_SOURCE__MODEL_DIR, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=source_train_dataset,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_20,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=UNSUPERVISED_MODEL_DIR,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, UNSUPERVISED_MODEL_DIR)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, office_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised domain adaptation: 10% target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if FINETUNE_SEMI_SUPERVISED_MODEL_10:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            labeled_dataset_10, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=source_val_loader,\n",
    "        output_dir=SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        previous_history=None,\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if TRAIN_SEMI_SUPERVISED_MODEL_10:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_10, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset_10,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=1, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_10,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=None,\n",
    "            previous_target_history=None,\n",
    "            verbose=False\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, office_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised domain adaptation: 20% target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if FINETUNE_SEMI_SUPERVISED_MODEL_20:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            labeled_dataset_20, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=source_val_loader,\n",
    "        output_dir=SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20,\n",
    "        num_epochs=25,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        previous_history=history\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device)\n",
    "\n",
    "if TRAIN_SEMI_SUPERVISED_MODEL_20:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(SEMI_SUPERVISED_FINETUNED_MODEL_DIR_20, \"model.pt\"))\n",
    "    optimizer_ft = optim.Adam(model.parameters())\n",
    "\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=labeled_dataset_20,\n",
    "            source_val_dataset=source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, sampler=sampler, shuffle=False\n",
    "            ),\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=unlabeled_dataset_20,\n",
    "            target_val_dataset=target_val_dataset,\n",
    "            output_dir=SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, office_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST/MNIST-M dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is near-identical to the one running the models on the Modern Office-31 dataset. \n",
    "\n",
    "* We slightly change hyperparameters because of the different nature of the dataset (batch size, $\\rho$, weight decay...)\n",
    "* We change the optimizer from Adam to AdamW (since the second implements weight decay in a mathematically correct manner)\n",
    "* We swap the classification head from the 1000 classes to 10 since:\n",
    "    * There is no correlation between the MNIST and ImageNet classes\n",
    "    * It makes the model smaller and thus harder to overfit, which is likely given the comparatively easier task of MNIST classification\n",
    "* We do not use pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 10e-6\n",
    "RHO = 2.5\n",
    "SAMPLING_PERIOD = 7\n",
    "\n",
    "AD_DATA_DIR = \"data/digits\"\n",
    "AD_OUTPUT_DIR = \"output/digits\"\n",
    "\n",
    "AD_SOURCE_DATASET = \"mnist\"\n",
    "AD_TARGET_DATASET = \"mnist-m\"\n",
    "\n",
    "AD_FINETUNED_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"classifier\")\n",
    "AD_FINETUNED_TARGET_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"target_classifier\")\n",
    "AD_UNSUPERVISED_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"unsupervised\")\n",
    "AD_CORAL_SOURCE_MODEL_DIR = os.path.join(AD_OUTPUT_DIR, \"coral\")\n",
    "AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_LARGE = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-finetuned-large\")\n",
    "AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_LARGE = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-adaptive-large\")\n",
    "AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_SMALL = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-finetuned-small\")\n",
    "AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_SMALL = os.path.join(AD_OUTPUT_DIR, \"semi-supervised-adaptive-small\")\n",
    "\n",
    "AD_FINETUNE_MODEL = False\n",
    "AD_FINETUNE_TARGET_MODEL = False\n",
    "AD_FINETUNE_CORAL_MODEL = False\n",
    "AD_TRAIN_UNSUPERVISED_MODEL = True\n",
    "AD_FINETUNE_SEMI_SUPERVISED_MODEL_LARGE = False\n",
    "AD_TRAIN_SEMI_SUPERVISED_MODEL_LARGE = False\n",
    "AD_FINETUNE_SEMI_SUPERVISED_MODEL_SMALL = False\n",
    "AD_TRAIN_SEMI_SUPERVISED_MODEL_SMALL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baef85f4e1f469f83ced68bbaa33e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ad_source_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "ad_source_dataset.load_from_directory(os.path.join(AD_DATA_DIR, AD_SOURCE_DATASET))\n",
    "\n",
    "ad_source_train_dataset, ad_source_val_dataset, ad_source_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        ad_source_dataset, SOURCE_VAL_SPLIT, SOURCE_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "ad_source_train_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_source_train_dataset, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_source_val_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_source_val_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_source_test_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_source_test_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c9faadb9874e808eec31485d4f1516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ad_target_dataset = lib.data.ImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    "    label_encoder=ad_source_dataset.label_encoder,  # use same classes\n",
    ")\n",
    "ad_target_dataset.load_from_directory(os.path.join(AD_DATA_DIR, AD_TARGET_DATASET))\n",
    "\n",
    "ad_target_train_dataset, ad_target_val_dataset, ad_target_test_dataset = (\n",
    "    lib.data.train_val_test_split(\n",
    "        ad_target_dataset, TARGET_VAL_SPLIT, TARGET_TEST_SPLIT\n",
    "    )\n",
    ")\n",
    "\n",
    "ad_target_train_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_target_train_dataset, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_target_val_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_target_val_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")\n",
    "ad_target_test_loader = tasks.preprocessing.create_padded_dataloader(\n",
    "    ad_target_test_dataset, shuffle=False, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_unlabeled_dataset = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "ad_unlabeled_dataset.load_from_image_dataset(ad_target_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52601, 52500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_to_be_unlabeled_dataset_small, ad_labeled_dataset_small = lib.data.stratified_split(\n",
    "    ad_target_train_dataset, test_size=100/len(ad_target_train_dataset)\n",
    ")\n",
    "\n",
    "ad_unlabeled_dataset_small = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=ad_labeled_dataset_small.parser_func,\n",
    "    preprocessing_func=ad_source_dataset.preprocessing_func,\n",
    ")\n",
    "ad_unlabeled_dataset_small.load_from_image_dataset(ad_to_be_unlabeled_dataset_small)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in ad_source_train_dataset.samples:\n",
    "    ad_labeled_dataset_small.add(sample_img, sample_label)\n",
    "\n",
    "len(ad_labeled_dataset_small), len(ad_source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54848, 52500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_unlabeled_dataset_large = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=tasks.preprocessing.image_read_func,\n",
    "    preprocessing_func=tasks.preprocessing.resnet_preprocessor,\n",
    ")\n",
    "ad_unlabeled_dataset_large.load_from_image_dataset(ad_target_train_dataset)\n",
    "\n",
    "ad_to_be_unlabeled_dataset_large, ad_labeled_dataset_large = lib.data.stratified_split(\n",
    "    ad_target_train_dataset, test_size=0.05\n",
    ")\n",
    "\n",
    "ad_unlabeled_dataset_large = lib.data.UnlabeledImageDataset(\n",
    "    parser_func=ad_labeled_dataset_large.parser_func,\n",
    "    preprocessing_func=ad_labeled_dataset_large.preprocessing_func,\n",
    ")\n",
    "ad_unlabeled_dataset_large.load_from_image_dataset(ad_to_be_unlabeled_dataset_large)\n",
    "\n",
    "# combine data from both domain and target datasets\n",
    "for sample_img, sample_label in ad_source_train_dataset.samples:\n",
    "    ad_labeled_dataset_large.add(sample_img, sample_label)\n",
    "\n",
    "len(ad_labeled_dataset_large), len(ad_source_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ad_source_train_dataset.label_encoder.classes_\n",
    "\n",
    "mnist_encodings = {\n",
    "    label: class_name\n",
    "    for label, class_name in enumerate(ad_source_train_dataset.label_encoder.classes_)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [16, 10]                  --\n",
       "├─Conv2d: 1-1                            [16, 64, 15, 15]          9,408\n",
       "├─BatchNorm2d: 1-2                       [16, 64, 15, 15]          128\n",
       "├─ReLU: 1-3                              [16, 64, 15, 15]          --\n",
       "├─MaxPool2d: 1-4                         [16, 64, 8, 8]            --\n",
       "├─Sequential: 1-5                        [16, 64, 8, 8]            --\n",
       "│    └─BasicBlock: 2-1                   [16, 64, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-1                  [16, 64, 8, 8]            36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [16, 64, 8, 8]            128\n",
       "│    │    └─ReLU: 3-3                    [16, 64, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-4                  [16, 64, 8, 8]            36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [16, 64, 8, 8]            128\n",
       "│    │    └─ReLU: 3-6                    [16, 64, 8, 8]            --\n",
       "│    └─BasicBlock: 2-2                   [16, 64, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-7                  [16, 64, 8, 8]            36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [16, 64, 8, 8]            128\n",
       "│    │    └─ReLU: 3-9                    [16, 64, 8, 8]            --\n",
       "│    │    └─Conv2d: 3-10                 [16, 64, 8, 8]            36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [16, 64, 8, 8]            128\n",
       "│    │    └─ReLU: 3-12                   [16, 64, 8, 8]            --\n",
       "├─Sequential: 1-6                        [16, 128, 4, 4]           --\n",
       "│    └─BasicBlock: 2-3                   [16, 128, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-13                 [16, 128, 4, 4]           73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [16, 128, 4, 4]           256\n",
       "│    │    └─ReLU: 3-15                   [16, 128, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-16                 [16, 128, 4, 4]           147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [16, 128, 4, 4]           256\n",
       "│    │    └─Sequential: 3-18             [16, 128, 4, 4]           8,448\n",
       "│    │    └─ReLU: 3-19                   [16, 128, 4, 4]           --\n",
       "│    └─BasicBlock: 2-4                   [16, 128, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-20                 [16, 128, 4, 4]           147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [16, 128, 4, 4]           256\n",
       "│    │    └─ReLU: 3-22                   [16, 128, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-23                 [16, 128, 4, 4]           147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [16, 128, 4, 4]           256\n",
       "│    │    └─ReLU: 3-25                   [16, 128, 4, 4]           --\n",
       "├─Sequential: 1-7                        [16, 256, 2, 2]           --\n",
       "│    └─BasicBlock: 2-5                   [16, 256, 2, 2]           --\n",
       "│    │    └─Conv2d: 3-26                 [16, 256, 2, 2]           294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [16, 256, 2, 2]           512\n",
       "│    │    └─ReLU: 3-28                   [16, 256, 2, 2]           --\n",
       "│    │    └─Conv2d: 3-29                 [16, 256, 2, 2]           589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [16, 256, 2, 2]           512\n",
       "│    │    └─Sequential: 3-31             [16, 256, 2, 2]           33,280\n",
       "│    │    └─ReLU: 3-32                   [16, 256, 2, 2]           --\n",
       "│    └─BasicBlock: 2-6                   [16, 256, 2, 2]           --\n",
       "│    │    └─Conv2d: 3-33                 [16, 256, 2, 2]           589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [16, 256, 2, 2]           512\n",
       "│    │    └─ReLU: 3-35                   [16, 256, 2, 2]           --\n",
       "│    │    └─Conv2d: 3-36                 [16, 256, 2, 2]           589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [16, 256, 2, 2]           512\n",
       "│    │    └─ReLU: 3-38                   [16, 256, 2, 2]           --\n",
       "├─Sequential: 1-8                        [16, 512, 1, 1]           --\n",
       "│    └─BasicBlock: 2-7                   [16, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-39                 [16, 512, 1, 1]           1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [16, 512, 1, 1]           1,024\n",
       "│    │    └─ReLU: 3-41                   [16, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-42                 [16, 512, 1, 1]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [16, 512, 1, 1]           1,024\n",
       "│    │    └─Sequential: 3-44             [16, 512, 1, 1]           132,096\n",
       "│    │    └─ReLU: 3-45                   [16, 512, 1, 1]           --\n",
       "│    └─BasicBlock: 2-8                   [16, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-46                 [16, 512, 1, 1]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [16, 512, 1, 1]           1,024\n",
       "│    │    └─ReLU: 3-48                   [16, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-49                 [16, 512, 1, 1]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [16, 512, 1, 1]           1,024\n",
       "│    │    └─ReLU: 3-51                   [16, 512, 1, 1]           --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [16, 512, 1, 1]           --\n",
       "├─Linear: 1-10                           [16, 10]                  5,130\n",
       "==========================================================================================\n",
       "Total params: 11,181,642\n",
       "Trainable params: 11,181,642\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 587.75\n",
       "==========================================================================================\n",
       "Input size (MB): 0.17\n",
       "Forward/backward pass size (MB): 12.47\n",
       "Params size (MB): 44.73\n",
       "Estimated Total Size (MB): 57.37\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "#https://arxiv.org/pdf/2405.13698\n",
    "# disable lr for adam\n",
    "exp_lr_scheduler = None\n",
    "\n",
    "\n",
    "torchinfo.summary(tasks.utils.get_model(device=device,\n",
    "                                        replace_fc_layer=True,\n",
    "                                        num_classes=len(mnist_encodings)),\n",
    "                                        input_size=(BATCH_SIZE, 3, 30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device,\n",
    "                            replace_fc_layer=True,\n",
    "                            num_classes=len(mnist_encodings),\n",
    "                            use_default_weights=False)\n",
    "\n",
    "if AD_FINETUNE_MODEL:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        ad_source_train_loader,\n",
    "        ad_source_val_loader,\n",
    "        output_dir=AD_FINETUNED_MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=1,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_FINETUNED_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device, replace_fc_layer=True, num_classes=len(mnist_encodings))\n",
    "\n",
    "if AD_FINETUNE_CORAL_MODEL:\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_CORAL_SOURCE_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_CORAL_SOURCE_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        ad_target_train_loader,\n",
    "        ad_target_val_loader,\n",
    "        output_dir=AD_CORAL_SOURCE_MODEL_DIR,\n",
    "        num_epochs=15,\n",
    "        patience=3,\n",
    "        warmup_period=1,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_CORAL_SOURCE_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_CORAL_SOURCE_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(history)\n",
    "tasks.results.learning_curves_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAL Source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(device=device, replace_fc_layer=True, num_classes=len(mnist_encodings))\n",
    "\n",
    "if AD_FINETUNE_CORAL_MODEL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\"))\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_CORAL_SOURCE_MODEL_DIR, \"history.pickle\"))\n",
    "    model, history = lib.coral_train.coral_train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        exp_lr_scheduler,\n",
    "        device,\n",
    "        source_train_dataloader=ad_source_train_loader,\n",
    "        source_val_dataloader=ad_source_val_loader,\n",
    "        target_train_dataloader=ad_target_train_loader,\n",
    "        output_dir=AD_CORAL_SOURCE_MODEL_DIR,\n",
    "        num_epochs=50,\n",
    "        patience=5,\n",
    "        warmup_period=5,\n",
    "        gradient_accumulation=1,\n",
    "        previous_history=history,\n",
    "        train_stats_period=PRINT_STATS_PERIOD,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_CORAL_SOURCE_MODEL_DIR, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_CORAL_SOURCE_MODEL_DIR, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimits/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/159\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimits/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1248/46958 images on threshold 0.9215826759537272\n",
      "[('data/digits/mnist-m/2/00010680.png', 2), ('data/digits/mnist-m/0/00051980.png', 0), ('data/digits/mnist-m/7/00031799.png', 7), ('data/digits/mnist-m/0/00028160.png', 0), ('data/digits/mnist-m/8/00004092.png', 8), ('data/digits/mnist-m/5/00016980.png', 5), ('data/digits/mnist-m/8/00018439.png', 8), ('data/digits/mnist-m/3/00050644.png', 3), ('data/digits/mnist-m/9/00058243.png', 9), ('data/digits/mnist-m/9/00045621.png', 9), ('data/digits/mnist-m/4/00005639.png', 4), ('data/digits/mnist-m/0/00035634.png', 0), ('data/digits/mnist-m/1/00031038.png', 1), ('data/digits/mnist-m/9/00026173.png', 9), ('data/digits/mnist-m/0/00056512.png', 0), ('data/digits/mnist-m/3/00039136.png', 8), ('data/digits/mnist-m/3/00052476.png', 3), ('data/digits/mnist-m/6/00014801.png', 6), ('data/digits/mnist-m/4/00046564.png', 4), ('data/digits/mnist-m/2/00052938.png', 7), ('data/digits/mnist-m/8/00010726.png', 9), ('data/digits/mnist-m/3/00007412.png', 3), ('data/digits/mnist-m/8/00019351.png', 8), ('data/digits/mnist-m/2/00051614.png', 2), ('data/digits/mnist-m/0/00000485.png', 0), ('data/digits/mnist-m/9/00055815.png', 9), ('data/digits/mnist-m/7/00030670.png', 7), ('data/digits/mnist-m/8/00051850.png', 8), ('data/digits/mnist-m/8/00001780.png', 8), ('data/digits/mnist-m/2/00021773.png', 2), ('data/digits/mnist-m/9/00010819.png', 9), ('data/digits/mnist-m/4/00042750.png', 4), ('data/digits/mnist-m/9/00002637.png', 0), ('data/digits/mnist-m/0/00005886.png', 0), ('data/digits/mnist-m/7/00005795.png', 7), ('data/digits/mnist-m/7/00008143.png', 7), ('data/digits/mnist-m/0/00058012.png', 0), ('data/digits/mnist-m/3/00007046.png', 3), ('data/digits/mnist-m/9/00044154.png', 9), ('data/digits/mnist-m/0/00036198.png', 0), ('data/digits/mnist-m/8/00023448.png', 8), ('data/digits/mnist-m/4/00044275.png', 4), ('data/digits/mnist-m/4/00009116.png', 4), ('data/digits/mnist-m/8/00055284.png', 8), ('data/digits/mnist-m/7/00051500.png', 7), ('data/digits/mnist-m/9/00014934.png', 9), ('data/digits/mnist-m/9/00054504.png', 9), ('data/digits/mnist-m/0/00032184.png', 0), ('data/digits/mnist-m/4/00003639.png', 4), ('data/digits/mnist-m/9/00054144.png', 9), ('data/digits/mnist-m/8/00053651.png', 8), ('data/digits/mnist-m/7/00032853.png', 7), ('data/digits/mnist-m/3/00022999.png', 3), ('data/digits/mnist-m/8/00013866.png', 9), ('data/digits/mnist-m/8/00041347.png', 8), ('data/digits/mnist-m/7/00051429.png', 7), ('data/digits/mnist-m/8/00049947.png', 8), ('data/digits/mnist-m/9/00018401.png', 9), ('data/digits/mnist-m/7/00004781.png', 7), ('data/digits/mnist-m/0/00000818.png', 0), ('data/digits/mnist-m/3/00024883.png', 3), ('data/digits/mnist-m/0/00014820.png', 0), ('data/digits/mnist-m/2/00008238.png', 2), ('data/digits/mnist-m/8/00013620.png', 8), ('data/digits/mnist-m/4/00003596.png', 4), ('data/digits/mnist-m/0/00031109.png', 0), ('data/digits/mnist-m/3/00017208.png', 3), ('data/digits/mnist-m/7/00023010.png', 7), ('data/digits/mnist-m/3/00050969.png', 3), ('data/digits/mnist-m/5/00036602.png', 5), ('data/digits/mnist-m/4/00002424.png', 4), ('data/digits/mnist-m/8/00047948.png', 8), ('data/digits/mnist-m/4/00007402.png', 4), ('data/digits/mnist-m/8/00038067.png', 8), ('data/digits/mnist-m/8/00004145.png', 8), ('data/digits/mnist-m/5/00004830.png', 5), ('data/digits/mnist-m/7/00038783.png', 7), ('data/digits/mnist-m/9/00044586.png', 9), ('data/digits/mnist-m/8/00052764.png', 8), ('data/digits/mnist-m/4/00036286.png', 4), ('data/digits/mnist-m/0/00001625.png', 0), ('data/digits/mnist-m/5/00029569.png', 5), ('data/digits/mnist-m/2/00025148.png', 2), ('data/digits/mnist-m/7/00041698.png', 7), ('data/digits/mnist-m/5/00033235.png', 8), ('data/digits/mnist-m/3/00028418.png', 3), ('data/digits/mnist-m/0/00013726.png', 0), ('data/digits/mnist-m/9/00027397.png', 9), ('data/digits/mnist-m/4/00007047.png', 4), ('data/digits/mnist-m/4/00013104.png', 4), ('data/digits/mnist-m/2/00001539.png', 2), ('data/digits/mnist-m/4/00002886.png', 4), ('data/digits/mnist-m/4/00021541.png', 4), ('data/digits/mnist-m/3/00007861.png', 3), ('data/digits/mnist-m/9/00007208.png', 9), ('data/digits/mnist-m/9/00032909.png', 0), ('data/digits/mnist-m/9/00050248.png', 0), ('data/digits/mnist-m/0/00035348.png', 0), ('data/digits/mnist-m/3/00049821.png', 3), ('data/digits/mnist-m/0/00026980.png', 0), ('data/digits/mnist-m/6/00007209.png', 8), ('data/digits/mnist-m/2/00008168.png', 2), ('data/digits/mnist-m/8/00036429.png', 8), ('data/digits/mnist-m/7/00033996.png', 7), ('data/digits/mnist-m/2/00001727.png', 2), ('data/digits/mnist-m/7/00006752.png', 7), ('data/digits/mnist-m/3/00033400.png', 3), ('data/digits/mnist-m/4/00046128.png', 4), ('data/digits/mnist-m/4/00032458.png', 4), ('data/digits/mnist-m/5/00054528.png', 5), ('data/digits/mnist-m/0/00018073.png', 0), ('data/digits/mnist-m/2/00057416.png', 2), ('data/digits/mnist-m/9/00000080.png', 8), ('data/digits/mnist-m/3/00031722.png', 3), ('data/digits/mnist-m/4/00048919.png', 4), ('data/digits/mnist-m/7/00022348.png', 7), ('data/digits/mnist-m/8/00005140.png', 8), ('data/digits/mnist-m/7/00005412.png', 7), ('data/digits/mnist-m/8/00015861.png', 8), ('data/digits/mnist-m/3/00058163.png', 3), ('data/digits/mnist-m/4/00003088.png', 4), ('data/digits/mnist-m/4/00000855.png', 4), ('data/digits/mnist-m/1/00025785.png', 1), ('data/digits/mnist-m/4/00047330.png', 4), ('data/digits/mnist-m/7/00048450.png', 7), ('data/digits/mnist-m/9/00041924.png', 0), ('data/digits/mnist-m/8/00050133.png', 8), ('data/digits/mnist-m/0/00015208.png', 0), ('data/digits/mnist-m/4/00005500.png', 4), ('data/digits/mnist-m/3/00024336.png', 3), ('data/digits/mnist-m/9/00015302.png', 9), ('data/digits/mnist-m/8/00050626.png', 8), ('data/digits/mnist-m/0/00007012.png', 0), ('data/digits/mnist-m/3/00018503.png', 3), ('data/digits/mnist-m/7/00042427.png', 7), ('data/digits/mnist-m/4/00021480.png', 4), ('data/digits/mnist-m/0/00004033.png', 0), ('data/digits/mnist-m/8/00006372.png', 8), ('data/digits/mnist-m/2/00001988.png', 2), ('data/digits/mnist-m/7/00056392.png', 7), ('data/digits/mnist-m/4/00020149.png', 4), ('data/digits/mnist-m/5/00007114.png', 5), ('data/digits/mnist-m/7/00025155.png', 7), ('data/digits/mnist-m/0/00025729.png', 0), ('data/digits/mnist-m/8/00019131.png', 8), ('data/digits/mnist-m/8/00004272.png', 8), ('data/digits/mnist-m/8/00012295.png', 8), ('data/digits/mnist-m/7/00016717.png', 7), ('data/digits/mnist-m/7/00001106.png', 7), ('data/digits/mnist-m/2/00058019.png', 2), ('data/digits/mnist-m/2/00022446.png', 2), ('data/digits/mnist-m/1/00001697.png', 1), ('data/digits/mnist-m/8/00055026.png', 8), ('data/digits/mnist-m/9/00010374.png', 9), ('data/digits/mnist-m/7/00007352.png', 7), ('data/digits/mnist-m/1/00016468.png', 1), ('data/digits/mnist-m/2/00014800.png', 2), ('data/digits/mnist-m/9/00009633.png', 9), ('data/digits/mnist-m/7/00000994.png', 7), ('data/digits/mnist-m/4/00005393.png', 4), ('data/digits/mnist-m/9/00007541.png', 9), ('data/digits/mnist-m/2/00044710.png', 2), ('data/digits/mnist-m/4/00002185.png', 4), ('data/digits/mnist-m/4/00015615.png', 4), ('data/digits/mnist-m/0/00045966.png', 0), ('data/digits/mnist-m/0/00053800.png', 0), ('data/digits/mnist-m/9/00025824.png', 9), ('data/digits/mnist-m/2/00005167.png', 2), ('data/digits/mnist-m/4/00001212.png', 4), ('data/digits/mnist-m/4/00021743.png', 4), ('data/digits/mnist-m/3/00008528.png', 3), ('data/digits/mnist-m/9/00018694.png', 9), ('data/digits/mnist-m/8/00000800.png', 8), ('data/digits/mnist-m/2/00022486.png', 2), ('data/digits/mnist-m/3/00030642.png', 3), ('data/digits/mnist-m/4/00012595.png', 4), ('data/digits/mnist-m/2/00012238.png', 2), ('data/digits/mnist-m/4/00030542.png', 4), ('data/digits/mnist-m/3/00006545.png', 3), ('data/digits/mnist-m/8/00013849.png', 8), ('data/digits/mnist-m/9/00031041.png', 9), ('data/digits/mnist-m/8/00037051.png', 8), ('data/digits/mnist-m/4/00030736.png', 4), ('data/digits/mnist-m/8/00049332.png', 8), ('data/digits/mnist-m/7/00001055.png', 7), ('data/digits/mnist-m/0/00008851.png', 0), ('data/digits/mnist-m/4/00048327.png', 4), ('data/digits/mnist-m/0/00018599.png', 0), ('data/digits/mnist-m/0/00006179.png', 0), ('data/digits/mnist-m/7/00040451.png', 7), ('data/digits/mnist-m/7/00009571.png', 7), ('data/digits/mnist-m/7/00055608.png', 7), ('data/digits/mnist-m/7/00003202.png', 7), ('data/digits/mnist-m/0/00022954.png', 0), ('data/digits/mnist-m/4/00034947.png', 4), ('data/digits/mnist-m/0/00014034.png', 0), ('data/digits/mnist-m/8/00052983.png', 8), ('data/digits/mnist-m/8/00037870.png', 8), ('data/digits/mnist-m/5/00014709.png', 5), ('data/digits/mnist-m/2/00003166.png', 2), ('data/digits/mnist-m/0/00019072.png', 0), ('data/digits/mnist-m/3/00029913.png', 3), ('data/digits/mnist-m/8/00018195.png', 8), ('data/digits/mnist-m/9/00009229.png', 9), ('data/digits/mnist-m/8/00016284.png', 8), ('data/digits/mnist-m/7/00048874.png', 7), ('data/digits/mnist-m/4/00000412.png', 4), ('data/digits/mnist-m/8/00023405.png', 8), ('data/digits/mnist-m/9/00001659.png', 9), ('data/digits/mnist-m/9/00051919.png', 9), ('data/digits/mnist-m/7/00024700.png', 7), ('data/digits/mnist-m/5/00041045.png', 5), ('data/digits/mnist-m/4/00031050.png', 4), ('data/digits/mnist-m/4/00011323.png', 4), ('data/digits/mnist-m/8/00007777.png', 8), ('data/digits/mnist-m/2/00022878.png', 2), ('data/digits/mnist-m/3/00040818.png', 3), ('data/digits/mnist-m/7/00044170.png', 7), ('data/digits/mnist-m/7/00028880.png', 7), ('data/digits/mnist-m/7/00004255.png', 7), ('data/digits/mnist-m/3/00034846.png', 3), ('data/digits/mnist-m/7/00052480.png', 7), ('data/digits/mnist-m/4/00049211.png', 4), ('data/digits/mnist-m/4/00047771.png', 4), ('data/digits/mnist-m/2/00045096.png', 2), ('data/digits/mnist-m/8/00004572.png', 8), ('data/digits/mnist-m/9/00038030.png', 9), ('data/digits/mnist-m/0/00052558.png', 0), ('data/digits/mnist-m/9/00003999.png', 9), ('data/digits/mnist-m/7/00001789.png', 7), ('data/digits/mnist-m/4/00026546.png', 4), ('data/digits/mnist-m/9/00010102.png', 9), ('data/digits/mnist-m/4/00008802.png', 4), ('data/digits/mnist-m/7/00013834.png', 7), ('data/digits/mnist-m/4/00046711.png', 4), ('data/digits/mnist-m/3/00051388.png', 3), ('data/digits/mnist-m/4/00007092.png', 4), ('data/digits/mnist-m/9/00014146.png', 9), ('data/digits/mnist-m/0/00036937.png', 0), ('data/digits/mnist-m/7/00057110.png', 7), ('data/digits/mnist-m/4/00021655.png', 4), ('data/digits/mnist-m/7/00001306.png', 7), ('data/digits/mnist-m/8/00058150.png', 8), ('data/digits/mnist-m/8/00015424.png', 9), ('data/digits/mnist-m/0/00039360.png', 0), ('data/digits/mnist-m/8/00007433.png', 8), ('data/digits/mnist-m/4/00043600.png', 4), ('data/digits/mnist-m/7/00022018.png', 7), ('data/digits/mnist-m/7/00007362.png', 7), ('data/digits/mnist-m/5/00011790.png', 5), ('data/digits/mnist-m/7/00056680.png', 7), ('data/digits/mnist-m/9/00042500.png', 9), ('data/digits/mnist-m/4/00052652.png', 4), ('data/digits/mnist-m/5/00002452.png', 5), ('data/digits/mnist-m/1/00002416.png', 1), ('data/digits/mnist-m/2/00003577.png', 2), ('data/digits/mnist-m/8/00001694.png', 8), ('data/digits/mnist-m/9/00028808.png', 9), ('data/digits/mnist-m/0/00037432.png', 0), ('data/digits/mnist-m/4/00006332.png', 4), ('data/digits/mnist-m/7/00020498.png', 7), ('data/digits/mnist-m/4/00015027.png', 4), ('data/digits/mnist-m/3/00049981.png', 3), ('data/digits/mnist-m/2/00025854.png', 2), ('data/digits/mnist-m/4/00025682.png', 4), ('data/digits/mnist-m/4/00048448.png', 4), ('data/digits/mnist-m/7/00002252.png', 7), ('data/digits/mnist-m/9/00040128.png', 9), ('data/digits/mnist-m/7/00010708.png', 7), ('data/digits/mnist-m/7/00022220.png', 7), ('data/digits/mnist-m/9/00007094.png', 9), ('data/digits/mnist-m/9/00017550.png', 9), ('data/digits/mnist-m/9/00016388.png', 9), ('data/digits/mnist-m/4/00010870.png', 4), ('data/digits/mnist-m/8/00001560.png', 8), ('data/digits/mnist-m/0/00029414.png', 0), ('data/digits/mnist-m/3/00002392.png', 3), ('data/digits/mnist-m/9/00000733.png', 9), ('data/digits/mnist-m/2/00037838.png', 7), ('data/digits/mnist-m/0/00050852.png', 0), ('data/digits/mnist-m/7/00016630.png', 7), ('data/digits/mnist-m/3/00045135.png', 3), ('data/digits/mnist-m/4/00058420.png', 4), ('data/digits/mnist-m/8/00008456.png', 8), ('data/digits/mnist-m/0/00025626.png', 0), ('data/digits/mnist-m/0/00019117.png', 0), ('data/digits/mnist-m/4/00004473.png', 4), ('data/digits/mnist-m/9/00022692.png', 9), ('data/digits/mnist-m/2/00022792.png', 2), ('data/digits/mnist-m/6/00029147.png', 6), ('data/digits/mnist-m/8/00044654.png', 8), ('data/digits/mnist-m/4/00042263.png', 4), ('data/digits/mnist-m/9/00000558.png', 9), ('data/digits/mnist-m/4/00020337.png', 4), ('data/digits/mnist-m/3/00000992.png', 3), ('data/digits/mnist-m/0/00058462.png', 0), ('data/digits/mnist-m/8/00034382.png', 8), ('data/digits/mnist-m/2/00036064.png', 2), ('data/digits/mnist-m/7/00024907.png', 7), ('data/digits/mnist-m/2/00022930.png', 2), ('data/digits/mnist-m/4/00036630.png', 4), ('data/digits/mnist-m/3/00005193.png', 3), ('data/digits/mnist-m/7/00050360.png', 7), ('data/digits/mnist-m/4/00023979.png', 4), ('data/digits/mnist-m/3/00001144.png', 3), ('data/digits/mnist-m/7/00000229.png', 7), ('data/digits/mnist-m/8/00054437.png', 8), ('data/digits/mnist-m/9/00033252.png', 0), ('data/digits/mnist-m/0/00005899.png', 0), ('data/digits/mnist-m/3/00038735.png', 3), ('data/digits/mnist-m/7/00006525.png', 7), ('data/digits/mnist-m/0/00003514.png', 0), ('data/digits/mnist-m/4/00011585.png', 4), ('data/digits/mnist-m/9/00025712.png', 9), ('data/digits/mnist-m/9/00015894.png', 9), ('data/digits/mnist-m/8/00046631.png', 8), ('data/digits/mnist-m/7/00055384.png', 7), ('data/digits/mnist-m/7/00027098.png', 7), ('data/digits/mnist-m/0/00050042.png', 0), ('data/digits/mnist-m/8/00045351.png', 8), ('data/digits/mnist-m/4/00034221.png', 4), ('data/digits/mnist-m/7/00046697.png', 7), ('data/digits/mnist-m/0/00003242.png', 0), ('data/digits/mnist-m/9/00052923.png', 9), ('data/digits/mnist-m/0/00025101.png', 0), ('data/digits/mnist-m/4/00014103.png', 4), ('data/digits/mnist-m/0/00053932.png', 0), ('data/digits/mnist-m/5/00007612.png', 5), ('data/digits/mnist-m/8/00005430.png', 8), ('data/digits/mnist-m/2/00010122.png', 2), ('data/digits/mnist-m/3/00002453.png', 3), ('data/digits/mnist-m/7/00000904.png', 7), ('data/digits/mnist-m/8/00058156.png', 8), ('data/digits/mnist-m/5/00026823.png', 5), ('data/digits/mnist-m/9/00042192.png', 9), ('data/digits/mnist-m/9/00038182.png', 9), ('data/digits/mnist-m/4/00003169.png', 4), ('data/digits/mnist-m/4/00010679.png', 4), ('data/digits/mnist-m/9/00041274.png', 0), ('data/digits/mnist-m/5/00037551.png', 5), ('data/digits/mnist-m/7/00034223.png', 7), ('data/digits/mnist-m/9/00042360.png', 0), ('data/digits/mnist-m/7/00039760.png', 7), ('data/digits/mnist-m/8/00053653.png', 8), ('data/digits/mnist-m/3/00004234.png', 3), ('data/digits/mnist-m/8/00043335.png', 8), ('data/digits/mnist-m/0/00012550.png', 0), ('data/digits/mnist-m/7/00049635.png', 7), ('data/digits/mnist-m/7/00035176.png', 7), ('data/digits/mnist-m/7/00037550.png', 7), ('data/digits/mnist-m/8/00012513.png', 8), ('data/digits/mnist-m/9/00011089.png', 9), ('data/digits/mnist-m/6/00035537.png', 8), ('data/digits/mnist-m/4/00005725.png', 4), ('data/digits/mnist-m/8/00004582.png', 8), ('data/digits/mnist-m/2/00020042.png', 2), ('data/digits/mnist-m/7/00012526.png', 7), ('data/digits/mnist-m/7/00019663.png', 7), ('data/digits/mnist-m/2/00041579.png', 2), ('data/digits/mnist-m/2/00041087.png', 2), ('data/digits/mnist-m/2/00006423.png', 2), ('data/digits/mnist-m/9/00043738.png', 9), ('data/digits/mnist-m/2/00033680.png', 2), ('data/digits/mnist-m/3/00052379.png', 3), ('data/digits/mnist-m/0/00028408.png', 0), ('data/digits/mnist-m/0/00001534.png', 0), ('data/digits/mnist-m/0/00055664.png', 0), ('data/digits/mnist-m/8/00037149.png', 8), ('data/digits/mnist-m/3/00045088.png', 3), ('data/digits/mnist-m/4/00037450.png', 9), ('data/digits/mnist-m/9/00016617.png', 9), ('data/digits/mnist-m/4/00040606.png', 4), ('data/digits/mnist-m/7/00042594.png', 7), ('data/digits/mnist-m/4/00036970.png', 4), ('data/digits/mnist-m/4/00028806.png', 4), ('data/digits/mnist-m/4/00048389.png', 4), ('data/digits/mnist-m/2/00024012.png', 2), ('data/digits/mnist-m/9/00026428.png', 9), ('data/digits/mnist-m/9/00053515.png', 9), ('data/digits/mnist-m/9/00017181.png', 9), ('data/digits/mnist-m/4/00011040.png', 4), ('data/digits/mnist-m/6/00051977.png', 6), ('data/digits/mnist-m/3/00049859.png', 3), ('data/digits/mnist-m/8/00015406.png', 8), ('data/digits/mnist-m/7/00048832.png', 7), ('data/digits/mnist-m/9/00002478.png', 9), ('data/digits/mnist-m/5/00004462.png', 5), ('data/digits/mnist-m/7/00058571.png', 7), ('data/digits/mnist-m/0/00004284.png', 0), ('data/digits/mnist-m/7/00035978.png', 7), ('data/digits/mnist-m/0/00031272.png', 0), ('data/digits/mnist-m/5/00015611.png', 5), ('data/digits/mnist-m/0/00045760.png', 0), ('data/digits/mnist-m/2/00002205.png', 2), ('data/digits/mnist-m/8/00014474.png', 8), ('data/digits/mnist-m/4/00019378.png', 4), ('data/digits/mnist-m/3/00045656.png', 2), ('data/digits/mnist-m/4/00006865.png', 4), ('data/digits/mnist-m/3/00006413.png', 3), ('data/digits/mnist-m/0/00026728.png', 0), ('data/digits/mnist-m/7/00037657.png', 7), ('data/digits/mnist-m/7/00033082.png', 7), ('data/digits/mnist-m/8/00019043.png', 8), ('data/digits/mnist-m/4/00045213.png', 4), ('data/digits/mnist-m/0/00035708.png', 0), ('data/digits/mnist-m/2/00013452.png', 7), ('data/digits/mnist-m/7/00030902.png', 7), ('data/digits/mnist-m/2/00039338.png', 2), ('data/digits/mnist-m/7/00050147.png', 7), ('data/digits/mnist-m/0/00033066.png', 8), ('data/digits/mnist-m/8/00052892.png', 8), ('data/digits/mnist-m/0/00003632.png', 0), ('data/digits/mnist-m/9/00025532.png', 9), ('data/digits/mnist-m/7/00043990.png', 7), ('data/digits/mnist-m/4/00048635.png', 4), ('data/digits/mnist-m/7/00003061.png', 7), ('data/digits/mnist-m/3/00012968.png', 3), ('data/digits/mnist-m/9/00028168.png', 9), ('data/digits/mnist-m/4/00044688.png', 4), ('data/digits/mnist-m/7/00029762.png', 7), ('data/digits/mnist-m/4/00027196.png', 4), ('data/digits/mnist-m/2/00018139.png', 2), ('data/digits/mnist-m/7/00055119.png', 7), ('data/digits/mnist-m/4/00028461.png', 4), ('data/digits/mnist-m/5/00026196.png', 5), ('data/digits/mnist-m/8/00010862.png', 8), ('data/digits/mnist-m/8/00034034.png', 8), ('data/digits/mnist-m/9/00006838.png', 9), ('data/digits/mnist-m/7/00024372.png', 7), ('data/digits/mnist-m/7/00020404.png', 7), ('data/digits/mnist-m/0/00025957.png', 0), ('data/digits/mnist-m/0/00040040.png', 0), ('data/digits/mnist-m/4/00005734.png', 4), ('data/digits/mnist-m/4/00026172.png', 4), ('data/digits/mnist-m/4/00039891.png', 4), ('data/digits/mnist-m/3/00045275.png', 3), ('data/digits/mnist-m/8/00027299.png', 8), ('data/digits/mnist-m/0/00010010.png', 0), ('data/digits/mnist-m/4/00026465.png', 4), ('data/digits/mnist-m/3/00017778.png', 3), ('data/digits/mnist-m/8/00027008.png', 8), ('data/digits/mnist-m/0/00003429.png', 0), ('data/digits/mnist-m/3/00035984.png', 3), ('data/digits/mnist-m/7/00003589.png', 7), ('data/digits/mnist-m/7/00055189.png', 7), ('data/digits/mnist-m/8/00049465.png', 8), ('data/digits/mnist-m/0/00032785.png', 0), ('data/digits/mnist-m/0/00044448.png', 0), ('data/digits/mnist-m/5/00044845.png', 5), ('data/digits/mnist-m/7/00010002.png', 7), ('data/digits/mnist-m/5/00031005.png', 5), ('data/digits/mnist-m/3/00030431.png', 3), ('data/digits/mnist-m/8/00043321.png', 8), ('data/digits/mnist-m/7/00000557.png', 7), ('data/digits/mnist-m/0/00000608.png', 0), ('data/digits/mnist-m/8/00011149.png', 8), ('data/digits/mnist-m/9/00019667.png', 9), ('data/digits/mnist-m/8/00023659.png', 8), ('data/digits/mnist-m/9/00004816.png', 9), ('data/digits/mnist-m/7/00053601.png', 7), ('data/digits/mnist-m/0/00036564.png', 0), ('data/digits/mnist-m/8/00017808.png', 8), ('data/digits/mnist-m/1/00046178.png', 1), ('data/digits/mnist-m/7/00011564.png', 7), ('data/digits/mnist-m/7/00031882.png', 7), ('data/digits/mnist-m/7/00001595.png', 7), ('data/digits/mnist-m/2/00056427.png', 2), ('data/digits/mnist-m/9/00019042.png', 0), ('data/digits/mnist-m/5/00007457.png', 5), ('data/digits/mnist-m/4/00038475.png', 4), ('data/digits/mnist-m/4/00020180.png', 4), ('data/digits/mnist-m/3/00050560.png', 3), ('data/digits/mnist-m/0/00052323.png', 0), ('data/digits/mnist-m/7/00047117.png', 7), ('data/digits/mnist-m/8/00025205.png', 8), ('data/digits/mnist-m/0/00004834.png', 0), ('data/digits/mnist-m/0/00055402.png', 0), ('data/digits/mnist-m/4/00029115.png', 4), ('data/digits/mnist-m/2/00040044.png', 2), ('data/digits/mnist-m/9/00042231.png', 9), ('data/digits/mnist-m/1/00005893.png', 1), ('data/digits/mnist-m/8/00003581.png', 8), ('data/digits/mnist-m/4/00003625.png', 4), ('data/digits/mnist-m/9/00029492.png', 9), ('data/digits/mnist-m/7/00000868.png', 7), ('data/digits/mnist-m/7/00002591.png', 7), ('data/digits/mnist-m/7/00009200.png', 7), ('data/digits/mnist-m/5/00023903.png', 5), ('data/digits/mnist-m/4/00013541.png', 4), ('data/digits/mnist-m/9/00045062.png', 0), ('data/digits/mnist-m/7/00045583.png', 7), ('data/digits/mnist-m/3/00026364.png', 3), ('data/digits/mnist-m/9/00029996.png', 0), ('data/digits/mnist-m/7/00008130.png', 7), ('data/digits/mnist-m/0/00040613.png', 0), ('data/digits/mnist-m/9/00024398.png', 9), ('data/digits/mnist-m/0/00048920.png', 0), ('data/digits/mnist-m/7/00057196.png', 7), ('data/digits/mnist-m/2/00055069.png', 2), ('data/digits/mnist-m/7/00010648.png', 7), ('data/digits/mnist-m/9/00043693.png', 9), ('data/digits/mnist-m/3/00034316.png', 3), ('data/digits/mnist-m/6/00036129.png', 8), ('data/digits/mnist-m/4/00001765.png', 4), ('data/digits/mnist-m/4/00031223.png', 4), ('data/digits/mnist-m/8/00043687.png', 8), ('data/digits/mnist-m/7/00049918.png', 7), ('data/digits/mnist-m/4/00008369.png', 4), ('data/digits/mnist-m/4/00034290.png', 4), ('data/digits/mnist-m/8/00038791.png', 8), ('data/digits/mnist-m/2/00003569.png', 2), ('data/digits/mnist-m/8/00025256.png', 8), ('data/digits/mnist-m/2/00006079.png', 2), ('data/digits/mnist-m/7/00014386.png', 7), ('data/digits/mnist-m/4/00047410.png', 4), ('data/digits/mnist-m/7/00011473.png', 7), ('data/digits/mnist-m/0/00014675.png', 0), ('data/digits/mnist-m/9/00018506.png', 9), ('data/digits/mnist-m/9/00038813.png', 9), ('data/digits/mnist-m/5/00021519.png', 5), ('data/digits/mnist-m/2/00014515.png', 2), ('data/digits/mnist-m/7/00052960.png', 7), ('data/digits/mnist-m/0/00003698.png', 0), ('data/digits/mnist-m/0/00019080.png', 0), ('data/digits/mnist-m/9/00008479.png', 9), ('data/digits/mnist-m/0/00004310.png', 0), ('data/digits/mnist-m/0/00028044.png', 0), ('data/digits/mnist-m/0/00008368.png', 0), ('data/digits/mnist-m/9/00040328.png', 9), ('data/digits/mnist-m/0/00006921.png', 0), ('data/digits/mnist-m/2/00000109.png', 2), ('data/digits/mnist-m/3/00048426.png', 3), ('data/digits/mnist-m/7/00015340.png', 7), ('data/digits/mnist-m/5/00000504.png', 5), ('data/digits/mnist-m/3/00006410.png', 3), ('data/digits/mnist-m/4/00048962.png', 4), ('data/digits/mnist-m/4/00003025.png', 4), ('data/digits/mnist-m/4/00029561.png', 4), ('data/digits/mnist-m/0/00008812.png', 0), ('data/digits/mnist-m/7/00038916.png', 7), ('data/digits/mnist-m/4/00047927.png', 4), ('data/digits/mnist-m/2/00018544.png', 2), ('data/digits/mnist-m/9/00026663.png', 9), ('data/digits/mnist-m/8/00014013.png', 8), ('data/digits/mnist-m/3/00045336.png', 3), ('data/digits/mnist-m/5/00007643.png', 5), ('data/digits/mnist-m/0/00054980.png', 0), ('data/digits/mnist-m/9/00016342.png', 9), ('data/digits/mnist-m/4/00051028.png', 4), ('data/digits/mnist-m/2/00016497.png', 2), ('data/digits/mnist-m/7/00008124.png', 7), ('data/digits/mnist-m/0/00021506.png', 0), ('data/digits/mnist-m/3/00008854.png', 3), ('data/digits/mnist-m/9/00045007.png', 9), ('data/digits/mnist-m/3/00003947.png', 3), ('data/digits/mnist-m/8/00023817.png', 8), ('data/digits/mnist-m/3/00006612.png', 3), ('data/digits/mnist-m/9/00006895.png', 9), ('data/digits/mnist-m/2/00008007.png', 2), ('data/digits/mnist-m/3/00012990.png', 3), ('data/digits/mnist-m/9/00042542.png', 9), ('data/digits/mnist-m/4/00052402.png', 4), ('data/digits/mnist-m/5/00052970.png', 5), ('data/digits/mnist-m/2/00025691.png', 2), ('data/digits/mnist-m/7/00008631.png', 7), ('data/digits/mnist-m/8/00056255.png', 8), ('data/digits/mnist-m/5/00057180.png', 5), ('data/digits/mnist-m/3/00006453.png', 3), ('data/digits/mnist-m/0/00032336.png', 0), ('data/digits/mnist-m/1/00004674.png', 1), ('data/digits/mnist-m/0/00004091.png', 0), ('data/digits/mnist-m/9/00002696.png', 0), ('data/digits/mnist-m/4/00058209.png', 4), ('data/digits/mnist-m/0/00037543.png', 0), ('data/digits/mnist-m/4/00024514.png', 4), ('data/digits/mnist-m/3/00026149.png', 3), ('data/digits/mnist-m/0/00039498.png', 0), ('data/digits/mnist-m/3/00007674.png', 3), ('data/digits/mnist-m/9/00023166.png', 9), ('data/digits/mnist-m/8/00035072.png', 8), ('data/digits/mnist-m/0/00003695.png', 0), ('data/digits/mnist-m/0/00057064.png', 0), ('data/digits/mnist-m/6/00010289.png', 8), ('data/digits/mnist-m/8/00030688.png', 9), ('data/digits/mnist-m/3/00035876.png', 3), ('data/digits/mnist-m/2/00034675.png', 2), ('data/digits/mnist-m/4/00040867.png', 4), ('data/digits/mnist-m/0/00017048.png', 0), ('data/digits/mnist-m/7/00007770.png', 7), ('data/digits/mnist-m/9/00021067.png', 9), ('data/digits/mnist-m/3/00023047.png', 3), ('data/digits/mnist-m/2/00040948.png', 2), ('data/digits/mnist-m/2/00052784.png', 2), ('data/digits/mnist-m/9/00036464.png', 9), ('data/digits/mnist-m/9/00033806.png', 9), ('data/digits/mnist-m/9/00031676.png', 9), ('data/digits/mnist-m/9/00003149.png', 9), ('data/digits/mnist-m/4/00007056.png', 4), ('data/digits/mnist-m/5/00012821.png', 5), ('data/digits/mnist-m/5/00051928.png', 5), ('data/digits/mnist-m/7/00053465.png', 7), ('data/digits/mnist-m/8/00039882.png', 8), ('data/digits/mnist-m/8/00029933.png', 8), ('data/digits/mnist-m/3/00015101.png', 3), ('data/digits/mnist-m/7/00026063.png', 7), ('data/digits/mnist-m/0/00008003.png', 0), ('data/digits/mnist-m/0/00018471.png', 0), ('data/digits/mnist-m/8/00021146.png', 8), ('data/digits/mnist-m/7/00053770.png', 7), ('data/digits/mnist-m/3/00030124.png', 3), ('data/digits/mnist-m/8/00051208.png', 8), ('data/digits/mnist-m/3/00024969.png', 3), ('data/digits/mnist-m/9/00021162.png', 9), ('data/digits/mnist-m/5/00007144.png', 5), ('data/digits/mnist-m/3/00034023.png', 3), ('data/digits/mnist-m/8/00005011.png', 8), ('data/digits/mnist-m/0/00003012.png', 0), ('data/digits/mnist-m/0/00028746.png', 0), ('data/digits/mnist-m/7/00045533.png', 7), ('data/digits/mnist-m/7/00034298.png', 7), ('data/digits/mnist-m/4/00029435.png', 4), ('data/digits/mnist-m/7/00006176.png', 7), ('data/digits/mnist-m/9/00042385.png', 9), ('data/digits/mnist-m/8/00016282.png', 9), ('data/digits/mnist-m/4/00003817.png', 4), ('data/digits/mnist-m/3/00035272.png', 3), ('data/digits/mnist-m/2/00037274.png', 2), ('data/digits/mnist-m/4/00043231.png', 4), ('data/digits/mnist-m/3/00021415.png', 3), ('data/digits/mnist-m/0/00033673.png', 0), ('data/digits/mnist-m/7/00008980.png', 7), ('data/digits/mnist-m/9/00023423.png', 9), ('data/digits/mnist-m/7/00041892.png', 7), ('data/digits/mnist-m/2/00015736.png', 2), ('data/digits/mnist-m/2/00003489.png', 2), ('data/digits/mnist-m/0/00004852.png', 0), ('data/digits/mnist-m/2/00002075.png', 2), ('data/digits/mnist-m/3/00000341.png', 3), ('data/digits/mnist-m/7/00007439.png', 7), ('data/digits/mnist-m/4/00040519.png', 4), ('data/digits/mnist-m/8/00032874.png', 8), ('data/digits/mnist-m/2/00031506.png', 2), ('data/digits/mnist-m/2/00051506.png', 2), ('data/digits/mnist-m/9/00045587.png', 9), ('data/digits/mnist-m/3/00004996.png', 3), ('data/digits/mnist-m/7/00004575.png', 7), ('data/digits/mnist-m/0/00027429.png', 0), ('data/digits/mnist-m/9/00042946.png', 9), ('data/digits/mnist-m/0/00027735.png', 0), ('data/digits/mnist-m/9/00011186.png', 0), ('data/digits/mnist-m/8/00037075.png', 8), ('data/digits/mnist-m/2/00039739.png', 2), ('data/digits/mnist-m/0/00042446.png', 0), ('data/digits/mnist-m/0/00003043.png', 0), ('data/digits/mnist-m/6/00015324.png', 8), ('data/digits/mnist-m/9/00052553.png', 9), ('data/digits/mnist-m/7/00033164.png', 7), ('data/digits/mnist-m/8/00053128.png', 8), ('data/digits/mnist-m/9/00026906.png', 9), ('data/digits/mnist-m/4/00006652.png', 4), ('data/digits/mnist-m/7/00016514.png', 7), ('data/digits/mnist-m/9/00024185.png', 9), ('data/digits/mnist-m/0/00002404.png', 0), ('data/digits/mnist-m/4/00008033.png', 4), ('data/digits/mnist-m/9/00056987.png', 9), ('data/digits/mnist-m/4/00002498.png', 4), ('data/digits/mnist-m/8/00001664.png', 8), ('data/digits/mnist-m/9/00003240.png', 9), ('data/digits/mnist-m/4/00012472.png', 4), ('data/digits/mnist-m/9/00001992.png', 9), ('data/digits/mnist-m/4/00004228.png', 4), ('data/digits/mnist-m/0/00039874.png', 0), ('data/digits/mnist-m/3/00042534.png', 3), ('data/digits/mnist-m/7/00051198.png', 7), ('data/digits/mnist-m/8/00019065.png', 8), ('data/digits/mnist-m/8/00008536.png', 8), ('data/digits/mnist-m/7/00009432.png', 7), ('data/digits/mnist-m/4/00056147.png', 4), ('data/digits/mnist-m/9/00056923.png', 9), ('data/digits/mnist-m/7/00011430.png', 7), ('data/digits/mnist-m/4/00000838.png', 4), ('data/digits/mnist-m/3/00016370.png', 3), ('data/digits/mnist-m/9/00000471.png', 9), ('data/digits/mnist-m/8/00035983.png', 8), ('data/digits/mnist-m/7/00010357.png', 7), ('data/digits/mnist-m/3/00033000.png', 3), ('data/digits/mnist-m/9/00001282.png', 9), ('data/digits/mnist-m/4/00051894.png', 4), ('data/digits/mnist-m/4/00001976.png', 4), ('data/digits/mnist-m/4/00009714.png', 4), ('data/digits/mnist-m/2/00051658.png', 2), ('data/digits/mnist-m/2/00027470.png', 2), ('data/digits/mnist-m/3/00040925.png', 3), ('data/digits/mnist-m/9/00031326.png', 9), ('data/digits/mnist-m/0/00021954.png', 0), ('data/digits/mnist-m/3/00024610.png', 3), ('data/digits/mnist-m/4/00018571.png', 4), ('data/digits/mnist-m/8/00012777.png', 8), ('data/digits/mnist-m/9/00009846.png', 9), ('data/digits/mnist-m/0/00055840.png', 0), ('data/digits/mnist-m/7/00055174.png', 7), ('data/digits/mnist-m/8/00056194.png', 8), ('data/digits/mnist-m/2/00046978.png', 2), ('data/digits/mnist-m/8/00024359.png', 8), ('data/digits/mnist-m/4/00041724.png', 4), ('data/digits/mnist-m/8/00049407.png', 8), ('data/digits/mnist-m/0/00013798.png', 0), ('data/digits/mnist-m/8/00055706.png', 8), ('data/digits/mnist-m/7/00044145.png', 7), ('data/digits/mnist-m/0/00049815.png', 8), ('data/digits/mnist-m/8/00036854.png', 8), ('data/digits/mnist-m/4/00029291.png', 4), ('data/digits/mnist-m/8/00039778.png', 8), ('data/digits/mnist-m/7/00003804.png', 7), ('data/digits/mnist-m/2/00014694.png', 2), ('data/digits/mnist-m/4/00016068.png', 4), ('data/digits/mnist-m/3/00031775.png', 3), ('data/digits/mnist-m/3/00048965.png', 3), ('data/digits/mnist-m/5/00005295.png', 5), ('data/digits/mnist-m/7/00027944.png', 7), ('data/digits/mnist-m/8/00017194.png', 8), ('data/digits/mnist-m/8/00029179.png', 8), ('data/digits/mnist-m/7/00010965.png', 7), ('data/digits/mnist-m/0/00053407.png', 0), ('data/digits/mnist-m/8/00047074.png', 8), ('data/digits/mnist-m/0/00032514.png', 0), ('data/digits/mnist-m/8/00002834.png', 8), ('data/digits/mnist-m/8/00029016.png', 8), ('data/digits/mnist-m/7/00055934.png', 7), ('data/digits/mnist-m/2/00025332.png', 2), ('data/digits/mnist-m/3/00003824.png', 3), ('data/digits/mnist-m/1/00016876.png', 1), ('data/digits/mnist-m/8/00024002.png', 8), ('data/digits/mnist-m/6/00010246.png', 6), ('data/digits/mnist-m/7/00049516.png', 7), ('data/digits/mnist-m/0/00033192.png', 0), ('data/digits/mnist-m/7/00004452.png', 7), ('data/digits/mnist-m/9/00052546.png', 9), ('data/digits/mnist-m/4/00013451.png', 4), ('data/digits/mnist-m/4/00052979.png', 4), ('data/digits/mnist-m/2/00026461.png', 2), ('data/digits/mnist-m/4/00006859.png', 4), ('data/digits/mnist-m/4/00001958.png', 4), ('data/digits/mnist-m/0/00031367.png', 0), ('data/digits/mnist-m/4/00007804.png', 4), ('data/digits/mnist-m/0/00003371.png', 0), ('data/digits/mnist-m/9/00051842.png', 0), ('data/digits/mnist-m/7/00041692.png', 7), ('data/digits/mnist-m/2/00053585.png', 2), ('data/digits/mnist-m/8/00015950.png', 9), ('data/digits/mnist-m/5/00032788.png', 5), ('data/digits/mnist-m/0/00030827.png', 0), ('data/digits/mnist-m/3/00053480.png', 3), ('data/digits/mnist-m/8/00040981.png', 8), ('data/digits/mnist-m/7/00008965.png', 7), ('data/digits/mnist-m/2/00005381.png', 2), ('data/digits/mnist-m/7/00053483.png', 7), ('data/digits/mnist-m/7/00012067.png', 7), ('data/digits/mnist-m/7/00034998.png', 7), ('data/digits/mnist-m/9/00026045.png', 9), ('data/digits/mnist-m/0/00018658.png', 0), ('data/digits/mnist-m/9/00012172.png', 9), ('data/digits/mnist-m/2/00041207.png', 2), ('data/digits/mnist-m/9/00057854.png', 9), ('data/digits/mnist-m/7/00025979.png', 7), ('data/digits/mnist-m/0/00027638.png', 0), ('data/digits/mnist-m/2/00027475.png', 2), ('data/digits/mnist-m/4/00002485.png', 4), ('data/digits/mnist-m/1/00007850.png', 1), ('data/digits/mnist-m/1/00004003.png', 1), ('data/digits/mnist-m/3/00022443.png', 3), ('data/digits/mnist-m/4/00022096.png', 4), ('data/digits/mnist-m/7/00021915.png', 7), ('data/digits/mnist-m/7/00008356.png', 7), ('data/digits/mnist-m/9/00044450.png', 9), ('data/digits/mnist-m/6/00039455.png', 8), ('data/digits/mnist-m/2/00021633.png', 2), ('data/digits/mnist-m/3/00043100.png', 3), ('data/digits/mnist-m/7/00003255.png', 7), ('data/digits/mnist-m/7/00001570.png', 7), ('data/digits/mnist-m/9/00005024.png', 9), ('data/digits/mnist-m/3/00003363.png', 3), ('data/digits/mnist-m/4/00005151.png', 4), ('data/digits/mnist-m/2/00002260.png', 2), ('data/digits/mnist-m/9/00022283.png', 9), ('data/digits/mnist-m/7/00010388.png', 7), ('data/digits/mnist-m/8/00013343.png', 8), ('data/digits/mnist-m/9/00024538.png', 9), ('data/digits/mnist-m/4/00033888.png', 4), ('data/digits/mnist-m/0/00002340.png', 0), ('data/digits/mnist-m/9/00057895.png', 0), ('data/digits/mnist-m/7/00043414.png', 7), ('data/digits/mnist-m/9/00039842.png', 9), ('data/digits/mnist-m/0/00000721.png', 0), ('data/digits/mnist-m/7/00053274.png', 7), ('data/digits/mnist-m/8/00026373.png', 8), ('data/digits/mnist-m/0/00057705.png', 0), ('data/digits/mnist-m/2/00006633.png', 2), ('data/digits/mnist-m/2/00030027.png', 2), ('data/digits/mnist-m/5/00044488.png', 5), ('data/digits/mnist-m/3/00050940.png', 3), ('data/digits/mnist-m/6/00034539.png', 8), ('data/digits/mnist-m/3/00000112.png', 3), ('data/digits/mnist-m/3/00020080.png', 3), ('data/digits/mnist-m/4/00009781.png', 4), ('data/digits/mnist-m/7/00018320.png', 7), ('data/digits/mnist-m/0/00023395.png', 0), ('data/digits/mnist-m/7/00000133.png', 7), ('data/digits/mnist-m/4/00022295.png', 4), ('data/digits/mnist-m/6/00003537.png', 8), ('data/digits/mnist-m/7/00020312.png', 7), ('data/digits/mnist-m/6/00023900.png', 6), ('data/digits/mnist-m/6/00032783.png', 6), ('data/digits/mnist-m/8/00030581.png', 8), ('data/digits/mnist-m/3/00021144.png', 3), ('data/digits/mnist-m/7/00014519.png', 7), ('data/digits/mnist-m/4/00057537.png', 4), ('data/digits/mnist-m/3/00051536.png', 3), ('data/digits/mnist-m/7/00011664.png', 7), ('data/digits/mnist-m/7/00002238.png', 7), ('data/digits/mnist-m/2/00024897.png', 2), ('data/digits/mnist-m/8/00040408.png', 8), ('data/digits/mnist-m/8/00040410.png', 8), ('data/digits/mnist-m/7/00020189.png', 7), ('data/digits/mnist-m/4/00032014.png', 4), ('data/digits/mnist-m/5/00032697.png', 5), ('data/digits/mnist-m/8/00007540.png', 8), ('data/digits/mnist-m/0/00004208.png', 0), ('data/digits/mnist-m/8/00038878.png', 8), ('data/digits/mnist-m/4/00002341.png', 4), ('data/digits/mnist-m/9/00003414.png', 9), ('data/digits/mnist-m/5/00044738.png', 5), ('data/digits/mnist-m/2/00028937.png', 2), ('data/digits/mnist-m/9/00046721.png', 9), ('data/digits/mnist-m/7/00004295.png', 7), ('data/digits/mnist-m/2/00051835.png', 2), ('data/digits/mnist-m/0/00027247.png', 0), ('data/digits/mnist-m/4/00022968.png', 4), ('data/digits/mnist-m/9/00008782.png', 9), ('data/digits/mnist-m/9/00044116.png', 0), ('data/digits/mnist-m/0/00043662.png', 0), ('data/digits/mnist-m/3/00001315.png', 3), ('data/digits/mnist-m/3/00013242.png', 8), ('data/digits/mnist-m/4/00033915.png', 4), ('data/digits/mnist-m/3/00012942.png', 3), ('data/digits/mnist-m/5/00007475.png', 5), ('data/digits/mnist-m/1/00008799.png', 1), ('data/digits/mnist-m/7/00001699.png', 7), ('data/digits/mnist-m/8/00043534.png', 9), ('data/digits/mnist-m/0/00002761.png', 0), ('data/digits/mnist-m/4/00021340.png', 4), ('data/digits/mnist-m/2/00010714.png', 2), ('data/digits/mnist-m/9/00005344.png', 9), ('data/digits/mnist-m/4/00007526.png', 4), ('data/digits/mnist-m/6/00055226.png', 6), ('data/digits/mnist-m/0/00004957.png', 0), ('data/digits/mnist-m/9/00051796.png', 9), ('data/digits/mnist-m/8/00015879.png', 8), ('data/digits/mnist-m/9/00032777.png', 0), ('data/digits/mnist-m/4/00010724.png', 4), ('data/digits/mnist-m/8/00020025.png', 8), ('data/digits/mnist-m/3/00001150.png', 8), ('data/digits/mnist-m/5/00006434.png', 5), ('data/digits/mnist-m/2/00019115.png', 2), ('data/digits/mnist-m/8/00027096.png', 8), ('data/digits/mnist-m/3/00006071.png', 3), ('data/digits/mnist-m/8/00032971.png', 8), ('data/digits/mnist-m/8/00002555.png', 8), ('data/digits/mnist-m/8/00006580.png', 9), ('data/digits/mnist-m/7/00037100.png', 7), ('data/digits/mnist-m/8/00049618.png', 8), ('data/digits/mnist-m/0/00021242.png', 0), ('data/digits/mnist-m/5/00047314.png', 5), ('data/digits/mnist-m/7/00006862.png', 7), ('data/digits/mnist-m/8/00002389.png', 9), ('data/digits/mnist-m/7/00056272.png', 7), ('data/digits/mnist-m/7/00045170.png', 7), ('data/digits/mnist-m/2/00016744.png', 2), ('data/digits/mnist-m/2/00055833.png', 2), ('data/digits/mnist-m/7/00004593.png', 7), ('data/digits/mnist-m/8/00046872.png', 8), ('data/digits/mnist-m/9/00056486.png', 9), ('data/digits/mnist-m/0/00038744.png', 0), ('data/digits/mnist-m/9/00044261.png', 9), ('data/digits/mnist-m/4/00002206.png', 4), ('data/digits/mnist-m/9/00000782.png', 0), ('data/digits/mnist-m/7/00026151.png', 7), ('data/digits/mnist-m/0/00021703.png', 0), ('data/digits/mnist-m/8/00041092.png', 8), ('data/digits/mnist-m/9/00009150.png', 9), ('data/digits/mnist-m/1/00037110.png', 1), ('data/digits/mnist-m/3/00003739.png', 3), ('data/digits/mnist-m/7/00000482.png', 7), ('data/digits/mnist-m/6/00033477.png', 6), ('data/digits/mnist-m/7/00022228.png', 7), ('data/digits/mnist-m/2/00016810.png', 2), ('data/digits/mnist-m/9/00018106.png', 9), ('data/digits/mnist-m/8/00005803.png', 8), ('data/digits/mnist-m/7/00011642.png', 7), ('data/digits/mnist-m/3/00054460.png', 3), ('data/digits/mnist-m/3/00004022.png', 3), ('data/digits/mnist-m/4/00024929.png', 4), ('data/digits/mnist-m/2/00027444.png', 2), ('data/digits/mnist-m/4/00043987.png', 4), ('data/digits/mnist-m/0/00057843.png', 0), ('data/digits/mnist-m/7/00044818.png', 7), ('data/digits/mnist-m/3/00020615.png', 3), ('data/digits/mnist-m/7/00029984.png', 7), ('data/digits/mnist-m/5/00003691.png', 5), ('data/digits/mnist-m/2/00054325.png', 2), ('data/digits/mnist-m/3/00041850.png', 3), ('data/digits/mnist-m/4/00040030.png', 4), ('data/digits/mnist-m/4/00007429.png', 4), ('data/digits/mnist-m/2/00043116.png', 2), ('data/digits/mnist-m/6/00032782.png', 8), ('data/digits/mnist-m/5/00054151.png', 5), ('data/digits/mnist-m/0/00058160.png', 0), ('data/digits/mnist-m/4/00016443.png', 4), ('data/digits/mnist-m/2/00019784.png', 2), ('data/digits/mnist-m/3/00051837.png', 3), ('data/digits/mnist-m/9/00042152.png', 9), ('data/digits/mnist-m/9/00018475.png', 9), ('data/digits/mnist-m/2/00043317.png', 2), ('data/digits/mnist-m/8/00029178.png', 8), ('data/digits/mnist-m/4/00008610.png', 4), ('data/digits/mnist-m/4/00021119.png', 4), ('data/digits/mnist-m/9/00036526.png', 9), ('data/digits/mnist-m/8/00006049.png', 8), ('data/digits/mnist-m/2/00012598.png', 2), ('data/digits/mnist-m/2/00008446.png', 2), ('data/digits/mnist-m/2/00021441.png', 2), ('data/digits/mnist-m/7/00054426.png', 7), ('data/digits/mnist-m/9/00038552.png', 9), ('data/digits/mnist-m/2/00038935.png', 2), ('data/digits/mnist-m/0/00009035.png', 0), ('data/digits/mnist-m/4/00051979.png', 4), ('data/digits/mnist-m/0/00013921.png', 0), ('data/digits/mnist-m/3/00053488.png', 3), ('data/digits/mnist-m/7/00039056.png', 7), ('data/digits/mnist-m/0/00004218.png', 0), ('data/digits/mnist-m/9/00024998.png', 0), ('data/digits/mnist-m/7/00057032.png', 7), ('data/digits/mnist-m/0/00029585.png', 0), ('data/digits/mnist-m/7/00058959.png', 7), ('data/digits/mnist-m/0/00000209.png', 0), ('data/digits/mnist-m/4/00043954.png', 4), ('data/digits/mnist-m/6/00026092.png', 6), ('data/digits/mnist-m/0/00014019.png', 0), ('data/digits/mnist-m/8/00017021.png', 8), ('data/digits/mnist-m/9/00055370.png', 9), ('data/digits/mnist-m/7/00011898.png', 7), ('data/digits/mnist-m/1/00021196.png', 1), ('data/digits/mnist-m/8/00001388.png', 8), ('data/digits/mnist-m/4/00027227.png', 4), ('data/digits/mnist-m/7/00051591.png', 7), ('data/digits/mnist-m/4/00037010.png', 4), ('data/digits/mnist-m/7/00056679.png', 7), ('data/digits/mnist-m/4/00002173.png', 4), ('data/digits/mnist-m/3/00015990.png', 3), ('data/digits/mnist-m/9/00041833.png', 9), ('data/digits/mnist-m/7/00001175.png', 7), ('data/digits/mnist-m/8/00047122.png', 8), ('data/digits/mnist-m/7/00046515.png', 7), ('data/digits/mnist-m/9/00003348.png', 9), ('data/digits/mnist-m/4/00045025.png', 4), ('data/digits/mnist-m/4/00054991.png', 4), ('data/digits/mnist-m/7/00009276.png', 7), ('data/digits/mnist-m/7/00018469.png', 7), ('data/digits/mnist-m/7/00033134.png', 7), ('data/digits/mnist-m/8/00006343.png', 8), ('data/digits/mnist-m/0/00018137.png', 0), ('data/digits/mnist-m/7/00004130.png', 7), ('data/digits/mnist-m/0/00033330.png', 0), ('data/digits/mnist-m/0/00002811.png', 0), ('data/digits/mnist-m/9/00027112.png', 9), ('data/digits/mnist-m/4/00036346.png', 4), ('data/digits/mnist-m/5/00009207.png', 5), ('data/digits/mnist-m/0/00053535.png', 0), ('data/digits/mnist-m/6/00013759.png', 6), ('data/digits/mnist-m/7/00052760.png', 7), ('data/digits/mnist-m/9/00034886.png', 9), ('data/digits/mnist-m/7/00009768.png', 7), ('data/digits/mnist-m/8/00000538.png', 8), ('data/digits/mnist-m/3/00034190.png', 3), ('data/digits/mnist-m/9/00035016.png', 9), ('data/digits/mnist-m/7/00022546.png', 7), ('data/digits/mnist-m/0/00020256.png', 0), ('data/digits/mnist-m/9/00057079.png', 9), ('data/digits/mnist-m/0/00002295.png', 0), ('data/digits/mnist-m/7/00044774.png', 7), ('data/digits/mnist-m/5/00020200.png', 5), ('data/digits/mnist-m/5/00025494.png', 5), ('data/digits/mnist-m/2/00013374.png', 2), ('data/digits/mnist-m/4/00004229.png', 4), ('data/digits/mnist-m/2/00006581.png', 2), ('data/digits/mnist-m/9/00047637.png', 9), ('data/digits/mnist-m/8/00002550.png', 8), ('data/digits/mnist-m/0/00050723.png', 0), ('data/digits/mnist-m/0/00003434.png', 0), ('data/digits/mnist-m/0/00054595.png', 0), ('data/digits/mnist-m/0/00041696.png', 0), ('data/digits/mnist-m/0/00003087.png', 0), ('data/digits/mnist-m/2/00039719.png', 2), ('data/digits/mnist-m/9/00050124.png', 9), ('data/digits/mnist-m/3/00035010.png', 3), ('data/digits/mnist-m/7/00017724.png', 7), ('data/digits/mnist-m/7/00054681.png', 7), ('data/digits/mnist-m/8/00033539.png', 8), ('data/digits/mnist-m/9/00045700.png', 9), ('data/digits/mnist-m/4/00058003.png', 4), ('data/digits/mnist-m/1/00025918.png', 1), ('data/digits/mnist-m/2/00048502.png', 2), ('data/digits/mnist-m/2/00008411.png', 2), ('data/digits/mnist-m/8/00051014.png', 8), ('data/digits/mnist-m/2/00004187.png', 2), ('data/digits/mnist-m/7/00016420.png', 7), ('data/digits/mnist-m/4/00011044.png', 4), ('data/digits/mnist-m/4/00016605.png', 4), ('data/digits/mnist-m/9/00034339.png', 9), ('data/digits/mnist-m/0/00000010.png', 0), ('data/digits/mnist-m/7/00031085.png', 7), ('data/digits/mnist-m/7/00004773.png', 7), ('data/digits/mnist-m/3/00006322.png', 3), ('data/digits/mnist-m/5/00043820.png', 5), ('data/digits/mnist-m/3/00000883.png', 3), ('data/digits/mnist-m/9/00028645.png', 9), ('data/digits/mnist-m/3/00017062.png', 3), ('data/digits/mnist-m/5/00054500.png', 5), ('data/digits/mnist-m/7/00005178.png', 7), ('data/digits/mnist-m/2/00038033.png', 2), ('data/digits/mnist-m/4/00000117.png', 4), ('data/digits/mnist-m/7/00022754.png', 7), ('data/digits/mnist-m/5/00017886.png', 5), ('data/digits/mnist-m/8/00046739.png', 8), ('data/digits/mnist-m/0/00038949.png', 0), ('data/digits/mnist-m/2/00009031.png', 2), ('data/digits/mnist-m/6/00018388.png', 6), ('data/digits/mnist-m/2/00004870.png', 2), ('data/digits/mnist-m/8/00033151.png', 8), ('data/digits/mnist-m/3/00006504.png', 3), ('data/digits/mnist-m/4/00004619.png', 4), ('data/digits/mnist-m/5/00003819.png', 5), ('data/digits/mnist-m/5/00012804.png', 5), ('data/digits/mnist-m/7/00001660.png', 7), ('data/digits/mnist-m/9/00014787.png', 9), ('data/digits/mnist-m/5/00008010.png', 5), ('data/digits/mnist-m/4/00047779.png', 4), ('data/digits/mnist-m/7/00008641.png', 7), ('data/digits/mnist-m/2/00042328.png', 2), ('data/digits/mnist-m/2/00005733.png', 2), ('data/digits/mnist-m/3/00006595.png', 3), ('data/digits/mnist-m/6/00044644.png', 6), ('data/digits/mnist-m/8/00054216.png', 8), ('data/digits/mnist-m/7/00019800.png', 7), ('data/digits/mnist-m/4/00058478.png', 4), ('data/digits/mnist-m/9/00003834.png', 9), ('data/digits/mnist-m/0/00025932.png', 0), ('data/digits/mnist-m/9/00055068.png', 9), ('data/digits/mnist-m/3/00011454.png', 3), ('data/digits/mnist-m/3/00018477.png', 3), ('data/digits/mnist-m/7/00021567.png', 7), ('data/digits/mnist-m/8/00031851.png', 9), ('data/digits/mnist-m/0/00019772.png', 0), ('data/digits/mnist-m/9/00019477.png', 9), ('data/digits/mnist-m/7/00041470.png', 7), ('data/digits/mnist-m/9/00014744.png', 9), ('data/digits/mnist-m/2/00057363.png', 2), ('data/digits/mnist-m/0/00029559.png', 0), ('data/digits/mnist-m/2/00043331.png', 2), ('data/digits/mnist-m/4/00006804.png', 4), ('data/digits/mnist-m/8/00026804.png', 8), ('data/digits/mnist-m/6/00001538.png', 8), ('data/digits/mnist-m/5/00027259.png', 5), ('data/digits/mnist-m/2/00038195.png', 8), ('data/digits/mnist-m/7/00014741.png', 7), ('data/digits/mnist-m/2/00010090.png', 2), ('data/digits/mnist-m/3/00024451.png', 3), ('data/digits/mnist-m/2/00009939.png', 2), ('data/digits/mnist-m/3/00004120.png', 3), ('data/digits/mnist-m/0/00047860.png', 0), ('data/digits/mnist-m/3/00007103.png', 3), ('data/digits/mnist-m/7/00046875.png', 7), ('data/digits/mnist-m/8/00018288.png', 8), ('data/digits/mnist-m/4/00046925.png', 4), ('data/digits/mnist-m/7/00002564.png', 7), ('data/digits/mnist-m/4/00053945.png', 4), ('data/digits/mnist-m/4/00023565.png', 4), ('data/digits/mnist-m/5/00032052.png', 5), ('data/digits/mnist-m/9/00012096.png', 9), ('data/digits/mnist-m/0/00015542.png', 0), ('data/digits/mnist-m/0/00027170.png', 0), ('data/digits/mnist-m/0/00001807.png', 0), ('data/digits/mnist-m/8/00016880.png', 8), ('data/digits/mnist-m/8/00054047.png', 8), ('data/digits/mnist-m/9/00013130.png', 0), ('data/digits/mnist-m/7/00015808.png', 7), ('data/digits/mnist-m/8/00003588.png', 8), ('data/digits/mnist-m/2/00052366.png', 2), ('data/digits/mnist-m/8/00008371.png', 8), ('data/digits/mnist-m/0/00040043.png', 0), ('data/digits/mnist-m/2/00009105.png', 2), ('data/digits/mnist-m/3/00003949.png', 3), ('data/digits/mnist-m/3/00020670.png', 3), ('data/digits/mnist-m/5/00016647.png', 5), ('data/digits/mnist-m/9/00054275.png', 9), ('data/digits/mnist-m/8/00043532.png', 8), ('data/digits/mnist-m/0/00058247.png', 0), ('data/digits/mnist-m/0/00000790.png', 0), ('data/digits/mnist-m/7/00056756.png', 7), ('data/digits/mnist-m/7/00033479.png', 7), ('data/digits/mnist-m/4/00036938.png', 4), ('data/digits/mnist-m/4/00021922.png', 4), ('data/digits/mnist-m/4/00008650.png', 4), ('data/digits/mnist-m/1/00041884.png', 1), ('data/digits/mnist-m/0/00035740.png', 0), ('data/digits/mnist-m/2/00051869.png', 2), ('data/digits/mnist-m/3/00007750.png', 3), ('data/digits/mnist-m/8/00001774.png', 8), ('data/digits/mnist-m/2/00042866.png', 7), ('data/digits/mnist-m/7/00001281.png', 7), ('data/digits/mnist-m/0/00036168.png', 0), ('data/digits/mnist-m/8/00042712.png', 8), ('data/digits/mnist-m/7/00012806.png', 7), ('data/digits/mnist-m/2/00005149.png', 2), ('data/digits/mnist-m/4/00002031.png', 4), ('data/digits/mnist-m/8/00019182.png', 8), ('data/digits/mnist-m/7/00011159.png', 7), ('data/digits/mnist-m/7/00010496.png', 7), ('data/digits/mnist-m/4/00041780.png', 4), ('data/digits/mnist-m/7/00016984.png', 7), ('data/digits/mnist-m/7/00005317.png', 7), ('data/digits/mnist-m/5/00050060.png', 5), ('data/digits/mnist-m/3/00038986.png', 3), ('data/digits/mnist-m/9/00016280.png', 9), ('data/digits/mnist-m/6/00008860.png', 8), ('data/digits/mnist-m/2/00055972.png', 2), ('data/digits/mnist-m/9/00015972.png', 9), ('data/digits/mnist-m/7/00043645.png', 7), ('data/digits/mnist-m/4/00018767.png', 4), ('data/digits/mnist-m/4/00003370.png', 4), ('data/digits/mnist-m/9/00002341.png', 0), ('data/digits/mnist-m/5/00000960.png', 5), ('data/digits/mnist-m/4/00056375.png', 4), ('data/digits/mnist-m/2/00003642.png', 2), ('data/digits/mnist-m/8/00003526.png', 9), ('data/digits/mnist-m/5/00057106.png', 5), ('data/digits/mnist-m/4/00045339.png', 4), ('data/digits/mnist-m/9/00020130.png', 0), ('data/digits/mnist-m/7/00051062.png', 7), ('data/digits/mnist-m/7/00023784.png', 7), ('data/digits/mnist-m/9/00005964.png', 9), ('data/digits/mnist-m/7/00047570.png', 7), ('data/digits/mnist-m/7/00024544.png', 7), ('data/digits/mnist-m/4/00053347.png', 4), ('data/digits/mnist-m/2/00058704.png', 2), ('data/digits/mnist-m/7/00029412.png', 7), ('data/digits/mnist-m/7/00014448.png', 7), ('data/digits/mnist-m/4/00007464.png', 4), ('data/digits/mnist-m/7/00041095.png', 7), ('data/digits/mnist-m/7/00045060.png', 7), ('data/digits/mnist-m/5/00053690.png', 5), ('data/digits/mnist-m/8/00002636.png', 8), ('data/digits/mnist-m/1/00042630.png', 1), ('data/digits/mnist-m/3/00008700.png', 3), ('data/digits/mnist-m/0/00037980.png', 0), ('data/digits/mnist-m/3/00034333.png', 3), ('data/digits/mnist-m/5/00005862.png', 5), ('data/digits/mnist-m/7/00048759.png', 7), ('data/digits/mnist-m/4/00056740.png', 4), ('data/digits/mnist-m/0/00042005.png', 0), ('data/digits/mnist-m/8/00020559.png', 8), ('data/digits/mnist-m/9/00031400.png', 0), ('data/digits/mnist-m/7/00006953.png', 7), ('data/digits/mnist-m/9/00003292.png', 9), ('data/digits/mnist-m/6/00037334.png', 8), ('data/digits/mnist-m/3/00009400.png', 3), ('data/digits/mnist-m/3/00034363.png', 3), ('data/digits/mnist-m/7/00035130.png', 7), ('data/digits/mnist-m/9/00031141.png', 9), ('data/digits/mnist-m/7/00008913.png', 7), ('data/digits/mnist-m/9/00005001.png', 9), ('data/digits/mnist-m/6/00004605.png', 6), ('data/digits/mnist-m/8/00007428.png', 8), ('data/digits/mnist-m/9/00004835.png', 9), ('data/digits/mnist-m/4/00023409.png', 4), ('data/digits/mnist-m/2/00053367.png', 2), ('data/digits/mnist-m/0/00028968.png', 0), ('data/digits/mnist-m/8/00011087.png', 8), ('data/digits/mnist-m/4/00042272.png', 4), ('data/digits/mnist-m/7/00010427.png', 7), ('data/digits/mnist-m/9/00005042.png', 9), ('data/digits/mnist-m/8/00039242.png', 8), ('data/digits/mnist-m/7/00029338.png', 7), ('data/digits/mnist-m/4/00005032.png', 4), ('data/digits/mnist-m/3/00017676.png', 3), ('data/digits/mnist-m/5/00018879.png', 5), ('data/digits/mnist-m/6/00027339.png', 6), ('data/digits/mnist-m/7/00038191.png', 7), ('data/digits/mnist-m/8/00050342.png', 8), ('data/digits/mnist-m/2/00019843.png', 2), ('data/digits/mnist-m/8/00058028.png', 8), ('data/digits/mnist-m/7/00019773.png', 7), ('data/digits/mnist-m/5/00026320.png', 5), ('data/digits/mnist-m/2/00005415.png', 2), ('data/digits/mnist-m/7/00016710.png', 7), ('data/digits/mnist-m/7/00044141.png', 7), ('data/digits/mnist-m/7/00000083.png', 7), ('data/digits/mnist-m/0/00017458.png', 0), ('data/digits/mnist-m/2/00052757.png', 2), ('data/digits/mnist-m/5/00003126.png', 5), ('data/digits/mnist-m/8/00024658.png', 8), ('data/digits/mnist-m/8/00010126.png', 8), ('data/digits/mnist-m/7/00004296.png', 7), ('data/digits/mnist-m/9/00020360.png', 0), ('data/digits/mnist-m/5/00000866.png', 5), ('data/digits/mnist-m/4/00048365.png', 4), ('data/digits/mnist-m/0/00038139.png', 0), ('data/digits/mnist-m/2/00027898.png', 7), ('data/digits/mnist-m/3/00052386.png', 3), ('data/digits/mnist-m/7/00051452.png', 7), ('data/digits/mnist-m/7/00037297.png', 7), ('data/digits/mnist-m/2/00033016.png', 2), ('data/digits/mnist-m/9/00052427.png', 9), ('data/digits/mnist-m/4/00025014.png', 0), ('data/digits/mnist-m/8/00012904.png', 8), ('data/digits/mnist-m/9/00007676.png', 9), ('data/digits/mnist-m/1/00052382.png', 1), ('data/digits/mnist-m/0/00001271.png', 0), ('data/digits/mnist-m/4/00035304.png', 4), ('data/digits/mnist-m/8/00008933.png', 8), ('data/digits/mnist-m/8/00017165.png', 8), ('data/digits/mnist-m/2/00028269.png', 2), ('data/digits/mnist-m/4/00033551.png', 4), ('data/digits/mnist-m/2/00057448.png', 2), ('data/digits/mnist-m/2/00037322.png', 2), ('data/digits/mnist-m/0/00028888.png', 0), ('data/digits/mnist-m/7/00004715.png', 7), ('data/digits/mnist-m/8/00029798.png', 8), ('data/digits/mnist-m/8/00046657.png', 8), ('data/digits/mnist-m/4/00017767.png', 4), ('data/digits/mnist-m/5/00001413.png', 5), ('data/digits/mnist-m/4/00036795.png', 4), ('data/digits/mnist-m/7/00006830.png', 7), ('data/digits/mnist-m/9/00038344.png', 9), ('data/digits/mnist-m/2/00008036.png', 2), ('data/digits/mnist-m/8/00004468.png', 8), ('data/digits/mnist-m/6/00049258.png', 8), ('data/digits/mnist-m/7/00018020.png', 7)]\n",
      "Target dataset Val Loss: 2.2312 Val Acc: 0.3174\n",
      "Epoch 1/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8859 Train Acc: 0.9351\n",
      "Source dataset Val Loss: 0.8373 Val Acc: 0.9605\n",
      "\n",
      "Target dataset Val Loss: 2.6968 Val Acc: 0.2619\n",
      "Epoch 2/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8414 Train Acc: 0.9487\n",
      "Source dataset Val Loss: 0.7991 Val Acc: 0.9737\n",
      "\n",
      "Target dataset Val Loss: 2.1642 Val Acc: 0.3982\n",
      "Epoch 3/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7991 Train Acc: 0.9688\n",
      "Source dataset Val Loss: 0.7624 Val Acc: 0.9766\n",
      "\n",
      "Target dataset Val Loss: 2.0825 Val Acc: 0.3494\n",
      "Epoch 4/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8455 Train Acc: 0.9471\n",
      "Source dataset Val Loss: 0.7896 Val Acc: 0.9671\n",
      "\n",
      "Target dataset Val Loss: 1.9467 Val Acc: 0.4067\n",
      "Epoch 5/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8078 Train Acc: 0.9631\n",
      "Source dataset Val Loss: 0.8196 Val Acc: 0.9546\n",
      "\n",
      "Target dataset Val Loss: 2.0787 Val Acc: 0.3777\n",
      "Epoch 6/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8203 Train Acc: 0.9543\n",
      "Source dataset Val Loss: 0.7888 Val Acc: 0.9738\n",
      "\n",
      "Target dataset Val Loss: 2.1141 Val Acc: 0.3630\n",
      "Epoch 7/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7880 Train Acc: 0.9631\n",
      "Source dataset Val Loss: 0.7738 Val Acc: 0.9725\n",
      "\n",
      "Selected 251/45710 images on threshold 0.9191750201481034\n",
      "[('data/digits/mnist-m/6/00018068.png', 6), ('data/digits/mnist-m/6/00008888.png', 6), ('data/digits/mnist-m/6/00025579.png', 6), ('data/digits/mnist-m/6/00019559.png', 6), ('data/digits/mnist-m/6/00003118.png', 6), ('data/digits/mnist-m/6/00035452.png', 6), ('data/digits/mnist-m/6/00029766.png', 6), ('data/digits/mnist-m/6/00029769.png', 6), ('data/digits/mnist-m/6/00014421.png', 6), ('data/digits/mnist-m/6/00044189.png', 6), ('data/digits/mnist-m/6/00006814.png', 6), ('data/digits/mnist-m/6/00009946.png', 6), ('data/digits/mnist-m/6/00046420.png', 6), ('data/digits/mnist-m/6/00058303.png', 6), ('data/digits/mnist-m/6/00055725.png', 6), ('data/digits/mnist-m/6/00028066.png', 6), ('data/digits/mnist-m/6/00000955.png', 6), ('data/digits/mnist-m/6/00019002.png', 6), ('data/digits/mnist-m/6/00026955.png', 6), ('data/digits/mnist-m/6/00051055.png', 6), ('data/digits/mnist-m/6/00057198.png', 6), ('data/digits/mnist-m/6/00007867.png', 6), ('data/digits/mnist-m/6/00040356.png', 6), ('data/digits/mnist-m/6/00035888.png', 6), ('data/digits/mnist-m/8/00028033.png', 8), ('data/digits/mnist-m/6/00032839.png', 6), ('data/digits/mnist-m/6/00050122.png', 6), ('data/digits/mnist-m/6/00030279.png', 6), ('data/digits/mnist-m/6/00025489.png', 6), ('data/digits/mnist-m/6/00008697.png', 6), ('data/digits/mnist-m/6/00023653.png', 6), ('data/digits/mnist-m/6/00009262.png', 6), ('data/digits/mnist-m/6/00032978.png', 6), ('data/digits/mnist-m/6/00030295.png', 6), ('data/digits/mnist-m/6/00021585.png', 6), ('data/digits/mnist-m/6/00013020.png', 6), ('data/digits/mnist-m/6/00027505.png', 6), ('data/digits/mnist-m/6/00045643.png', 6), ('data/digits/mnist-m/6/00049293.png', 6), ('data/digits/mnist-m/6/00013858.png', 6), ('data/digits/mnist-m/6/00006551.png', 6), ('data/digits/mnist-m/6/00019617.png', 6), ('data/digits/mnist-m/6/00045267.png', 6), ('data/digits/mnist-m/6/00050921.png', 6), ('data/digits/mnist-m/6/00004416.png', 6), ('data/digits/mnist-m/6/00015733.png', 6), ('data/digits/mnist-m/6/00013224.png', 6), ('data/digits/mnist-m/6/00020320.png', 6), ('data/digits/mnist-m/6/00053646.png', 6), ('data/digits/mnist-m/6/00047209.png', 6), ('data/digits/mnist-m/6/00023577.png', 6), ('data/digits/mnist-m/6/00031104.png', 6), ('data/digits/mnist-m/6/00015875.png', 6), ('data/digits/mnist-m/6/00051220.png', 6), ('data/digits/mnist-m/6/00009928.png', 6), ('data/digits/mnist-m/6/00038659.png', 6), ('data/digits/mnist-m/6/00033006.png', 6), ('data/digits/mnist-m/6/00002849.png', 6), ('data/digits/mnist-m/2/00008908.png', 6), ('data/digits/mnist-m/6/00030152.png', 6), ('data/digits/mnist-m/6/00018432.png', 6), ('data/digits/mnist-m/6/00003577.png', 6), ('data/digits/mnist-m/6/00022178.png', 6), ('data/digits/mnist-m/6/00026633.png', 6), ('data/digits/mnist-m/6/00002286.png', 6), ('data/digits/mnist-m/6/00006388.png', 6), ('data/digits/mnist-m/6/00025338.png', 6), ('data/digits/mnist-m/6/00002920.png', 6), ('data/digits/mnist-m/6/00000241.png', 6), ('data/digits/mnist-m/6/00028001.png', 6), ('data/digits/mnist-m/6/00012851.png', 6), ('data/digits/mnist-m/6/00022681.png', 6), ('data/digits/mnist-m/6/00021485.png', 6), ('data/digits/mnist-m/6/00044463.png', 6), ('data/digits/mnist-m/6/00022160.png', 6), ('data/digits/mnist-m/6/00003901.png', 6), ('data/digits/mnist-m/6/00055379.png', 6), ('data/digits/mnist-m/6/00028459.png', 6), ('data/digits/mnist-m/6/00007705.png', 6), ('data/digits/mnist-m/6/00047824.png', 6), ('data/digits/mnist-m/6/00027198.png', 6), ('data/digits/mnist-m/6/00055255.png', 6), ('data/digits/mnist-m/6/00005834.png', 6), ('data/digits/mnist-m/6/00044666.png', 6), ('data/digits/mnist-m/6/00006354.png', 6), ('data/digits/mnist-m/6/00006591.png', 6), ('data/digits/mnist-m/6/00034490.png', 6), ('data/digits/mnist-m/6/00034009.png', 6), ('data/digits/mnist-m/6/00054523.png', 6), ('data/digits/mnist-m/6/00000138.png', 6), ('data/digits/mnist-m/8/00043828.png', 8), ('data/digits/mnist-m/6/00028639.png', 6), ('data/digits/mnist-m/6/00027794.png', 6), ('data/digits/mnist-m/6/00002153.png', 6), ('data/digits/mnist-m/6/00016976.png', 6), ('data/digits/mnist-m/6/00002196.png', 6), ('data/digits/mnist-m/0/00015520.png', 6), ('data/digits/mnist-m/6/00028115.png', 6), ('data/digits/mnist-m/6/00048219.png', 6), ('data/digits/mnist-m/6/00030904.png', 6), ('data/digits/mnist-m/6/00028711.png', 6), ('data/digits/mnist-m/6/00005066.png', 6), ('data/digits/mnist-m/6/00004517.png', 6), ('data/digits/mnist-m/6/00002573.png', 6), ('data/digits/mnist-m/6/00053197.png', 6), ('data/digits/mnist-m/6/00018302.png', 6), ('data/digits/mnist-m/6/00049802.png', 6), ('data/digits/mnist-m/6/00013712.png', 6), ('data/digits/mnist-m/6/00056692.png', 6), ('data/digits/mnist-m/6/00001733.png', 6), ('data/digits/mnist-m/0/00036783.png', 6), ('data/digits/mnist-m/6/00053892.png', 6), ('data/digits/mnist-m/6/00024074.png', 6), ('data/digits/mnist-m/6/00024963.png', 6), ('data/digits/mnist-m/8/00054626.png', 8), ('data/digits/mnist-m/6/00018140.png', 6), ('data/digits/mnist-m/6/00044925.png', 6), ('data/digits/mnist-m/6/00037895.png', 6), ('data/digits/mnist-m/6/00010996.png', 6), ('data/digits/mnist-m/6/00050761.png', 6), ('data/digits/mnist-m/6/00043720.png', 6), ('data/digits/mnist-m/6/00053481.png', 6), ('data/digits/mnist-m/6/00044571.png', 6), ('data/digits/mnist-m/6/00021061.png', 6), ('data/digits/mnist-m/6/00022221.png', 6), ('data/digits/mnist-m/6/00011448.png', 6), ('data/digits/mnist-m/8/00006297.png', 8), ('data/digits/mnist-m/6/00005230.png', 6), ('data/digits/mnist-m/6/00000392.png', 6), ('data/digits/mnist-m/6/00002337.png', 6), ('data/digits/mnist-m/6/00019881.png', 6), ('data/digits/mnist-m/6/00029981.png', 6), ('data/digits/mnist-m/6/00056182.png', 6), ('data/digits/mnist-m/6/00041869.png', 6), ('data/digits/mnist-m/6/00006922.png', 6), ('data/digits/mnist-m/6/00044336.png', 6), ('data/digits/mnist-m/8/00034304.png', 8), ('data/digits/mnist-m/6/00000973.png', 6), ('data/digits/mnist-m/6/00019157.png', 6), ('data/digits/mnist-m/6/00006550.png', 6), ('data/digits/mnist-m/6/00037675.png', 6), ('data/digits/mnist-m/6/00032999.png', 6), ('data/digits/mnist-m/6/00053336.png', 6), ('data/digits/mnist-m/8/00001717.png', 8), ('data/digits/mnist-m/6/00051528.png', 6), ('data/digits/mnist-m/6/00034342.png', 6), ('data/digits/mnist-m/6/00055135.png', 6), ('data/digits/mnist-m/6/00033258.png', 6), ('data/digits/mnist-m/6/00036561.png', 6), ('data/digits/mnist-m/6/00031071.png', 6), ('data/digits/mnist-m/6/00005279.png', 6), ('data/digits/mnist-m/6/00033434.png', 6), ('data/digits/mnist-m/6/00017273.png', 6), ('data/digits/mnist-m/6/00008850.png', 6), ('data/digits/mnist-m/6/00022758.png', 6), ('data/digits/mnist-m/6/00026342.png', 6), ('data/digits/mnist-m/6/00031903.png', 6), ('data/digits/mnist-m/6/00034816.png', 6), ('data/digits/mnist-m/6/00021772.png', 6), ('data/digits/mnist-m/6/00054113.png', 6), ('data/digits/mnist-m/6/00008224.png', 6), ('data/digits/mnist-m/6/00042154.png', 6), ('data/digits/mnist-m/6/00002226.png', 6), ('data/digits/mnist-m/4/00023270.png', 6), ('data/digits/mnist-m/6/00057122.png', 6), ('data/digits/mnist-m/6/00011195.png', 6), ('data/digits/mnist-m/6/00016317.png', 6), ('data/digits/mnist-m/6/00026768.png', 6), ('data/digits/mnist-m/6/00022814.png', 6), ('data/digits/mnist-m/6/00025141.png', 6), ('data/digits/mnist-m/6/00044809.png', 6), ('data/digits/mnist-m/6/00018033.png', 6), ('data/digits/mnist-m/6/00032623.png', 6), ('data/digits/mnist-m/6/00005334.png', 6), ('data/digits/mnist-m/0/00012880.png', 6), ('data/digits/mnist-m/6/00019873.png', 6), ('data/digits/mnist-m/6/00025593.png', 6), ('data/digits/mnist-m/6/00053606.png', 6), ('data/digits/mnist-m/6/00015431.png', 6), ('data/digits/mnist-m/6/00034741.png', 6), ('data/digits/mnist-m/6/00044953.png', 6), ('data/digits/mnist-m/6/00033887.png', 6), ('data/digits/mnist-m/6/00028997.png', 6), ('data/digits/mnist-m/0/00049110.png', 6), ('data/digits/mnist-m/6/00007096.png', 6), ('data/digits/mnist-m/6/00008779.png', 6), ('data/digits/mnist-m/8/00050114.png', 8), ('data/digits/mnist-m/6/00007758.png', 6), ('data/digits/mnist-m/6/00051995.png', 6), ('data/digits/mnist-m/6/00028934.png', 6), ('data/digits/mnist-m/6/00038535.png', 6), ('data/digits/mnist-m/6/00040515.png', 6), ('data/digits/mnist-m/6/00058488.png', 6), ('data/digits/mnist-m/6/00030199.png', 6), ('data/digits/mnist-m/6/00037356.png', 6), ('data/digits/mnist-m/6/00018134.png', 6), ('data/digits/mnist-m/6/00029774.png', 6), ('data/digits/mnist-m/6/00058404.png', 6), ('data/digits/mnist-m/6/00041198.png', 6), ('data/digits/mnist-m/6/00019761.png', 6), ('data/digits/mnist-m/8/00003711.png', 8), ('data/digits/mnist-m/6/00004388.png', 6), ('data/digits/mnist-m/6/00055515.png', 6), ('data/digits/mnist-m/6/00007224.png', 6), ('data/digits/mnist-m/6/00040862.png', 6), ('data/digits/mnist-m/6/00003979.png', 6), ('data/digits/mnist-m/6/00033929.png', 6), ('data/digits/mnist-m/6/00033735.png', 6), ('data/digits/mnist-m/6/00008042.png', 6), ('data/digits/mnist-m/6/00045209.png', 6), ('data/digits/mnist-m/6/00007456.png', 6), ('data/digits/mnist-m/8/00054469.png', 8), ('data/digits/mnist-m/6/00038548.png', 6), ('data/digits/mnist-m/6/00043620.png', 6), ('data/digits/mnist-m/6/00054409.png', 6), ('data/digits/mnist-m/6/00038327.png', 6), ('data/digits/mnist-m/6/00051410.png', 6), ('data/digits/mnist-m/6/00047265.png', 6), ('data/digits/mnist-m/6/00048427.png', 6), ('data/digits/mnist-m/6/00006355.png', 6), ('data/digits/mnist-m/6/00032752.png', 6), ('data/digits/mnist-m/6/00023471.png', 6), ('data/digits/mnist-m/6/00013758.png', 6), ('data/digits/mnist-m/6/00030128.png', 6), ('data/digits/mnist-m/6/00000083.png', 6), ('data/digits/mnist-m/6/00002533.png', 6), ('data/digits/mnist-m/8/00004075.png', 8), ('data/digits/mnist-m/6/00047864.png', 6), ('data/digits/mnist-m/6/00052859.png', 6), ('data/digits/mnist-m/6/00013349.png', 6), ('data/digits/mnist-m/6/00008769.png', 6), ('data/digits/mnist-m/6/00022425.png', 6), ('data/digits/mnist-m/6/00048317.png', 6), ('data/digits/mnist-m/6/00052897.png', 6), ('data/digits/mnist-m/6/00002545.png', 6), ('data/digits/mnist-m/0/00052202.png', 6), ('data/digits/mnist-m/6/00031054.png', 6), ('data/digits/mnist-m/6/00023465.png', 6), ('data/digits/mnist-m/6/00004908.png', 6), ('data/digits/mnist-m/6/00006315.png', 6), ('data/digits/mnist-m/6/00028386.png', 6), ('data/digits/mnist-m/6/00050108.png', 6), ('data/digits/mnist-m/6/00038431.png', 6), ('data/digits/mnist-m/6/00030444.png', 6), ('data/digits/mnist-m/6/00039251.png', 6), ('data/digits/mnist-m/6/00027483.png', 6), ('data/digits/mnist-m/6/00051152.png', 6), ('data/digits/mnist-m/6/00015348.png', 6), ('data/digits/mnist-m/6/00005441.png', 6), ('data/digits/mnist-m/6/00058176.png', 6), ('data/digits/mnist-m/6/00054948.png', 6)]\n",
      "Target dataset Val Loss: 2.0351 Val Acc: 0.4185\n",
      "Epoch 8/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7999 Train Acc: 0.9666\n",
      "Source dataset Val Loss: 0.8001 Val Acc: 0.9666\n",
      "\n",
      "Target dataset Val Loss: 2.0312 Val Acc: 0.4361\n",
      "Epoch 9/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7560 Train Acc: 0.9787\n",
      "Source dataset Val Loss: 0.7589 Val Acc: 0.9706\n",
      "\n",
      "Target dataset Val Loss: 2.1209 Val Acc: 0.4238\n",
      "Epoch 10/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7512 Train Acc: 0.9800\n",
      "Source dataset Val Loss: 0.7483 Val Acc: 0.9803\n",
      "\n",
      "Target dataset Val Loss: 2.1052 Val Acc: 0.4202\n",
      "Epoch 11/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7863 Train Acc: 0.9713\n",
      "Source dataset Val Loss: 0.7553 Val Acc: 0.9798\n",
      "\n",
      "Target dataset Val Loss: 1.9456 Val Acc: 0.4577\n",
      "Epoch 12/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7834 Train Acc: 0.9700\n",
      "Source dataset Val Loss: 0.7526 Val Acc: 0.9770\n",
      "\n",
      "Target dataset Val Loss: 2.0110 Val Acc: 0.4400\n",
      "Epoch 13/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7801 Train Acc: 0.9673\n",
      "Source dataset Val Loss: 0.7893 Val Acc: 0.9643\n",
      "\n",
      "Target dataset Val Loss: 2.4158 Val Acc: 0.3801\n",
      "Epoch 14/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7736 Train Acc: 0.9713\n",
      "Source dataset Val Loss: 0.7668 Val Acc: 0.9737\n",
      "\n",
      "Selected 295/45459 images on threshold 0.9194046743655258\n",
      "[('data/digits/mnist-m/9/00028383.png', 9), ('data/digits/mnist-m/8/00013562.png', 9), ('data/digits/mnist-m/9/00007991.png', 9), ('data/digits/mnist-m/8/00038146.png', 9), ('data/digits/mnist-m/7/00055329.png', 9), ('data/digits/mnist-m/9/00006272.png', 9), ('data/digits/mnist-m/9/00035182.png', 9), ('data/digits/mnist-m/9/00026815.png', 9), ('data/digits/mnist-m/9/00002331.png', 9), ('data/digits/mnist-m/9/00038204.png', 9), ('data/digits/mnist-m/9/00036007.png', 9), ('data/digits/mnist-m/9/00008503.png', 9), ('data/digits/mnist-m/9/00041065.png', 9), ('data/digits/mnist-m/9/00021842.png', 9), ('data/digits/mnist-m/9/00038887.png', 9), ('data/digits/mnist-m/9/00039073.png', 9), ('data/digits/mnist-m/8/00025072.png', 9), ('data/digits/mnist-m/9/00004547.png', 9), ('data/digits/mnist-m/9/00017826.png', 9), ('data/digits/mnist-m/9/00037474.png', 9), ('data/digits/mnist-m/9/00041605.png', 9), ('data/digits/mnist-m/9/00000304.png', 9), ('data/digits/mnist-m/9/00014331.png', 9), ('data/digits/mnist-m/9/00002267.png', 9), ('data/digits/mnist-m/9/00048257.png', 9), ('data/digits/mnist-m/9/00000616.png', 9), ('data/digits/mnist-m/9/00005577.png', 9), ('data/digits/mnist-m/9/00037352.png', 9), ('data/digits/mnist-m/9/00055007.png', 9), ('data/digits/mnist-m/9/00039538.png', 9), ('data/digits/mnist-m/9/00002460.png', 9), ('data/digits/mnist-m/9/00030237.png', 9), ('data/digits/mnist-m/9/00000162.png', 9), ('data/digits/mnist-m/7/00006315.png', 9), ('data/digits/mnist-m/9/00055114.png', 9), ('data/digits/mnist-m/7/00016197.png', 9), ('data/digits/mnist-m/8/00038476.png', 9), ('data/digits/mnist-m/9/00036777.png', 9), ('data/digits/mnist-m/9/00032767.png', 9), ('data/digits/mnist-m/9/00057047.png', 9), ('data/digits/mnist-m/9/00004667.png', 9), ('data/digits/mnist-m/9/00026921.png', 9), ('data/digits/mnist-m/9/00057085.png', 9), ('data/digits/mnist-m/9/00006807.png', 9), ('data/digits/mnist-m/9/00041738.png', 9), ('data/digits/mnist-m/9/00039614.png', 9), ('data/digits/mnist-m/9/00034301.png', 9), ('data/digits/mnist-m/9/00045873.png', 9), ('data/digits/mnist-m/9/00004294.png', 9), ('data/digits/mnist-m/9/00010860.png', 9), ('data/digits/mnist-m/9/00034983.png', 9), ('data/digits/mnist-m/7/00006381.png', 9), ('data/digits/mnist-m/7/00001851.png', 9), ('data/digits/mnist-m/9/00009784.png', 9), ('data/digits/mnist-m/9/00026833.png', 9), ('data/digits/mnist-m/9/00021757.png', 9), ('data/digits/mnist-m/8/00014625.png', 9), ('data/digits/mnist-m/9/00006190.png', 9), ('data/digits/mnist-m/9/00056733.png', 9), ('data/digits/mnist-m/9/00029426.png', 9), ('data/digits/mnist-m/5/00002077.png', 5), ('data/digits/mnist-m/8/00006470.png', 9), ('data/digits/mnist-m/9/00044402.png', 9), ('data/digits/mnist-m/9/00055571.png', 9), ('data/digits/mnist-m/9/00050391.png', 9), ('data/digits/mnist-m/9/00023327.png', 9), ('data/digits/mnist-m/8/00000429.png', 9), ('data/digits/mnist-m/9/00046985.png', 9), ('data/digits/mnist-m/9/00015746.png', 9), ('data/digits/mnist-m/7/00039557.png', 9), ('data/digits/mnist-m/9/00007478.png', 9), ('data/digits/mnist-m/9/00022804.png', 9), ('data/digits/mnist-m/9/00033592.png', 9), ('data/digits/mnist-m/9/00039660.png', 9), ('data/digits/mnist-m/9/00017959.png', 9), ('data/digits/mnist-m/9/00010306.png', 9), ('data/digits/mnist-m/1/00030920.png', 9), ('data/digits/mnist-m/9/00000170.png', 9), ('data/digits/mnist-m/9/00003889.png', 9), ('data/digits/mnist-m/9/00018131.png', 9), ('data/digits/mnist-m/9/00011947.png', 9), ('data/digits/mnist-m/9/00019781.png', 9), ('data/digits/mnist-m/9/00056230.png', 9), ('data/digits/mnist-m/9/00053683.png', 9), ('data/digits/mnist-m/7/00055863.png', 9), ('data/digits/mnist-m/9/00026643.png', 9), ('data/digits/mnist-m/9/00006887.png', 9), ('data/digits/mnist-m/8/00013476.png', 9), ('data/digits/mnist-m/9/00050349.png', 9), ('data/digits/mnist-m/9/00053915.png', 9), ('data/digits/mnist-m/9/00011423.png', 9), ('data/digits/mnist-m/9/00018510.png', 9), ('data/digits/mnist-m/9/00015253.png', 9), ('data/digits/mnist-m/9/00003041.png', 9), ('data/digits/mnist-m/9/00023314.png', 9), ('data/digits/mnist-m/9/00004627.png', 9), ('data/digits/mnist-m/9/00004240.png', 9), ('data/digits/mnist-m/9/00034292.png', 9), ('data/digits/mnist-m/9/00012900.png', 9), ('data/digits/mnist-m/9/00027848.png', 9), ('data/digits/mnist-m/9/00018860.png', 9), ('data/digits/mnist-m/9/00055313.png', 9), ('data/digits/mnist-m/9/00003286.png', 9), ('data/digits/mnist-m/8/00039438.png', 9), ('data/digits/mnist-m/9/00021020.png', 9), ('data/digits/mnist-m/8/00024768.png', 9), ('data/digits/mnist-m/9/00021631.png', 9), ('data/digits/mnist-m/9/00014726.png', 9), ('data/digits/mnist-m/8/00050571.png', 9), ('data/digits/mnist-m/9/00031700.png', 9), ('data/digits/mnist-m/9/00009347.png', 9), ('data/digits/mnist-m/9/00040531.png', 9), ('data/digits/mnist-m/9/00004407.png', 9), ('data/digits/mnist-m/9/00002274.png', 9), ('data/digits/mnist-m/9/00022471.png', 9), ('data/digits/mnist-m/8/00025018.png', 9), ('data/digits/mnist-m/9/00001939.png', 9), ('data/digits/mnist-m/9/00057239.png', 9), ('data/digits/mnist-m/9/00010577.png', 9), ('data/digits/mnist-m/9/00001714.png', 9), ('data/digits/mnist-m/9/00035135.png', 9), ('data/digits/mnist-m/9/00005979.png', 9), ('data/digits/mnist-m/9/00001732.png', 9), ('data/digits/mnist-m/9/00032064.png', 9), ('data/digits/mnist-m/9/00014037.png', 9), ('data/digits/mnist-m/9/00054984.png', 9), ('data/digits/mnist-m/9/00025589.png', 9), ('data/digits/mnist-m/9/00023467.png', 9), ('data/digits/mnist-m/9/00049319.png', 9), ('data/digits/mnist-m/9/00036960.png', 9), ('data/digits/mnist-m/9/00032380.png', 9), ('data/digits/mnist-m/9/00001895.png', 9), ('data/digits/mnist-m/9/00039911.png', 9), ('data/digits/mnist-m/9/00038917.png', 9), ('data/digits/mnist-m/4/00014193.png', 9), ('data/digits/mnist-m/9/00004124.png', 9), ('data/digits/mnist-m/9/00000566.png', 9), ('data/digits/mnist-m/9/00019437.png', 9), ('data/digits/mnist-m/9/00005424.png', 9), ('data/digits/mnist-m/9/00004856.png', 9), ('data/digits/mnist-m/9/00006036.png', 9), ('data/digits/mnist-m/9/00006055.png', 9), ('data/digits/mnist-m/9/00008487.png', 9), ('data/digits/mnist-m/9/00019715.png', 9), ('data/digits/mnist-m/9/00001061.png', 9), ('data/digits/mnist-m/8/00057826.png', 9), ('data/digits/mnist-m/4/00001604.png', 9), ('data/digits/mnist-m/9/00010516.png', 9), ('data/digits/mnist-m/8/00041106.png', 9), ('data/digits/mnist-m/9/00048130.png', 9), ('data/digits/mnist-m/9/00019644.png', 9), ('data/digits/mnist-m/8/00007122.png', 9), ('data/digits/mnist-m/9/00028463.png', 9), ('data/digits/mnist-m/9/00007975.png', 9), ('data/digits/mnist-m/9/00026973.png', 9), ('data/digits/mnist-m/9/00045044.png', 9), ('data/digits/mnist-m/9/00030942.png', 9), ('data/digits/mnist-m/8/00010936.png', 9), ('data/digits/mnist-m/9/00036606.png', 9), ('data/digits/mnist-m/9/00009825.png', 9), ('data/digits/mnist-m/9/00011991.png', 9), ('data/digits/mnist-m/7/00009143.png', 9), ('data/digits/mnist-m/9/00006249.png', 9), ('data/digits/mnist-m/8/00001974.png', 9), ('data/digits/mnist-m/4/00034988.png', 9), ('data/digits/mnist-m/8/00019618.png', 9), ('data/digits/mnist-m/8/00017743.png', 9), ('data/digits/mnist-m/4/00008520.png', 9), ('data/digits/mnist-m/9/00007867.png', 9), ('data/digits/mnist-m/9/00006779.png', 9), ('data/digits/mnist-m/8/00026664.png', 9), ('data/digits/mnist-m/9/00049466.png', 9), ('data/digits/mnist-m/9/00014974.png', 9), ('data/digits/mnist-m/9/00052943.png', 9), ('data/digits/mnist-m/8/00039994.png', 9), ('data/digits/mnist-m/9/00005813.png', 9), ('data/digits/mnist-m/9/00041155.png', 9), ('data/digits/mnist-m/7/00014033.png', 9), ('data/digits/mnist-m/9/00045762.png', 9), ('data/digits/mnist-m/9/00024983.png', 9), ('data/digits/mnist-m/9/00023278.png', 9), ('data/digits/mnist-m/9/00012342.png', 9), ('data/digits/mnist-m/9/00052653.png', 9), ('data/digits/mnist-m/9/00051430.png', 9), ('data/digits/mnist-m/9/00005268.png', 9), ('data/digits/mnist-m/9/00010495.png', 9), ('data/digits/mnist-m/9/00001465.png', 9), ('data/digits/mnist-m/9/00026861.png', 9), ('data/digits/mnist-m/9/00003099.png', 9), ('data/digits/mnist-m/9/00050754.png', 9), ('data/digits/mnist-m/8/00049822.png', 9), ('data/digits/mnist-m/9/00018355.png', 9), ('data/digits/mnist-m/9/00010004.png', 9), ('data/digits/mnist-m/9/00009066.png', 9), ('data/digits/mnist-m/9/00058639.png', 9), ('data/digits/mnist-m/9/00050169.png', 9), ('data/digits/mnist-m/9/00006221.png', 9), ('data/digits/mnist-m/9/00028674.png', 9), ('data/digits/mnist-m/9/00003827.png', 9), ('data/digits/mnist-m/9/00054081.png', 9), ('data/digits/mnist-m/9/00036679.png', 9), ('data/digits/mnist-m/9/00011280.png', 9), ('data/digits/mnist-m/9/00034019.png', 9), ('data/digits/mnist-m/9/00016953.png', 9), ('data/digits/mnist-m/9/00038230.png', 9), ('data/digits/mnist-m/9/00002692.png', 9), ('data/digits/mnist-m/9/00056883.png', 9), ('data/digits/mnist-m/9/00038704.png', 9), ('data/digits/mnist-m/9/00004701.png', 9), ('data/digits/mnist-m/9/00001152.png', 9), ('data/digits/mnist-m/9/00055464.png', 9), ('data/digits/mnist-m/9/00029161.png', 9), ('data/digits/mnist-m/9/00058927.png', 9), ('data/digits/mnist-m/8/00014191.png', 9), ('data/digits/mnist-m/9/00004594.png', 9), ('data/digits/mnist-m/9/00004769.png', 9), ('data/digits/mnist-m/9/00054146.png', 9), ('data/digits/mnist-m/9/00049844.png', 9), ('data/digits/mnist-m/9/00005892.png', 9), ('data/digits/mnist-m/9/00036402.png', 9), ('data/digits/mnist-m/9/00054021.png', 9), ('data/digits/mnist-m/7/00055895.png', 9), ('data/digits/mnist-m/9/00009870.png', 9), ('data/digits/mnist-m/9/00058213.png', 9), ('data/digits/mnist-m/9/00029406.png', 9), ('data/digits/mnist-m/9/00011246.png', 9), ('data/digits/mnist-m/9/00019860.png', 9), ('data/digits/mnist-m/9/00047341.png', 9), ('data/digits/mnist-m/9/00042118.png', 9), ('data/digits/mnist-m/9/00046066.png', 9), ('data/digits/mnist-m/9/00041813.png', 9), ('data/digits/mnist-m/9/00007853.png', 9), ('data/digits/mnist-m/9/00001552.png', 9), ('data/digits/mnist-m/9/00008076.png', 9), ('data/digits/mnist-m/9/00058735.png', 9), ('data/digits/mnist-m/9/00005817.png', 9), ('data/digits/mnist-m/8/00014068.png', 9), ('data/digits/mnist-m/9/00023989.png', 9), ('data/digits/mnist-m/9/00043777.png', 9), ('data/digits/mnist-m/9/00001013.png', 9), ('data/digits/mnist-m/4/00035136.png', 9), ('data/digits/mnist-m/9/00056708.png', 9), ('data/digits/mnist-m/9/00023463.png', 9), ('data/digits/mnist-m/9/00014057.png', 9), ('data/digits/mnist-m/9/00053847.png', 9), ('data/digits/mnist-m/9/00020434.png', 9), ('data/digits/mnist-m/8/00011984.png', 9), ('data/digits/mnist-m/9/00011001.png', 9), ('data/digits/mnist-m/9/00057281.png', 9), ('data/digits/mnist-m/9/00016521.png', 9), ('data/digits/mnist-m/9/00015858.png', 9), ('data/digits/mnist-m/9/00053752.png', 9), ('data/digits/mnist-m/9/00010387.png', 9), ('data/digits/mnist-m/9/00031987.png', 9), ('data/digits/mnist-m/9/00002845.png', 9), ('data/digits/mnist-m/9/00003621.png', 9), ('data/digits/mnist-m/9/00000788.png', 9), ('data/digits/mnist-m/9/00025446.png', 9), ('data/digits/mnist-m/9/00018552.png', 9), ('data/digits/mnist-m/9/00018111.png', 9), ('data/digits/mnist-m/7/00007533.png', 9), ('data/digits/mnist-m/4/00045121.png', 9), ('data/digits/mnist-m/9/00037136.png', 9), ('data/digits/mnist-m/9/00007089.png', 9), ('data/digits/mnist-m/9/00010576.png', 9), ('data/digits/mnist-m/9/00050177.png', 9), ('data/digits/mnist-m/9/00027581.png', 9), ('data/digits/mnist-m/9/00000481.png', 9), ('data/digits/mnist-m/9/00007962.png', 9), ('data/digits/mnist-m/7/00009433.png', 9), ('data/digits/mnist-m/7/00040323.png', 9), ('data/digits/mnist-m/7/00015468.png', 9), ('data/digits/mnist-m/9/00023446.png', 9), ('data/digits/mnist-m/7/00040545.png', 9), ('data/digits/mnist-m/9/00041058.png', 9), ('data/digits/mnist-m/9/00003970.png', 9), ('data/digits/mnist-m/9/00014042.png', 9), ('data/digits/mnist-m/4/00000740.png', 9), ('data/digits/mnist-m/4/00003235.png', 9), ('data/digits/mnist-m/9/00018688.png', 9), ('data/digits/mnist-m/9/00018722.png', 9), ('data/digits/mnist-m/9/00029419.png', 9), ('data/digits/mnist-m/9/00024285.png', 9), ('data/digits/mnist-m/9/00045415.png', 9), ('data/digits/mnist-m/4/00005002.png', 9), ('data/digits/mnist-m/9/00048036.png', 9), ('data/digits/mnist-m/9/00018668.png', 9), ('data/digits/mnist-m/9/00056270.png', 9), ('data/digits/mnist-m/9/00006886.png', 9), ('data/digits/mnist-m/9/00010929.png', 9), ('data/digits/mnist-m/9/00056634.png', 9), ('data/digits/mnist-m/7/00043307.png', 9), ('data/digits/mnist-m/9/00046839.png', 9), ('data/digits/mnist-m/9/00048633.png', 9), ('data/digits/mnist-m/8/00017658.png', 9)]\n",
      "Target dataset Val Loss: 2.0808 Val Acc: 0.3993\n",
      "Epoch 15/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8398 Train Acc: 0.9515\n",
      "Source dataset Val Loss: 0.7868 Val Acc: 0.9670\n",
      "\n",
      "Target dataset Val Loss: 2.1261 Val Acc: 0.4216\n",
      "Epoch 16/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7887 Train Acc: 0.9632\n",
      "Source dataset Val Loss: 0.7851 Val Acc: 0.9658\n",
      "\n",
      "Target dataset Val Loss: 2.0365 Val Acc: 0.4203\n",
      "Epoch 17/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7836 Train Acc: 0.9710\n",
      "Source dataset Val Loss: 0.7530 Val Acc: 0.9781\n",
      "\n",
      "Target dataset Val Loss: 1.8902 Val Acc: 0.4488\n",
      "Epoch 18/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7667 Train Acc: 0.9716\n",
      "Source dataset Val Loss: 0.7444 Val Acc: 0.9797\n",
      "\n",
      "Target dataset Val Loss: 2.0030 Val Acc: 0.4564\n",
      "Epoch 19/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7739 Train Acc: 0.9705\n",
      "Source dataset Val Loss: 0.7528 Val Acc: 0.9812\n",
      "\n",
      "Target dataset Val Loss: 2.0433 Val Acc: 0.4278\n",
      "Epoch 20/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8947 Train Acc: 0.9220\n",
      "Source dataset Val Loss: 0.7703 Val Acc: 0.9730\n",
      "\n",
      "Target dataset Val Loss: 2.0491 Val Acc: 0.3763\n",
      "Epoch 21/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7841 Train Acc: 0.9627\n",
      "Source dataset Val Loss: 0.7610 Val Acc: 0.9764\n",
      "\n",
      "Selected 0/45164 images on threshold 0.9198972932906156\n",
      "[]\n",
      "Target dataset Val Loss: 2.0671 Val Acc: 0.3812\n",
      "Epoch 22/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7552 Train Acc: 0.9755\n",
      "Source dataset Val Loss: 0.7504 Val Acc: 0.9795\n",
      "\n",
      "Target dataset Val Loss: 2.1504 Val Acc: 0.3706\n",
      "Epoch 23/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.8814 Train Acc: 0.9303\n",
      "Source dataset Val Loss: 0.7849 Val Acc: 0.9657\n",
      "\n",
      "Target dataset Val Loss: 2.0905 Val Acc: 0.3706\n",
      "Epoch 24/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7856 Train Acc: 0.9660\n",
      "Source dataset Val Loss: 0.7719 Val Acc: 0.9703\n",
      "\n",
      "Target dataset Val Loss: 2.0075 Val Acc: 0.3868\n",
      "Epoch 25/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7656 Train Acc: 0.9749\n",
      "Source dataset Val Loss: 0.7586 Val Acc: 0.9754\n",
      "\n",
      "Target dataset Val Loss: 1.9695 Val Acc: 0.4061\n",
      "Epoch 26/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7651 Train Acc: 0.9716\n",
      "Source dataset Val Loss: 0.7498 Val Acc: 0.9769\n",
      "\n",
      "Target dataset Val Loss: 1.9864 Val Acc: 0.3911\n",
      "Epoch 27/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7601 Train Acc: 0.9749\n",
      "Source dataset Val Loss: 0.7525 Val Acc: 0.9779\n",
      "\n",
      "Target dataset Val Loss: 1.9602 Val Acc: 0.4030\n",
      "Epoch 28/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7587 Train Acc: 0.9783\n",
      "Source dataset Val Loss: 0.7475 Val Acc: 0.9787\n",
      "\n",
      "Selected 3/45164 images on threshold 0.9203173486218406\n",
      "[('data/digits/mnist-m/4/00044593.png', 4), ('data/digits/mnist-m/4/00026808.png', 4), ('data/digits/mnist-m/4/00049843.png', 4)]\n",
      "Target dataset Val Loss: 2.0680 Val Acc: 0.3828\n",
      "Epoch 29/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7429 Train Acc: 0.9833\n",
      "Source dataset Val Loss: 0.7415 Val Acc: 0.9810\n",
      "\n",
      "Target dataset Val Loss: 2.1470 Val Acc: 0.3712\n",
      "Epoch 30/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7374 Train Acc: 0.9878\n",
      "Source dataset Val Loss: 0.7360 Val Acc: 0.9824\n",
      "\n",
      "Target dataset Val Loss: 2.0386 Val Acc: 0.3916\n",
      "Epoch 31/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7562 Train Acc: 0.9755\n",
      "Source dataset Val Loss: 0.7390 Val Acc: 0.9818\n",
      "\n",
      "Target dataset Val Loss: 2.0070 Val Acc: 0.3997\n",
      "Epoch 32/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7489 Train Acc: 0.9794\n",
      "Source dataset Val Loss: 0.7423 Val Acc: 0.9819\n",
      "\n",
      "Target dataset Val Loss: 1.9999 Val Acc: 0.4037\n",
      "Epoch 33/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7518 Train Acc: 0.9811\n",
      "Source dataset Val Loss: 0.7325 Val Acc: 0.9842\n",
      "\n",
      "Target dataset Val Loss: 1.9444 Val Acc: 0.4268\n",
      "Epoch 34/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7341 Train Acc: 0.9850\n",
      "Source dataset Val Loss: 0.7347 Val Acc: 0.9850\n",
      "\n",
      "Target dataset Val Loss: 1.9138 Val Acc: 0.4334\n",
      "Epoch 35/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7267 Train Acc: 0.9861\n",
      "Source dataset Val Loss: 0.7332 Val Acc: 0.9857\n",
      "\n",
      "Selected 27/45161 images on threshold 0.9215998809046045\n",
      "[('data/digits/mnist-m/3/00020096.png', 3), ('data/digits/mnist-m/3/00028276.png', 3), ('data/digits/mnist-m/3/00003591.png', 3), ('data/digits/mnist-m/3/00003057.png', 3), ('data/digits/mnist-m/3/00008188.png', 3), ('data/digits/mnist-m/3/00045940.png', 3), ('data/digits/mnist-m/3/00015077.png', 3), ('data/digits/mnist-m/3/00032044.png', 3), ('data/digits/mnist-m/0/00000794.png', 0), ('data/digits/mnist-m/3/00010743.png', 3), ('data/digits/mnist-m/3/00022728.png', 3), ('data/digits/mnist-m/3/00053385.png', 3), ('data/digits/mnist-m/3/00013045.png', 3), ('data/digits/mnist-m/3/00038294.png', 3), ('data/digits/mnist-m/3/00018612.png', 3), ('data/digits/mnist-m/3/00045655.png', 3), ('data/digits/mnist-m/3/00044192.png', 3), ('data/digits/mnist-m/3/00049376.png', 3), ('data/digits/mnist-m/3/00001109.png', 3), ('data/digits/mnist-m/3/00017752.png', 3), ('data/digits/mnist-m/3/00052371.png', 3), ('data/digits/mnist-m/3/00032933.png', 3), ('data/digits/mnist-m/3/00048770.png', 3), ('data/digits/mnist-m/3/00030362.png', 3), ('data/digits/mnist-m/3/00050372.png', 3), ('data/digits/mnist-m/3/00005392.png', 3), ('data/digits/mnist-m/3/00034642.png', 3)]\n",
      "Target dataset Val Loss: 1.9062 Val Acc: 0.4312\n",
      "Epoch 36/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7525 Train Acc: 0.9781\n",
      "Source dataset Val Loss: 0.7307 Val Acc: 0.9856\n",
      "\n",
      "Target dataset Val Loss: 1.9562 Val Acc: 0.4333\n",
      "Epoch 37/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7310 Train Acc: 0.9874\n",
      "Source dataset Val Loss: 0.7352 Val Acc: 0.9831\n",
      "\n",
      "Target dataset Val Loss: 1.9665 Val Acc: 0.4298\n",
      "Epoch 38/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7432 Train Acc: 0.9803\n",
      "Source dataset Val Loss: 0.7366 Val Acc: 0.9849\n",
      "\n",
      "Target dataset Val Loss: 1.9576 Val Acc: 0.4373\n",
      "Epoch 39/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7403 Train Acc: 0.9836\n",
      "Source dataset Val Loss: 0.7344 Val Acc: 0.9834\n",
      "\n",
      "Target dataset Val Loss: 1.9017 Val Acc: 0.4581\n",
      "Epoch 40/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7436 Train Acc: 0.9797\n",
      "Source dataset Val Loss: 0.7357 Val Acc: 0.9835\n",
      "\n",
      "Target dataset Val Loss: 1.9899 Val Acc: 0.4307\n",
      "Epoch 41/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7501 Train Acc: 0.9786\n",
      "Source dataset Val Loss: 0.7357 Val Acc: 0.9839\n",
      "\n",
      "Target dataset Val Loss: 2.0730 Val Acc: 0.3810\n",
      "Epoch 42/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7393 Train Acc: 0.9836\n",
      "Source dataset Val Loss: 0.7481 Val Acc: 0.9805\n",
      "\n",
      "Selected 117/45134 images on threshold 0.9206484644622953\n",
      "[('data/digits/mnist-m/2/00004985.png', 2), ('data/digits/mnist-m/2/00018264.png', 2), ('data/digits/mnist-m/2/00031493.png', 2), ('data/digits/mnist-m/2/00028366.png', 2), ('data/digits/mnist-m/2/00057815.png', 2), ('data/digits/mnist-m/2/00020832.png', 2), ('data/digits/mnist-m/2/00030122.png', 2), ('data/digits/mnist-m/2/00029851.png', 2), ('data/digits/mnist-m/2/00027535.png', 2), ('data/digits/mnist-m/2/00028336.png', 2), ('data/digits/mnist-m/2/00049702.png', 2), ('data/digits/mnist-m/2/00046288.png', 2), ('data/digits/mnist-m/2/00012213.png', 2), ('data/digits/mnist-m/2/00000635.png', 2), ('data/digits/mnist-m/2/00007555.png', 2), ('data/digits/mnist-m/2/00009472.png', 2), ('data/digits/mnist-m/2/00040851.png', 2), ('data/digits/mnist-m/2/00008625.png', 2), ('data/digits/mnist-m/2/00047114.png', 2), ('data/digits/mnist-m/2/00037296.png', 2), ('data/digits/mnist-m/2/00029853.png', 2), ('data/digits/mnist-m/2/00016166.png', 2), ('data/digits/mnist-m/2/00047378.png', 2), ('data/digits/mnist-m/2/00000629.png', 2), ('data/digits/mnist-m/2/00021207.png', 2), ('data/digits/mnist-m/2/00056079.png', 2), ('data/digits/mnist-m/2/00054025.png', 2), ('data/digits/mnist-m/2/00057649.png', 2), ('data/digits/mnist-m/2/00030094.png', 2), ('data/digits/mnist-m/2/00058590.png', 2), ('data/digits/mnist-m/2/00031935.png', 2), ('data/digits/mnist-m/2/00017369.png', 2), ('data/digits/mnist-m/2/00051001.png', 2), ('data/digits/mnist-m/2/00009658.png', 2), ('data/digits/mnist-m/2/00040580.png', 2), ('data/digits/mnist-m/2/00044611.png', 2), ('data/digits/mnist-m/2/00005739.png', 2), ('data/digits/mnist-m/2/00008907.png', 2), ('data/digits/mnist-m/2/00015263.png', 2), ('data/digits/mnist-m/2/00039669.png', 2), ('data/digits/mnist-m/2/00036374.png', 2), ('data/digits/mnist-m/2/00041937.png', 2), ('data/digits/mnist-m/2/00000375.png', 2), ('data/digits/mnist-m/2/00041121.png', 2), ('data/digits/mnist-m/2/00024036.png', 2), ('data/digits/mnist-m/2/00002261.png', 2), ('data/digits/mnist-m/2/00038417.png', 2), ('data/digits/mnist-m/2/00047306.png', 2), ('data/digits/mnist-m/2/00008550.png', 2), ('data/digits/mnist-m/2/00036846.png', 2), ('data/digits/mnist-m/2/00000186.png', 2), ('data/digits/mnist-m/2/00033160.png', 2), ('data/digits/mnist-m/2/00039939.png', 2), ('data/digits/mnist-m/2/00056821.png', 2), ('data/digits/mnist-m/2/00007082.png', 2), ('data/digits/mnist-m/2/00004144.png', 2), ('data/digits/mnist-m/2/00040985.png', 2), ('data/digits/mnist-m/2/00006138.png', 2), ('data/digits/mnist-m/2/00009841.png', 2), ('data/digits/mnist-m/2/00028731.png', 2), ('data/digits/mnist-m/2/00019519.png', 2), ('data/digits/mnist-m/2/00038396.png', 2), ('data/digits/mnist-m/2/00016781.png', 2), ('data/digits/mnist-m/2/00004345.png', 2), ('data/digits/mnist-m/2/00025574.png', 2), ('data/digits/mnist-m/2/00001880.png', 2), ('data/digits/mnist-m/2/00049408.png', 2), ('data/digits/mnist-m/2/00046386.png', 2), ('data/digits/mnist-m/2/00038862.png', 2), ('data/digits/mnist-m/2/00007601.png', 2), ('data/digits/mnist-m/2/00056864.png', 2), ('data/digits/mnist-m/2/00031449.png', 2), ('data/digits/mnist-m/2/00053637.png', 2), ('data/digits/mnist-m/2/00045464.png', 2), ('data/digits/mnist-m/2/00022356.png', 2), ('data/digits/mnist-m/2/00052118.png', 2), ('data/digits/mnist-m/2/00044823.png', 2), ('data/digits/mnist-m/2/00019606.png', 2), ('data/digits/mnist-m/2/00007523.png', 2), ('data/digits/mnist-m/2/00001874.png', 2), ('data/digits/mnist-m/2/00020313.png', 2), ('data/digits/mnist-m/2/00032872.png', 2), ('data/digits/mnist-m/2/00023177.png', 2), ('data/digits/mnist-m/2/00050581.png', 2), ('data/digits/mnist-m/2/00016532.png', 2), ('data/digits/mnist-m/2/00011521.png', 2), ('data/digits/mnist-m/2/00042792.png', 2), ('data/digits/mnist-m/2/00053720.png', 2), ('data/digits/mnist-m/2/00018922.png', 2), ('data/digits/mnist-m/2/00006386.png', 2), ('data/digits/mnist-m/2/00053239.png', 2), ('data/digits/mnist-m/2/00037418.png', 2), ('data/digits/mnist-m/2/00008413.png', 2), ('data/digits/mnist-m/2/00019097.png', 2), ('data/digits/mnist-m/2/00046977.png', 2), ('data/digits/mnist-m/2/00008594.png', 2), ('data/digits/mnist-m/2/00013313.png', 2), ('data/digits/mnist-m/2/00008424.png', 2), ('data/digits/mnist-m/2/00051599.png', 2), ('data/digits/mnist-m/2/00009242.png', 2), ('data/digits/mnist-m/2/00005895.png', 2), ('data/digits/mnist-m/2/00010699.png', 2), ('data/digits/mnist-m/2/00002495.png', 2), ('data/digits/mnist-m/2/00030976.png', 2), ('data/digits/mnist-m/2/00045832.png', 2), ('data/digits/mnist-m/2/00006566.png', 2), ('data/digits/mnist-m/2/00016909.png', 2), ('data/digits/mnist-m/2/00000619.png', 2), ('data/digits/mnist-m/2/00008573.png', 2), ('data/digits/mnist-m/2/00052408.png', 2), ('data/digits/mnist-m/2/00002475.png', 2), ('data/digits/mnist-m/2/00033463.png', 2), ('data/digits/mnist-m/2/00016916.png', 2), ('data/digits/mnist-m/2/00050521.png', 2), ('data/digits/mnist-m/2/00002971.png', 2), ('data/digits/mnist-m/2/00031933.png', 2), ('data/digits/mnist-m/2/00055525.png', 2)]\n",
      "Target dataset Val Loss: 1.9897 Val Acc: 0.4243\n",
      "Epoch 43/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7427 Train Acc: 0.9830\n",
      "Source dataset Val Loss: 0.7399 Val Acc: 0.9837\n",
      "\n",
      "Target dataset Val Loss: 2.0622 Val Acc: 0.4049\n",
      "Epoch 44/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7368 Train Acc: 0.9851\n",
      "Source dataset Val Loss: 0.7505 Val Acc: 0.9779\n",
      "\n",
      "Target dataset Val Loss: 1.9958 Val Acc: 0.4346\n",
      "Epoch 45/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7422 Train Acc: 0.9799\n",
      "Source dataset Val Loss: 0.7318 Val Acc: 0.9876\n",
      "\n",
      "Target dataset Val Loss: 2.0319 Val Acc: 0.4322\n",
      "Epoch 46/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7473 Train Acc: 0.9784\n",
      "Source dataset Val Loss: 0.7394 Val Acc: 0.9843\n",
      "\n",
      "Target dataset Val Loss: 1.9035 Val Acc: 0.4679\n",
      "Epoch 47/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7529 Train Acc: 0.9815\n",
      "Source dataset Val Loss: 0.7474 Val Acc: 0.9780\n",
      "\n",
      "Target dataset Val Loss: 1.9162 Val Acc: 0.4510\n",
      "Epoch 48/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7371 Train Acc: 0.9830\n",
      "Source dataset Val Loss: 0.7390 Val Acc: 0.9819\n",
      "\n",
      "Target dataset Val Loss: 1.9192 Val Acc: 0.4487\n",
      "Epoch 49/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7350 Train Acc: 0.9851\n",
      "Source dataset Val Loss: 0.7287 Val Acc: 0.9871\n",
      "\n",
      "Selected 0/45017 images on threshold 0.9218575410269608\n",
      "[]\n",
      "Target dataset Val Loss: 2.0313 Val Acc: 0.4441\n",
      "Epoch 50/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7522 Train Acc: 0.9763\n",
      "Source dataset Val Loss: 0.7375 Val Acc: 0.9824\n",
      "\n",
      "Target dataset Val Loss: 1.8771 Val Acc: 0.4603\n",
      "Epoch 51/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7266 Train Acc: 0.9882\n",
      "Source dataset Val Loss: 0.7294 Val Acc: 0.9846\n",
      "\n",
      "Target dataset Val Loss: 1.9308 Val Acc: 0.4534\n",
      "Epoch 52/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7416 Train Acc: 0.9815\n",
      "Source dataset Val Loss: 0.7439 Val Acc: 0.9808\n",
      "\n",
      "Target dataset Val Loss: 1.9284 Val Acc: 0.4474\n",
      "Epoch 53/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7337 Train Acc: 0.9876\n",
      "Source dataset Val Loss: 0.7429 Val Acc: 0.9817\n",
      "\n",
      "Target dataset Val Loss: 1.9593 Val Acc: 0.4556\n",
      "Epoch 54/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7398 Train Acc: 0.9840\n",
      "Source dataset Val Loss: 0.7303 Val Acc: 0.9854\n",
      "\n",
      "Target dataset Val Loss: 2.0103 Val Acc: 0.4310\n",
      "Epoch 55/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7399 Train Acc: 0.9830\n",
      "Source dataset Val Loss: 0.7346 Val Acc: 0.9844\n",
      "\n",
      "Target dataset Val Loss: 2.0181 Val Acc: 0.4360\n",
      "Epoch 56/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7330 Train Acc: 0.9856\n",
      "Source dataset Val Loss: 0.7305 Val Acc: 0.9847\n",
      "\n",
      "Selected 0/45017 images on threshold 0.9214104363670915\n",
      "[]\n",
      "Target dataset Val Loss: 2.0871 Val Acc: 0.4190\n",
      "Epoch 57/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7559 Train Acc: 0.9748\n",
      "Source dataset Val Loss: 0.7358 Val Acc: 0.9848\n",
      "\n",
      "Target dataset Val Loss: 1.9382 Val Acc: 0.4577\n",
      "Epoch 58/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7316 Train Acc: 0.9840\n",
      "Source dataset Val Loss: 0.7340 Val Acc: 0.9843\n",
      "\n",
      "Target dataset Val Loss: 1.9710 Val Acc: 0.4455\n",
      "Epoch 59/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7361 Train Acc: 0.9835\n",
      "Source dataset Val Loss: 0.7379 Val Acc: 0.9827\n",
      "\n",
      "Target dataset Val Loss: 1.8760 Val Acc: 0.4698\n",
      "Epoch 60/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7163 Train Acc: 0.9897\n",
      "Source dataset Val Loss: 0.7227 Val Acc: 0.9874\n",
      "\n",
      "Target dataset Val Loss: 2.0883 Val Acc: 0.4088\n",
      "Epoch 61/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7334 Train Acc: 0.9840\n",
      "Source dataset Val Loss: 0.7300 Val Acc: 0.9862\n",
      "\n",
      "Target dataset Val Loss: 1.9991 Val Acc: 0.4432\n",
      "Epoch 62/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7356 Train Acc: 0.9815\n",
      "Source dataset Val Loss: 0.7441 Val Acc: 0.9784\n",
      "\n",
      "Target dataset Val Loss: 2.0921 Val Acc: 0.4105\n",
      "Epoch 63/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7430 Train Acc: 0.9809\n",
      "Source dataset Val Loss: 0.7406 Val Acc: 0.9825\n",
      "\n",
      "Selected 0/45017 images on threshold 0.9210129714032089\n",
      "[]\n",
      "Target dataset Val Loss: 2.0821 Val Acc: 0.4061\n",
      "Epoch 64/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7277 Train Acc: 0.9871\n",
      "Source dataset Val Loss: 0.7300 Val Acc: 0.9850\n",
      "\n",
      "Target dataset Val Loss: 2.0250 Val Acc: 0.4277\n",
      "Epoch 65/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7257 Train Acc: 0.9876\n",
      "Source dataset Val Loss: 0.7261 Val Acc: 0.9855\n",
      "\n",
      "Target dataset Val Loss: 2.0204 Val Acc: 0.4293\n",
      "Epoch 66/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7479 Train Acc: 0.9768\n",
      "Source dataset Val Loss: 0.7498 Val Acc: 0.9788\n",
      "\n",
      "Target dataset Val Loss: 1.9473 Val Acc: 0.4460\n",
      "Epoch 67/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7291 Train Acc: 0.9851\n",
      "Source dataset Val Loss: 0.7307 Val Acc: 0.9850\n",
      "\n",
      "Target dataset Val Loss: 1.9698 Val Acc: 0.4407\n",
      "Epoch 68/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7334 Train Acc: 0.9820\n",
      "Source dataset Val Loss: 0.7322 Val Acc: 0.9851\n",
      "\n",
      "Target dataset Val Loss: 1.9987 Val Acc: 0.4312\n",
      "Epoch 69/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7249 Train Acc: 0.9871\n",
      "Source dataset Val Loss: 0.7313 Val Acc: 0.9847\n",
      "\n",
      "Target dataset Val Loss: 2.1265 Val Acc: 0.4177\n",
      "Epoch 70/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7344 Train Acc: 0.9840\n",
      "Source dataset Val Loss: 0.7316 Val Acc: 0.9854\n",
      "\n",
      "Selected 0/45017 images on threshold 0.9215482556885095\n",
      "[]\n",
      "Target dataset Val Loss: 1.9232 Val Acc: 0.4680\n",
      "Epoch 71/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7291 Train Acc: 0.9845\n",
      "Source dataset Val Loss: 0.7264 Val Acc: 0.9860\n",
      "\n",
      "Target dataset Val Loss: 2.0312 Val Acc: 0.4449\n",
      "Epoch 72/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7247 Train Acc: 0.9887\n",
      "Source dataset Val Loss: 0.7243 Val Acc: 0.9877\n",
      "\n",
      "Target dataset Val Loss: 2.0031 Val Acc: 0.4503\n",
      "Epoch 73/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7288 Train Acc: 0.9887\n",
      "Source dataset Val Loss: 0.7253 Val Acc: 0.9869\n",
      "\n",
      "Target dataset Val Loss: 2.0239 Val Acc: 0.4438\n",
      "Epoch 74/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7336 Train Acc: 0.9830\n",
      "Source dataset Val Loss: 0.7358 Val Acc: 0.9839\n",
      "\n",
      "Target dataset Val Loss: 2.0202 Val Acc: 0.4526\n",
      "Epoch 75/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7417 Train Acc: 0.9820\n",
      "Source dataset Val Loss: 0.7289 Val Acc: 0.9879\n",
      "\n",
      "Target dataset Val Loss: 2.0415 Val Acc: 0.4243\n",
      "Epoch 76/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7320 Train Acc: 0.9871\n",
      "Source dataset Val Loss: 0.7321 Val Acc: 0.9847\n",
      "\n",
      "Target dataset Val Loss: 2.0476 Val Acc: 0.4406\n",
      "Epoch 77/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7164 Train Acc: 0.9928\n",
      "Source dataset Val Loss: 0.7181 Val Acc: 0.9897\n",
      "\n",
      "Selected 0/45017 images on threshold 0.9223193769186883\n",
      "[]\n",
      "Target dataset Val Loss: 2.0139 Val Acc: 0.4460\n",
      "Epoch 78/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7122 Train Acc: 0.9918\n",
      "Source dataset Val Loss: 0.7273 Val Acc: 0.9855\n",
      "\n",
      "Target dataset Val Loss: 2.0349 Val Acc: 0.4430\n",
      "Epoch 79/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7314 Train Acc: 0.9851\n",
      "Source dataset Val Loss: 0.7473 Val Acc: 0.9788\n",
      "\n",
      "Target dataset Val Loss: 2.1019 Val Acc: 0.4309\n",
      "Epoch 80/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7368 Train Acc: 0.9825\n",
      "Source dataset Val Loss: 0.7388 Val Acc: 0.9822\n",
      "\n",
      "Target dataset Val Loss: 2.1487 Val Acc: 0.4295\n",
      "Epoch 81/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7278 Train Acc: 0.9861\n",
      "Source dataset Val Loss: 0.7303 Val Acc: 0.9854\n",
      "\n",
      "Target dataset Val Loss: 2.1637 Val Acc: 0.4107\n",
      "Epoch 82/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7277 Train Acc: 0.9861\n",
      "Source dataset Val Loss: 0.7376 Val Acc: 0.9809\n",
      "\n",
      "Target dataset Val Loss: 2.1922 Val Acc: 0.4021\n",
      "Epoch 83/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7258 Train Acc: 0.9887\n",
      "Source dataset Val Loss: 0.7385 Val Acc: 0.9809\n",
      "\n",
      "Target dataset Val Loss: 2.1816 Val Acc: 0.4226\n",
      "Epoch 84/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7246 Train Acc: 0.9892\n",
      "Source dataset Val Loss: 0.7231 Val Acc: 0.9879\n",
      "\n",
      "Selected 0/45017 images on threshold 0.9219946426573606\n",
      "[]\n",
      "Target dataset Val Loss: 2.2247 Val Acc: 0.3965\n",
      "Epoch 85/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7445 Train Acc: 0.9815\n",
      "Source dataset Val Loss: 0.7289 Val Acc: 0.9862\n",
      "\n",
      "Target dataset Val Loss: 2.0122 Val Acc: 0.4475\n",
      "Epoch 86/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7178 Train Acc: 0.9907\n",
      "Source dataset Val Loss: 0.7233 Val Acc: 0.9882\n",
      "\n",
      "Target dataset Val Loss: 1.9892 Val Acc: 0.4473\n",
      "Epoch 87/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7243 Train Acc: 0.9876\n",
      "Source dataset Val Loss: 0.7315 Val Acc: 0.9827\n",
      "\n",
      "Target dataset Val Loss: 2.0209 Val Acc: 0.4464\n",
      "Epoch 88/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7314 Train Acc: 0.9840\n",
      "Source dataset Val Loss: 0.7248 Val Acc: 0.9870\n",
      "\n",
      "Target dataset Val Loss: 2.0122 Val Acc: 0.4543\n",
      "Epoch 89/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7138 Train Acc: 0.9918\n",
      "Source dataset Val Loss: 0.7168 Val Acc: 0.9901\n",
      "\n",
      "Target dataset Val Loss: 1.9549 Val Acc: 0.4669\n",
      "Epoch 90/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7239 Train Acc: 0.9866\n",
      "Source dataset Val Loss: 0.7221 Val Acc: 0.9875\n",
      "\n",
      "Target dataset Val Loss: 2.0679 Val Acc: 0.4434\n",
      "Epoch 91/159\n",
      "----------\n",
      "Source dataset Train Loss: 0.7161 Train Acc: 0.9912\n",
      "Source dataset Val Loss: 0.7245 Val Acc: 0.9871\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     source_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     target_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     model, source_history, target_history, label_history \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 17\u001b[0m         \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_train_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_adaptive_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_train_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mad_source_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_val_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mad_source_val_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabeled_dataloader_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_padded_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# we can not use padding with unlabeled data\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43munlabeled_dataloader_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_batch_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43munlabeled_target_train_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mad_unlabeled_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_val_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mad_target_val_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAD_UNSUPERVISED_MODEL_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpseudo_sample_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLING_PERIOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRHO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprevious_source_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprevious_target_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     res \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_trained_model(model, AD_UNSUPERVISED_MODEL_DIR)\n",
      "File \u001b[0;32m~/Documents/university/masters/deepl/project/lib/adaptive_train_eval.py:248\u001b[0m, in \u001b[0;36mtrain_adaptive_model\u001b[0;34m(model, criterion, optimizer, scheduler, device, source_train_dataset, source_val_dataset, labeled_dataloader_initializer, unlabeled_dataloader_initializer, unlabeled_target_train_dataset, target_val_dataset, output_dir, num_epochs, gradient_accumulation, pseudo_sample_period, rho, previous_source_history, previous_target_history, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m pseudo_sample_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# ========= Pseudo-labeling task =========\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m adaptive_threshold(classification_accuracy\u001b[38;5;241m=\u001b[39mlast_val_acc, rho\u001b[38;5;241m=\u001b[39mrho)\n\u001b[0;32m--> 248\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43mselect_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43munlabeled_dataloader_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43munlabeled_target_train_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unlabeled_target_train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images on threshold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    260\u001b[0m     pseudo_label_history\u001b[38;5;241m.\u001b[39mappend([])\n",
      "File \u001b[0;32m~/Documents/university/masters/deepl/project/lib/adaptive_train_eval.py:63\u001b[0m, in \u001b[0;36mselect_samples\u001b[0;34m(model, dataset, threshold, device, verbose)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 63\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     _, predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/models/resnet.py:93\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     92\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m---> 93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/functional.py:2482\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(mnist_encodings)\n",
    ")\n",
    "\n",
    "if AD_TRAIN_UNSUPERVISED_MODEL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_FINETUNED_MODEL_DIR, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=ad_source_train_dataset,\n",
    "            source_val_dataset=ad_source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            # we can not use padding with unlabeled data\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True, n_workers=8\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=ad_unlabeled_dataset,\n",
    "            target_val_dataset=ad_target_val_dataset,\n",
    "            output_dir=AD_UNSUPERVISED_MODEL_DIR,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(model, AD_UNSUPERVISED_MODEL_DIR)\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, mnist_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Supervised Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(mnist_encodings)\n",
    ")\n",
    "\n",
    "if AD_FINETUNE_SEMI_SUPERVISED_MODEL_SMALL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_SMALL, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_SMALL, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            ad_labeled_dataset_small, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=ad_source_val_loader,\n",
    "        output_dir=AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_SMALL,\n",
    "        num_epochs=1,\n",
    "        patience=2,\n",
    "        warmup_period=2,\n",
    "        previous_history=history,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_SMALL, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_SMALL, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(mnist_encodings)\n",
    ")\n",
    "\n",
    "if AD_TRAIN_SEMI_SUPERVISED_MODEL_SMALL:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_SMALL, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=ad_labeled_dataset_small,\n",
    "            source_val_dataset=ad_source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            # we can not use padding with unlabeled data\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True, n_workers=8\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=ad_unlabeled_dataset_small,\n",
    "            target_val_dataset=ad_target_val_dataset,\n",
    "            output_dir=AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_SMALL,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(\n",
    "        model, AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_SMALL\n",
    "    )\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, mnist_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(mnist_encodings)\n",
    ")\n",
    "\n",
    "if AD_FINETUNE_SEMI_SUPERVISED_MODEL_LARGE:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_LARGE, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_LARGE, \"history.pickle\"))\n",
    "    model, history = lib.torch_train_eval.train_model(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer_ft,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        device=device,\n",
    "        train_dataloader=tasks.preprocessing.create_padded_dataloader(\n",
    "            ad_labeled_dataset_large, shuffle=True, batch_size=BATCH_SIZE\n",
    "        ),\n",
    "        val_dataloader=ad_source_val_loader,\n",
    "        output_dir=AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_LARGE,\n",
    "        num_epochs=1,\n",
    "        patience=0,\n",
    "        warmup_period=0,\n",
    "        previous_history=history,\n",
    "        verbose=False\n",
    "    )\n",
    "else:\n",
    "    history = tasks.utils.try_load_history(os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_LARGE, \"history.pickle\"))\n",
    "    model = tasks.utils.try_load_weights(model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_LARGE, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_source_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(mnist_encodings)\n",
    ")\n",
    "\n",
    "if AD_TRAIN_SEMI_SUPERVISED_MODEL_LARGE:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    # import fine tuned model, not previous unsupervised model\n",
    "    # we are assuming training takes one go, no intermediate saving here\n",
    "    model = tasks.utils.try_load_weights(\n",
    "        model, os.path.join(AD_SEMI_SUPERVISED_FINETUNED_MODEL_DIR_LARGE, \"model.pt\")\n",
    "    )\n",
    "    optimizer_ft = optim.AdamW(model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    source_history = None\n",
    "    target_history = None\n",
    "    model, source_history, target_history, label_history = (\n",
    "        lib.adaptive_train_eval.train_adaptive_model(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer_ft,\n",
    "            scheduler=exp_lr_scheduler,\n",
    "            device=device,\n",
    "            source_train_dataset=ad_labeled_dataset_large,\n",
    "            source_val_dataset=ad_source_val_dataset,\n",
    "            labeled_dataloader_initializer=lambda dataset, sampler=None: tasks.preprocessing.create_padded_dataloader(\n",
    "                dataset, sampler=sampler, batch_size=BATCH_SIZE\n",
    "            ),\n",
    "            # we can not use padding with unlabeled data\n",
    "            unlabeled_dataloader_initializer=lambda dataset: tasks.preprocessing.single_batch_loader(\n",
    "                dataset, shuffle=True, n_workers=8\n",
    "            ),\n",
    "            unlabeled_target_train_dataset=ad_unlabeled_dataset_large,\n",
    "            target_val_dataset=ad_target_val_dataset,\n",
    "            output_dir=AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_LARGE,\n",
    "            num_epochs=160,\n",
    "            pseudo_sample_period=SAMPLING_PERIOD,\n",
    "            rho=RHO,\n",
    "            previous_source_history=source_history,\n",
    "            previous_target_history=target_history,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    res = tasks.utils.load_trained_model(\n",
    "        model, AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_LARGE\n",
    "    )\n",
    "    model = res[\"model\"]\n",
    "    source_history = res[\"source_history\"]\n",
    "    target_history = res[\"target_history\"]\n",
    "    label_history = res[\"label_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(source_history)\n",
    "tasks.results.learning_curves_accuracy(source_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.learning_curves_loss(target_history)\n",
    "tasks.results.learning_curves_accuracy(target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history(label_history, mnist_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.classification_results(model, ad_target_test_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "office_model_dirs = [\n",
    "        (FINETUNED_SOURCE__MODEL_DIR, \"Source\"),\n",
    "        (UNSUPERVISED_MODEL_DIR, \"Unsupervised\"),\n",
    "        (SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_10, \"Semi-Supervised 10\"),\n",
    "        (SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_20, \"Semi-Supervised 20\")\n",
    "    ]\n",
    "office_base_model = tasks.utils.get_model(device)\n",
    "\n",
    "mnist_model_dirs = [\n",
    "        (AD_FINETUNED_MODEL_DIR, \"Source\"),\n",
    "        (AD_UNSUPERVISED_MODEL_DIR, \"Unsupervised\"),\n",
    "        (AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_SMALL, \"Semi-Supervised 10\"),\n",
    "        (AD_SEMI_SUPERVISED_ADAPTIVE_MODEL_DIR_LARGE, \"Semi-Supervised 20\"),\n",
    "\n",
    "    ]\n",
    "mnist_base_model = tasks.utils.get_model(\n",
    "    device=device, replace_fc_layer=True, num_classes=len(mnist_encodings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_classification_matrices(\n",
    "    office_base_model,\n",
    "    office_model_dirs,\n",
    "    target_test_loader,\n",
    "    device,\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    save_path=os.path.join(RESULTS_DIR, \"office_cls_matrix.jpeg\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_classification_matrices(\n",
    "    mnist_base_model,\n",
    "    mnist_model_dirs[:2],\n",
    "    ad_target_test_loader,\n",
    "    device,\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    save_path=os.path.join(RESULTS_DIR, \"mnist_cls_matrix.jpeg\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history_grid(\n",
    "    mnist_model_dirs[1:],\n",
    "    mnist_encodings,\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    figsize=(15, 7),\n",
    "    save_path=os.path.join(RESULTS_DIR, \"mnist_misclassifications.jpeg\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.results.plot_label_history_grid(\n",
    "    office_model_dirs[1:],\n",
    "    office_encodings,\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    figsize=(15, 7),\n",
    "    save_path=os.path.join(RESULTS_DIR, \"office_misclassifications.jpeg\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "classification_accuracies = np.linspace(0, 1, 100)\n",
    "path = os.path.join(RESULTS_DIR, \"adaptive_threshold.jpeg\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for rho in [3, 4]:\n",
    "    thresholds = [lib.adaptive_train_eval.adaptive_threshold(acc, rho) for acc in classification_accuracies]\n",
    "    \n",
    "    sns.lineplot(x=classification_accuracies, y=thresholds, label=f'ρ = {rho}')\n",
    "\n",
    "plt.xlabel('Classification Accuracy', fontsize=14)\n",
    "plt.ylabel('Adaptive Threshold', fontsize=14)\n",
    "plt.title('Adaptive Threshold vs Classification Accuracy', fontsize=16)\n",
    "plt.legend(title='Parameter ρ', title_fontsize='13', fontsize='12')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved in {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
